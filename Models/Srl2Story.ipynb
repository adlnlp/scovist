{"cells":[{"cell_type":"markdown","metadata":{"id":"PLRe6-9nGSne"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5386,"status":"ok","timestamp":1674434838196,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"oy5pZbG5EMvC","outputId":"8e04777c-b692-4619-8dd5-7827224061e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17020,"status":"ok","timestamp":1674436257820,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"aIHtxL0DDmSL","outputId":"01b2de06-441f-4b17-a25c-2225b136ebb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import json\n","import torch\n","import os\n","import datetime\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import random\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from sklearn.model_selection import train_test_split\n","\n","from transformers.models.bart.modeling_bart import shift_tokens_right\n","from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n","\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('brown')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1674436258382,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"9YER8TEREWrx","outputId":"299e0b87-ab94-4a07-9bae-9ca34e2f36e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["We are using GPU.\n"]}],"source":["use_cuda = True if torch.cuda.is_available() else False\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('We are using GPU.' if use_cuda else 'We are using CPU.')"]},{"cell_type":"markdown","metadata":{"id":"qoyCHRz7IplU"},"source":["# Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5zJLXqtUEhY"},"outputs":[],"source":["def collate_fn(data):\n","    \"\"\"\n","    Process each batch\n","    \"\"\"\n","    # sort to find encoder input max length. data is nested list of storylines\n","    data.sort(key = lambda x: len(x[0]), reverse=True)\n","    input_ids, target_story_ids, story_ids, storyline = zip(*data)\n","    input_max_len = len(input_ids[0])\n","\n","    # sort by decoder len\n","    data.sort(key=lambda x: len(x[1]), reverse=True)\n","    input_ids, target_story_ids, story_ids, storyline = zip(*data)\n","    story_max_len = len(target_story_ids[0])\n","\n","    inputs_padded = []\n","    input_attention_ids = []\n","    gold_stories_padded = []\n","\n","    for i in range(len(data)):\n","\n","        input_line = input_ids[i] # ['input_ids']\n","        seq_len = len(input_line)\n","        pad_len = input_max_len - seq_len\n","        input_line.extend([tokenizer.pad_token_id] * pad_len) # add padding to right. Token IDs\n","        inputs_padded.append(torch.tensor(input_line))\n","\n","        # encoder attention ids\n","        attentions = ([1] * seq_len)+ ([0] * pad_len)\n","        input_attention_ids.append(torch.tensor(attentions))\n","\n","        gold_story = target_story_ids[i] # ['input_ids']\n","        seq_len = len(gold_story)\n","        pad_len = story_max_len - seq_len\n","        gold_story.extend([tokenizer.pad_token_id] * pad_len)\n","        gold_stories_padded.append(torch.tensor(gold_story))\n","\n","    inputs_padded = torch.stack(inputs_padded) # (batch_size, max_len)\n","    input_attention_ids = torch.stack(input_attention_ids) # (batch_size, max_len)\n","\n","    gold_stories_padded = torch.stack(gold_stories_padded)\n","\n","    batch_data = {\"inputs\": inputs_padded, # (batch_size, max_len)\n","                  \"gold_stories\": gold_stories_padded,\n","                  \"input_attention_ids\": input_attention_ids,\n","                  \"batch_max_lens\": (input_max_len, story_max_len),\n","                  \"story_ids\": story_ids,\n","                  \"text_inputs\": storyline} # <-- return sentence lengths\n","\n","    return batch_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-cJtmuSIqrH"},"outputs":[],"source":["class VSTDataLoader(Dataset):\n","\n","    def __init__(self, storyinfo_path, storyline_path, tokenizer,\n","                 split = \"train\", cap_type = \"CLIP\", weights = \"tgcn_cosine\"):\n","\n","        self.tokenizer = tokenizer\n","\n","        # Ground truth stories: for finding the images of the story\n","        self.gt_stories = json.load(open(os.path.join(storyinfo_path, \"{}_stories.json\".format(split))))\n","        self.story_ids = list(self.gt_stories.keys())\n","\n","\n","        self.srls = json.load(open(os.path.join(storyline_path, \"{}_{}_srl.json\".format(cap_type, weights))))\n","        srl_keys = set(list(self.srls.keys()))\n","        self.story_ids = [x for x in self.story_ids if x in srl_keys]\n","\n","    def __len__(self):\n","\n","        return len(self.story_ids)\n","\n","    def __getitem__(self, index):\n","\n","        story_id = self.story_ids[index]\n","\n","        srl = self.srls[story_id]\n","\n","        storyline = ' </s> '.join(srl)\n","        input_ids = self.tokenizer(storyline)['input_ids']\n","\n","        joined_story = ' '.join(self.gt_stories[story_id][\"story\"])\n","        target_story_ids = self.tokenizer(joined_story)[\"input_ids\"]\n","\n","\n","        return input_ids, target_story_ids, story_id, storyline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1674436272750,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"-GumK84lQL6P","outputId":"b7d27b24-27ee-4de8-bb4e-9342725e245f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer size before adding tokens: 50265\n"]}],"source":["tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n","print(\"Tokenizer size before adding tokens: {}\".format(len(tokenizer)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1674436273323,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"6MBnMWEaTI7f","outputId":"3417ff7e-ecde-4742-ec53-420c99241eee"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 208 ms, sys: 79.6 ms, total: 287 ms\n","Wall time: 397 ms\n"]}],"source":["%%time\n","storyinfo_path = \"/content/drive\"\n","srl_path = \"/content/drive\"\n","\n","valid_dl = VSTDataLoader(storyinfo_path, srl_path, tokenizer, split = \"valid\")\n","train_dl = VSTDataLoader(storyinfo_path, srl_path, tokenizer, split = \"train\")\n","test_dl = VSTDataLoader(storyinfo_path, srl_path, tokenizer, split = \"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1674436273323,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"xEzg9jHpTc0Q","outputId":"37e8acb1-9e6a-4c44-e2c6-232dcbcc7208"},"outputs":[{"output_type":"stream","name":"stdout","text":["input ids: 108\n","Length of story_ids: 43\n","Story id: 47944\n"]}],"source":["### test data loader\n","\n","index = 2414\n","\n","print(\"input ids: {}\".format(len(test_dl[index][0])))\n","print(\"Length of story_ids: {}\".format(len(test_dl[index][1])))\n","print(\"Story id: {}\".format(test_dl[index][2]))"]},{"cell_type":"code","source":["print(valid_dl.__len__())\n","print(train_dl.__len__())\n","print(test_dl.__len__())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38SZpDHna6kg","executionInfo":{"status":"ok","timestamp":1669860223860,"user_tz":-660,"elapsed":18,"user":{"displayName":"Eileen","userId":"01091172001417006453"}},"outputId":"baac1e1f-941f-4dd9-80e7-9e12200d6bdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4988\n","40137\n","5055\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4RoqgOeT5uO"},"outputs":[],"source":["valid_dataloader = torch.utils.data.DataLoader(valid_dl, shuffle = False, batch_size = 8, collate_fn = collate_fn)\n","test_dataloader = torch.utils.data.DataLoader(test_dl, shuffle = False, batch_size = 16, collate_fn = collate_fn)\n","train_dataloader = torch.utils.data.DataLoader(train_dl, shuffle = True, batch_size = 8, collate_fn = collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1665979710487,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"DZ3QfLk3T632","outputId":"3b6cf40b-23fe-408d-f87a-59d85ea98d8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 28.2 ms, sys: 0 ns, total: 28.2 ms\n","Wall time: 27.7 ms\n"]}],"source":["%%time\n","### test collate function\n","for batch_idx, data in enumerate(valid_dataloader):\n","    # if batch_idx % 100 == 0: print(batch_idx)\n","    if batch_idx > 0: break\n","    batch_data = data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1665979712432,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"N9cU8gYHT8AM","outputId":"e4c22b82-a190-41de-ddc3-318b1d282e81"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids: torch.Size([122])\n","Story ids:  5\n","input attention ids:  torch.Size([8, 122])\n","Gold Stories:  torch.Size([8, 74])\n"]}],"source":["print(\"input_ids: {}\".format(batch_data[\"inputs\"][0].shape))\n","print(\"Story ids: \", len(batch_data[\"story_ids\"][0]))\n","print(\"input attention ids: \", batch_data[\"input_attention_ids\"].shape)\n","print(\"Gold Stories: \", batch_data[\"gold_stories\"].shape)"]},{"cell_type":"markdown","metadata":{"id":"plZX312Tgzot"},"source":["# Bart Encoder Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u72dWDP1MhEu"},"outputs":[],"source":["class StoryDecoder(nn.Module):\n","    def __init__(self, bart_model, tokenizer):\n","        \"\"\"\n","        Event to Stories\n","        \"\"\"\n","        super(StoryDecoder, self).__init__()\n","\n","        self.tokenizer = tokenizer\n","        self.bart_model = bart_model\n","\n","        new_embeddings = self.bart_model.resize_token_embeddings(len(tokenizer))\n","\n","    def forward(self, batch_data, device):\n","\n","        decoder_input_ids = shift_tokens_right(batch_data[\"gold_stories\"],\n","                                               tokenizer.pad_token_id,\n","                                               tokenizer.eos_token_id)\n","\n","        input_ids = batch_data[\"inputs\"].to(device)\n","\n","        output = self.bart_model(input_ids = batch_data[\"inputs\"].to(device),\n","                                attention_mask = batch_data[\"input_attention_ids\"].to(device),\n","                                decoder_input_ids = decoder_input_ids.to(device))\n","\n","        lm_logits = output[0] # (batch_size, max_story_len, vocab_size)\n","\n","        return lm_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRmCunN8W4rO"},"outputs":[],"source":["story_decoder = StoryDecoder(bart_model, tokenizer)\n","lm_logits = story_decoder(batch_data, device)"]},{"cell_type":"markdown","metadata":{"id":"xNtR0P52M4RA"},"source":["# Plot2Story"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjOlvpboM7gI"},"outputs":[],"source":["class Plot2Story(nn.Module):\n","    def __init__(self, tokenizer, model_nm = \"facebook/bart-large\"):\n","        \"\"\"\n","        Event to Stories\n","        \"\"\"\n","        super(Plot2Story, self).__init__()\n","\n","        self.bart_model = BartForConditionalGeneration.from_pretrained(model_nm)\n","        new_embeddings = self.bart_model.resize_token_embeddings(len(tokenizer))\n","\n","        self.tokenizer = tokenizer\n","\n","    def forward(self, batch_data, device):\n","\n","        decoder_input_ids = shift_tokens_right(batch_data[\"gold stories\"],\n","                                               tokenizer.pad_token_id,\n","                                               tokenizer.bos_token_id)\n","\n","        output = self.bart_model(input_ids = batch_data[\"story lines\"].to(device),\n","                                attention_mask = batch_data[\"input attention ids\"].to(device),\n","                                decoder_input_ids = decoder_input_ids.to(device),\n","                                decoder_attention_mask = batch_data[\"decoder attention ids\"].to(device))\n","\n","        lm_logits = output[0] # (batch_size, max_story_len, vocab_size)\n","\n","        return lm_logits"]},{"cell_type":"markdown","metadata":{"id":"AobufK_KMXSc"},"source":["# Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xynyd1boMYsG"},"outputs":[],"source":["def train_epoch(model, dataloader, criterion, optimizer, mode = 'train'):\n","    \"\"\"\n","    Function to train each epoch.\n","\n","    Args:\n","        dataloader: either train or valid dataloader.\n","        criterion: loss function to use.\n","        optimizer: optimizer for training.\n","        mode: either \"train\" or \"validate\".\n","    \"\"\"\n","    time1 = datetime.datetime.now()\n","\n","    if mode == 'train':\n","        model.train()\n","    else: #put model in validation mode\n","        model.eval()\n","\n","    #keep track of training and validation loss and accuracy\n","    running_loss, running_acc = 0, 0\n","\n","    # mini-batch training with the dataloader\n","    for batch_idx, data in enumerate(dataloader):\n","\n","        # move this batch of data to specified device\n","        batch_data = data\n","        # gradient calculation when training\n","        with torch.set_grad_enabled(mode ==\"train\"):\n","\n","            # forward data through model to get logits\n","            lm_logits = model(batch_data, device) # (batch_size, max_sent_len, vocab_size)\n","\n","\n","            loss = criterion(lm_logits.view(-1, lm_logits.shape[-1]).to('cpu'),\n","                             batch_data['gold_stories'].view(-1))\n","\n","            if mode == 'train':\n","                loss.backward()       # backward the loss and calculate gradients for parameters.\n","                optimizer.step()      # update the parameters.\n","                optimizer.zero_grad() # zero the gradient to stop from accumulating\n","\n","        if (batch_idx + 1) % 10 == 0:\n","            print(\"Processed batch: {}. Loss: {}\".format(batch_idx+1, loss.item()))\n","\n","        running_loss += loss.item()\n","\n","    # note len(dataloader) is number of batches\n","    epoch_loss = running_loss/len(dataloader) # len(dataloader) = no. of examples / batch size\n","    time2 = datetime.datetime.now()\n","\n","    return epoch_loss, (time2-time1).total_seconds()\n","\n","def train_model(model, training_info, start_epoch = 0):\n","    \"\"\"\n","    Function for model training.\n","\n","    Args:\n","        model: initialised pytorch model (class)\n","        training_info: dict of loader, criterion and optimizer information.\n","        opt: the parsed arguments.\n","        start_epoch: starting epoch. Will be > 0 if loaded model checkpoint.\n","    \"\"\"\n","\n","    MIN_LOSS = float('inf')\n","    EARLY_STOPPING_COUNT = 0\n","    EVAL_EVERY_EPOCH = 1\n","\n","    scheduler = training_info[\"scheduler\"]\n","\n","    for epoch in range(start_epoch, training_info[\"num_epochs\"]):\n","\n","        # forward training data through model\n","        train_loss, runtime = train_epoch(model, training_info[\"train_loader\"],\n","                                                    training_info[\"criterion\"],\n","                                                    training_info[\"optimizer\"],\n","                                                    mode = 'train')\n","\n","        print(\"Epoch:%d, train loss: %.4f, time: %.2fs\" %(epoch+1, train_loss, runtime))\n","\n","        if (epoch + 1) % EVAL_EVERY_EPOCH == 0:\n","            valid_loss, runtime = train_epoch(model, training_info[\"valid_loader\"],\n","                                                         training_info[\"criterion\"],\n","                                                         training_info[\"optimizer\"],\n","                                                         mode = 'validate')\n","\n","            print('-'*60)\n","            print(\"Epoch:%d, valid loss: %.4f, time: %.2fs\" %(epoch+1, valid_loss, runtime))\n","            print('-'*60)\n","\n","            \"\"\"\n","            CHECK EARLY STOPPING CONDITIONS\n","            \"\"\"\n","            if valid_loss < MIN_LOSS:\n","                MIN_LOSS = valid_loss\n","                EARLY_STOPPING_COUNT = 0\n","\n","                # save the best model so far\n","                state = {\"epoch\": epoch + 1, \"model\": model.state_dict(), \"valid_loss\": valid_loss,\n","                         \"train_loss\": train_loss}\n","                model_name = \"srl2story_CLIP_tgcn_cosine_epoch{}.pth.tar\".format(epoch + 1)\n","                torch.save(state, os.path.join(training_info[\"save_path\"], model_name))\n","            else:\n","                EARLY_STOPPING_COUNT += 1\n","\n","            if EARLY_STOPPING_COUNT == training_info[\"num_es_epochs\"]:\n","                break\n","\n","            # apply learning rate decay\n","            scheduler.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PryTDKGSWy--","executionInfo":{"status":"ok","timestamp":1665979768028,"user_tz":-660,"elapsed":21456,"user":{"displayName":"Eileen","userId":"01091172001417006453"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["a0305d7fde414d4599f7d9db12db0a1d","192f87b1bdf0495793f7607c2d1d3411","5893e467596540919dcbfb98cde69661","e95cac1f79594aeb958a69546f58c48c","ce414fff05a242269e19c07d346af84a","0310f1742b394dd193f6c62235d42fd0","0eb023805c064c988750041e46cd6b92","934a99ddf60b4e858a744bb556295827","6ce6858df0034f66b0d44e8bee22728b","217efd8702f4403ca088f20a57d9d21c","511573d423cd4add98e508710089de96"]},"outputId":"1688db49-f887-4d39-fa34-9c15fb88de1f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0305d7fde414d4599f7d9db12db0a1d"}},"metadata":{}}],"source":["model_nm = \"facebook/bart-large\"\n","bart_model = BartForConditionalGeneration.from_pretrained(model_nm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xs0nV6kcP3g"},"outputs":[],"source":["model = StoryDecoder(bart_model, tokenizer)\n","model = model.to(device) # move to GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_SxZbLPPN8T","outputId":"132f9326-cdff-4e8d-f9f3-92bc1cb0d815"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processed batch: 10. Loss: 3.499938488006592\n","Processed batch: 20. Loss: 3.3829314708709717\n","Processed batch: 30. Loss: 3.360431671142578\n","Processed batch: 40. Loss: 3.1429808139801025\n","Processed batch: 50. Loss: 3.55824613571167\n","Processed batch: 60. Loss: 3.2122206687927246\n","Processed batch: 70. Loss: 3.5974764823913574\n","Processed batch: 80. Loss: 3.423002243041992\n","Processed batch: 90. Loss: 3.4307570457458496\n","Processed batch: 100. Loss: 3.218897819519043\n","Processed batch: 110. Loss: 3.5450572967529297\n","Processed batch: 120. Loss: 2.9842424392700195\n","Processed batch: 130. Loss: 3.2806568145751953\n","Processed batch: 140. Loss: 3.0795178413391113\n","Processed batch: 150. Loss: 3.1647517681121826\n","Processed batch: 160. Loss: 3.138158082962036\n","Processed batch: 170. Loss: 3.3837642669677734\n","Processed batch: 180. Loss: 2.8943586349487305\n","Processed batch: 190. Loss: 3.209237575531006\n","Processed batch: 200. Loss: 3.146991729736328\n","Processed batch: 210. Loss: 3.5114927291870117\n","Processed batch: 220. Loss: 3.1281590461730957\n","Processed batch: 230. Loss: 3.158027172088623\n","Processed batch: 240. Loss: 3.163551092147827\n","Processed batch: 250. Loss: 3.1192431449890137\n","Processed batch: 260. Loss: 3.2507896423339844\n","Processed batch: 270. Loss: 3.193657398223877\n","Processed batch: 280. Loss: 3.100546360015869\n","Processed batch: 290. Loss: 3.388275146484375\n","Processed batch: 300. Loss: 3.0038163661956787\n","Processed batch: 310. Loss: 3.222851037979126\n","Processed batch: 320. Loss: 2.9903669357299805\n","Processed batch: 330. Loss: 3.128769636154175\n","Processed batch: 340. Loss: 3.637779474258423\n","Processed batch: 350. Loss: 3.0685105323791504\n","Processed batch: 360. Loss: 3.0296061038970947\n","Processed batch: 370. Loss: 3.1946821212768555\n","Processed batch: 380. Loss: 2.8443055152893066\n","Processed batch: 390. Loss: 2.9958302974700928\n","Processed batch: 400. Loss: 3.490748643875122\n","Processed batch: 410. Loss: 3.120059013366699\n","Processed batch: 420. Loss: 3.0982820987701416\n","Processed batch: 430. Loss: 3.293607711791992\n","Processed batch: 440. Loss: 3.0038530826568604\n","Processed batch: 450. Loss: 2.9816904067993164\n","Processed batch: 460. Loss: 2.925598621368408\n","Processed batch: 470. Loss: 3.1945786476135254\n","Processed batch: 480. Loss: 3.127056121826172\n","Processed batch: 490. Loss: 3.0610928535461426\n","Processed batch: 500. Loss: 3.0109753608703613\n","Processed batch: 510. Loss: 3.099666118621826\n","Processed batch: 520. Loss: 3.186044931411743\n","Processed batch: 530. Loss: 3.224395751953125\n","Processed batch: 540. Loss: 3.1544899940490723\n","Processed batch: 550. Loss: 3.121181011199951\n","Processed batch: 560. Loss: 2.650233268737793\n","Processed batch: 570. Loss: 3.6355507373809814\n","Processed batch: 580. Loss: 2.931576728820801\n","Processed batch: 590. Loss: 2.86198091506958\n","Processed batch: 600. Loss: 2.9129457473754883\n","Processed batch: 610. Loss: 3.03924822807312\n","Processed batch: 620. Loss: 3.066204071044922\n","Processed batch: 630. Loss: 2.9316000938415527\n","Processed batch: 640. Loss: 2.8427999019622803\n","Processed batch: 650. Loss: 2.826233148574829\n","Processed batch: 660. Loss: 3.258009433746338\n","Processed batch: 670. Loss: 3.0379154682159424\n","Processed batch: 680. Loss: 3.138469696044922\n","Processed batch: 690. Loss: 2.83889102935791\n","Processed batch: 700. Loss: 2.940081834793091\n","Processed batch: 710. Loss: 2.864997386932373\n","Processed batch: 720. Loss: 3.049940824508667\n","Processed batch: 730. Loss: 3.083083152770996\n","Processed batch: 740. Loss: 3.027791738510132\n","Processed batch: 750. Loss: 3.040109634399414\n","Processed batch: 760. Loss: 2.923142194747925\n","Processed batch: 770. Loss: 3.1202986240386963\n","Processed batch: 780. Loss: 2.798750638961792\n","Processed batch: 790. Loss: 3.1576902866363525\n","Processed batch: 800. Loss: 2.7307956218719482\n","Processed batch: 810. Loss: 3.3855056762695312\n","Processed batch: 820. Loss: 2.9952075481414795\n","Processed batch: 830. Loss: 2.9785690307617188\n","Processed batch: 840. Loss: 3.183410882949829\n","Processed batch: 850. Loss: 2.9168572425842285\n","Processed batch: 860. Loss: 2.8941519260406494\n","Processed batch: 870. Loss: 3.020038366317749\n","Processed batch: 880. Loss: 3.178385019302368\n","Processed batch: 890. Loss: 3.144887924194336\n","Processed batch: 900. Loss: 2.8680710792541504\n","Processed batch: 910. Loss: 3.0807085037231445\n","Processed batch: 920. Loss: 2.9459357261657715\n","Processed batch: 930. Loss: 3.082129955291748\n","Processed batch: 940. Loss: 3.23698353767395\n","Processed batch: 950. Loss: 3.1961960792541504\n","Processed batch: 960. Loss: 3.258446455001831\n","Processed batch: 970. Loss: 3.0987305641174316\n","Processed batch: 980. Loss: 2.9129931926727295\n","Processed batch: 990. Loss: 2.8730556964874268\n","Processed batch: 1000. Loss: 2.950408697128296\n","Processed batch: 1010. Loss: 3.059939384460449\n","Processed batch: 1020. Loss: 2.754918098449707\n","Processed batch: 1030. Loss: 3.1348257064819336\n","Processed batch: 1040. Loss: 2.955007791519165\n","Processed batch: 1050. Loss: 2.819211006164551\n","Processed batch: 1060. Loss: 3.2291557788848877\n","Processed batch: 1070. Loss: 3.1938600540161133\n","Processed batch: 1080. Loss: 2.8949201107025146\n","Processed batch: 1090. Loss: 3.0352871417999268\n","Processed batch: 1100. Loss: 3.1387875080108643\n","Processed batch: 1110. Loss: 2.917508125305176\n","Processed batch: 1120. Loss: 2.9355111122131348\n","Processed batch: 1130. Loss: 2.9079809188842773\n","Processed batch: 1140. Loss: 3.0417850017547607\n","Processed batch: 1150. Loss: 3.262399673461914\n","Processed batch: 1160. Loss: 3.207348108291626\n","Processed batch: 1170. Loss: 3.0797841548919678\n","Processed batch: 1180. Loss: 3.1557114124298096\n","Processed batch: 1190. Loss: 3.0302700996398926\n","Processed batch: 1200. Loss: 2.6568386554718018\n","Processed batch: 1210. Loss: 2.9808459281921387\n","Processed batch: 1220. Loss: 3.253052234649658\n","Processed batch: 1230. Loss: 3.2074413299560547\n","Processed batch: 1240. Loss: 3.0949552059173584\n","Processed batch: 1250. Loss: 3.2705163955688477\n","Processed batch: 1260. Loss: 2.8949105739593506\n","Processed batch: 1270. Loss: 2.889031171798706\n","Processed batch: 1280. Loss: 3.005300760269165\n","Processed batch: 1290. Loss: 3.0881431102752686\n","Processed batch: 1300. Loss: 2.899538278579712\n","Processed batch: 1310. Loss: 2.963655710220337\n","Processed batch: 1320. Loss: 2.96235728263855\n","Processed batch: 1330. Loss: 2.869872570037842\n","Processed batch: 1340. Loss: 2.8073017597198486\n","Processed batch: 1350. Loss: 2.9670464992523193\n","Processed batch: 1360. Loss: 2.803081750869751\n","Processed batch: 1370. Loss: 3.070556163787842\n","Processed batch: 1380. Loss: 3.147669792175293\n","Processed batch: 1390. Loss: 3.2431883811950684\n","Processed batch: 1400. Loss: 2.9889369010925293\n","Processed batch: 1410. Loss: 2.9741387367248535\n","Processed batch: 1420. Loss: 3.000227451324463\n","Processed batch: 1430. Loss: 2.9003424644470215\n","Processed batch: 1440. Loss: 3.155294179916382\n","Processed batch: 1450. Loss: 3.06302809715271\n","Processed batch: 1460. Loss: 3.0060834884643555\n","Processed batch: 1470. Loss: 3.4088659286499023\n","Processed batch: 1480. Loss: 2.7131893634796143\n","Processed batch: 1490. Loss: 3.1712710857391357\n","Processed batch: 1500. Loss: 2.6315131187438965\n","Processed batch: 1510. Loss: 3.086658000946045\n","Processed batch: 1520. Loss: 3.0061452388763428\n","Processed batch: 1530. Loss: 2.7208266258239746\n","Processed batch: 1540. Loss: 2.765425205230713\n","Processed batch: 1550. Loss: 2.814873695373535\n","Processed batch: 1560. Loss: 2.8523802757263184\n","Processed batch: 1570. Loss: 2.828261613845825\n","Processed batch: 1580. Loss: 2.9862992763519287\n","Processed batch: 1590. Loss: 3.136287212371826\n","Processed batch: 1600. Loss: 2.8305652141571045\n","Processed batch: 1610. Loss: 2.879960536956787\n","Processed batch: 1620. Loss: 2.854276418685913\n","Processed batch: 1630. Loss: 3.0517232418060303\n","Processed batch: 1640. Loss: 3.395940065383911\n","Processed batch: 1650. Loss: 3.2179384231567383\n","Processed batch: 1660. Loss: 2.676062822341919\n","Processed batch: 1670. Loss: 2.820793390274048\n","Processed batch: 1680. Loss: 2.831296920776367\n","Processed batch: 1690. Loss: 2.8472397327423096\n","Processed batch: 1700. Loss: 3.0226352214813232\n","Processed batch: 1710. Loss: 3.062046527862549\n","Processed batch: 1720. Loss: 2.7782039642333984\n","Processed batch: 1730. Loss: 3.171659469604492\n","Processed batch: 1740. Loss: 2.951171636581421\n","Processed batch: 1750. Loss: 3.0256688594818115\n","Processed batch: 1760. Loss: 2.9148240089416504\n","Processed batch: 1770. Loss: 2.9175312519073486\n","Processed batch: 1780. Loss: 2.9930877685546875\n","Processed batch: 1790. Loss: 2.6938295364379883\n","Processed batch: 1800. Loss: 2.7553627490997314\n","Processed batch: 1810. Loss: 2.488278388977051\n","Processed batch: 1820. Loss: 3.007070779800415\n","Processed batch: 1830. Loss: 2.6663947105407715\n","Processed batch: 1840. Loss: 2.9680886268615723\n","Processed batch: 1850. Loss: 2.9290971755981445\n","Processed batch: 1860. Loss: 2.934237480163574\n","Processed batch: 1870. Loss: 2.7857751846313477\n","Processed batch: 1880. Loss: 2.4825174808502197\n","Processed batch: 1890. Loss: 2.647563934326172\n","Processed batch: 1900. Loss: 2.8718411922454834\n","Processed batch: 1910. Loss: 2.816986560821533\n","Processed batch: 1920. Loss: 2.8083717823028564\n","Processed batch: 1930. Loss: 2.764108419418335\n","Processed batch: 1940. Loss: 2.9746615886688232\n","Processed batch: 1950. Loss: 3.058119535446167\n","Processed batch: 1960. Loss: 3.1108906269073486\n","Processed batch: 1970. Loss: 2.860450029373169\n","Processed batch: 1980. Loss: 3.080885410308838\n","Processed batch: 1990. Loss: 2.636108875274658\n","Processed batch: 2000. Loss: 2.821256160736084\n","Processed batch: 2010. Loss: 3.2996513843536377\n","Processed batch: 2020. Loss: 2.5179779529571533\n","Processed batch: 2030. Loss: 2.953855276107788\n","Processed batch: 2040. Loss: 3.1553847789764404\n","Processed batch: 2050. Loss: 3.288741111755371\n","Processed batch: 2060. Loss: 2.6279685497283936\n","Processed batch: 2070. Loss: 2.8040270805358887\n","Processed batch: 2080. Loss: 2.9774575233459473\n","Processed batch: 2090. Loss: 2.92040753364563\n","Processed batch: 2100. Loss: 2.9577291011810303\n","Processed batch: 2110. Loss: 2.748934030532837\n","Processed batch: 2120. Loss: 2.749738931655884\n","Processed batch: 2130. Loss: 2.679600238800049\n","Processed batch: 2140. Loss: 2.813077211380005\n","Processed batch: 2150. Loss: 2.6780917644500732\n","Processed batch: 2160. Loss: 3.0000593662261963\n","Processed batch: 2170. Loss: 3.062326431274414\n","Processed batch: 2180. Loss: 2.8978705406188965\n","Processed batch: 2190. Loss: 3.0909316539764404\n","Processed batch: 2200. Loss: 2.8444178104400635\n","Processed batch: 2210. Loss: 3.0904393196105957\n","Processed batch: 2220. Loss: 3.0007479190826416\n","Processed batch: 2230. Loss: 2.785634994506836\n","Processed batch: 2240. Loss: 3.0935075283050537\n","Processed batch: 2250. Loss: 2.8131229877471924\n","Processed batch: 2260. Loss: 2.8214120864868164\n","Processed batch: 2270. Loss: 2.9754607677459717\n","Processed batch: 2280. Loss: 2.869137763977051\n","Processed batch: 2290. Loss: 2.751192808151245\n","Processed batch: 2300. Loss: 3.090956449508667\n","Processed batch: 2310. Loss: 2.778385639190674\n","Processed batch: 2320. Loss: 3.0956108570098877\n","Processed batch: 2330. Loss: 2.794818162918091\n","Processed batch: 2340. Loss: 2.9640085697174072\n","Processed batch: 2350. Loss: 2.8891923427581787\n","Processed batch: 2360. Loss: 2.8833465576171875\n","Processed batch: 2370. Loss: 3.1511924266815186\n","Processed batch: 2380. Loss: 2.8655989170074463\n","Processed batch: 2390. Loss: 2.676055431365967\n","Processed batch: 2400. Loss: 3.1099531650543213\n","Processed batch: 2410. Loss: 2.8171885013580322\n","Processed batch: 2420. Loss: 2.793961763381958\n","Processed batch: 2430. Loss: 2.764303207397461\n","Processed batch: 2440. Loss: 3.0531907081604004\n","Processed batch: 2450. Loss: 2.853985071182251\n","Processed batch: 2460. Loss: 3.212202310562134\n","Processed batch: 2470. Loss: 2.967245578765869\n","Processed batch: 2480. Loss: 2.9968717098236084\n","Processed batch: 2490. Loss: 2.896514654159546\n","Processed batch: 2500. Loss: 2.7438323497772217\n","Processed batch: 2510. Loss: 2.803072690963745\n","Processed batch: 2520. Loss: 2.7729947566986084\n","Processed batch: 2530. Loss: 2.8033645153045654\n","Processed batch: 2540. Loss: 2.8923144340515137\n","Processed batch: 2550. Loss: 3.073775053024292\n","Processed batch: 2560. Loss: 3.0384607315063477\n","Processed batch: 2570. Loss: 2.7766852378845215\n","Processed batch: 2580. Loss: 2.6379387378692627\n","Processed batch: 2590. Loss: 2.661324977874756\n","Processed batch: 2600. Loss: 3.303997755050659\n","Processed batch: 2610. Loss: 3.0487215518951416\n","Processed batch: 2620. Loss: 3.081021547317505\n","Processed batch: 2630. Loss: 2.8113996982574463\n","Processed batch: 2640. Loss: 2.7454938888549805\n","Processed batch: 2650. Loss: 2.755420684814453\n","Processed batch: 2660. Loss: 3.1089351177215576\n","Processed batch: 2670. Loss: 2.603965997695923\n","Processed batch: 2680. Loss: 2.847351312637329\n","Processed batch: 2690. Loss: 2.8141872882843018\n","Processed batch: 2700. Loss: 3.1480987071990967\n","Processed batch: 2710. Loss: 2.796961784362793\n","Processed batch: 2720. Loss: 2.927401542663574\n","Processed batch: 2730. Loss: 2.9080331325531006\n","Processed batch: 2740. Loss: 3.187682628631592\n","Processed batch: 2750. Loss: 2.7959582805633545\n","Processed batch: 2760. Loss: 3.073258638381958\n","Processed batch: 2770. Loss: 2.8167409896850586\n","Processed batch: 2780. Loss: 2.4307477474212646\n","Processed batch: 2790. Loss: 2.6654508113861084\n","Processed batch: 2800. Loss: 2.8644516468048096\n","Processed batch: 2810. Loss: 2.9468560218811035\n","Processed batch: 2820. Loss: 2.8451004028320312\n","Processed batch: 2830. Loss: 2.9474093914031982\n","Processed batch: 2840. Loss: 2.5569849014282227\n","Processed batch: 2850. Loss: 3.0110385417938232\n","Processed batch: 2860. Loss: 2.9316000938415527\n","Processed batch: 2870. Loss: 3.1357975006103516\n","Processed batch: 2880. Loss: 3.2337961196899414\n","Processed batch: 2890. Loss: 2.905060291290283\n","Processed batch: 2900. Loss: 2.6858768463134766\n","Processed batch: 2910. Loss: 3.1069693565368652\n","Processed batch: 2920. Loss: 2.9553232192993164\n","Processed batch: 2930. Loss: 3.1674020290374756\n","Processed batch: 2940. Loss: 2.6690258979797363\n","Processed batch: 2950. Loss: 3.0845577716827393\n","Processed batch: 2960. Loss: 2.776601552963257\n","Processed batch: 2970. Loss: 2.8920176029205322\n","Processed batch: 2980. Loss: 3.0168750286102295\n","Processed batch: 2990. Loss: 2.8916423320770264\n","Processed batch: 3000. Loss: 2.9490959644317627\n","Processed batch: 3010. Loss: 2.5002238750457764\n","Processed batch: 3020. Loss: 2.9268715381622314\n","Processed batch: 3030. Loss: 3.0323615074157715\n","Processed batch: 3040. Loss: 3.1699228286743164\n","Processed batch: 3050. Loss: 2.839937448501587\n","Processed batch: 3060. Loss: 2.748263359069824\n","Processed batch: 3070. Loss: 2.689389705657959\n","Processed batch: 3080. Loss: 2.993192195892334\n","Processed batch: 3090. Loss: 3.2081832885742188\n","Processed batch: 3100. Loss: 3.1162407398223877\n","Processed batch: 3110. Loss: 3.149317979812622\n","Processed batch: 3120. Loss: 2.9962522983551025\n","Processed batch: 3130. Loss: 2.9236700534820557\n","Processed batch: 3140. Loss: 2.711862087249756\n","Processed batch: 3150. Loss: 2.9776675701141357\n","Processed batch: 3160. Loss: 3.041079521179199\n","Processed batch: 3170. Loss: 2.4691619873046875\n","Processed batch: 3180. Loss: 2.8922839164733887\n","Processed batch: 3190. Loss: 3.0706119537353516\n","Processed batch: 3200. Loss: 2.901346206665039\n","Processed batch: 3210. Loss: 2.991161584854126\n","Processed batch: 3220. Loss: 3.203051805496216\n","Processed batch: 3230. Loss: 2.7862637042999268\n","Processed batch: 3240. Loss: 2.796509265899658\n","Processed batch: 3250. Loss: 2.7743377685546875\n","Processed batch: 3260. Loss: 3.0696704387664795\n","Processed batch: 3270. Loss: 2.779914140701294\n","Processed batch: 3280. Loss: 3.061628580093384\n","Processed batch: 3290. Loss: 3.09607195854187\n","Processed batch: 3300. Loss: 2.8444855213165283\n","Processed batch: 3310. Loss: 2.8962812423706055\n","Processed batch: 3320. Loss: 2.976717233657837\n","Processed batch: 3330. Loss: 3.1564292907714844\n","Processed batch: 3340. Loss: 3.300004243850708\n","Processed batch: 3350. Loss: 3.0952131748199463\n","Processed batch: 3360. Loss: 2.719507932662964\n","Processed batch: 3370. Loss: 2.949387550354004\n","Processed batch: 3380. Loss: 2.770113229751587\n","Processed batch: 3390. Loss: 2.6086173057556152\n","Processed batch: 3400. Loss: 2.6826224327087402\n","Processed batch: 3410. Loss: 2.7047765254974365\n","Processed batch: 3420. Loss: 3.099928379058838\n","Processed batch: 3430. Loss: 3.1226208209991455\n","Processed batch: 3440. Loss: 2.5497019290924072\n","Processed batch: 3450. Loss: 2.5670480728149414\n","Processed batch: 3460. Loss: 2.824039936065674\n","Processed batch: 3470. Loss: 2.6668362617492676\n","Processed batch: 3480. Loss: 2.6004927158355713\n","Processed batch: 3490. Loss: 2.9258129596710205\n","Processed batch: 3500. Loss: 2.742587089538574\n","Processed batch: 3510. Loss: 2.954127788543701\n","Processed batch: 3520. Loss: 2.6069371700286865\n","Processed batch: 3530. Loss: 2.4647715091705322\n","Processed batch: 3540. Loss: 2.8474066257476807\n","Processed batch: 3550. Loss: 3.1283442974090576\n","Processed batch: 3560. Loss: 2.756530523300171\n","Processed batch: 3570. Loss: 3.0406248569488525\n","Processed batch: 3580. Loss: 2.59248423576355\n","Processed batch: 3590. Loss: 3.091845750808716\n","Processed batch: 3600. Loss: 2.8633604049682617\n","Processed batch: 3610. Loss: 2.961589813232422\n","Processed batch: 3620. Loss: 2.712204694747925\n","Processed batch: 3630. Loss: 2.5647876262664795\n","Processed batch: 3640. Loss: 2.5907957553863525\n","Processed batch: 3650. Loss: 2.8967249393463135\n","Processed batch: 3660. Loss: 2.8388257026672363\n","Processed batch: 3670. Loss: 2.883601427078247\n","Processed batch: 3680. Loss: 2.982395887374878\n","Processed batch: 3690. Loss: 3.0452873706817627\n","Processed batch: 3700. Loss: 2.919529914855957\n","Processed batch: 3710. Loss: 2.7348086833953857\n","Processed batch: 3720. Loss: 2.635042428970337\n","Processed batch: 3730. Loss: 2.8443784713745117\n","Processed batch: 3740. Loss: 2.751847743988037\n","Processed batch: 3750. Loss: 3.031132221221924\n","Processed batch: 3760. Loss: 2.991201400756836\n","Processed batch: 3770. Loss: 3.0430479049682617\n","Processed batch: 3780. Loss: 2.88916015625\n","Processed batch: 3790. Loss: 2.4725844860076904\n","Processed batch: 3800. Loss: 3.18745493888855\n","Processed batch: 3810. Loss: 2.9318578243255615\n","Processed batch: 3820. Loss: 2.86262583732605\n","Processed batch: 3830. Loss: 3.2328925132751465\n","Processed batch: 3840. Loss: 2.6837308406829834\n","Processed batch: 3850. Loss: 2.9140381813049316\n","Processed batch: 3860. Loss: 2.9228551387786865\n","Processed batch: 3870. Loss: 2.8038125038146973\n","Processed batch: 3880. Loss: 3.355283498764038\n","Processed batch: 3890. Loss: 2.996016502380371\n","Processed batch: 3900. Loss: 3.0637331008911133\n","Processed batch: 3910. Loss: 2.896016836166382\n","Processed batch: 3920. Loss: 2.8164567947387695\n","Processed batch: 3930. Loss: 2.968109130859375\n","Processed batch: 3940. Loss: 2.91139554977417\n","Processed batch: 3950. Loss: 2.8304145336151123\n","Processed batch: 3960. Loss: 3.2038862705230713\n","Processed batch: 3970. Loss: 2.848123073577881\n","Processed batch: 3980. Loss: 2.8309741020202637\n","Processed batch: 3990. Loss: 2.761507034301758\n","Processed batch: 4000. Loss: 3.0159196853637695\n","Processed batch: 4010. Loss: 3.2219040393829346\n","Processed batch: 4020. Loss: 2.9233150482177734\n","Processed batch: 4030. Loss: 2.778794288635254\n","Processed batch: 4040. Loss: 2.852848768234253\n","Processed batch: 4050. Loss: 2.781139850616455\n","Processed batch: 4060. Loss: 2.908026695251465\n","Processed batch: 4070. Loss: 2.8605730533599854\n","Processed batch: 4080. Loss: 3.1024272441864014\n","Processed batch: 4090. Loss: 3.1737453937530518\n","Processed batch: 4100. Loss: 2.963879346847534\n","Processed batch: 4110. Loss: 2.563936471939087\n","Processed batch: 4120. Loss: 2.80592942237854\n","Processed batch: 4130. Loss: 2.774157762527466\n","Processed batch: 4140. Loss: 2.8104279041290283\n","Processed batch: 4150. Loss: 3.0960938930511475\n","Processed batch: 4160. Loss: 2.845815420150757\n","Processed batch: 4170. Loss: 2.905179977416992\n","Processed batch: 4180. Loss: 2.9449639320373535\n","Processed batch: 4190. Loss: 2.881197452545166\n","Processed batch: 4200. Loss: 2.8909692764282227\n","Processed batch: 4210. Loss: 2.581669807434082\n","Processed batch: 4220. Loss: 3.1181318759918213\n","Processed batch: 4230. Loss: 2.6037437915802\n","Processed batch: 4240. Loss: 2.664815902709961\n","Processed batch: 4250. Loss: 2.922207832336426\n","Processed batch: 4260. Loss: 3.230619430541992\n","Processed batch: 4270. Loss: 3.056854009628296\n","Processed batch: 4280. Loss: 2.664900541305542\n","Processed batch: 4290. Loss: 2.8593897819519043\n","Processed batch: 4300. Loss: 2.8344130516052246\n","Processed batch: 4310. Loss: 2.9063613414764404\n","Processed batch: 4320. Loss: 3.016892433166504\n","Processed batch: 4330. Loss: 3.337533712387085\n","Processed batch: 4340. Loss: 2.926706552505493\n","Processed batch: 4350. Loss: 2.864243745803833\n","Processed batch: 4360. Loss: 2.7392561435699463\n","Processed batch: 4370. Loss: 2.9310755729675293\n","Processed batch: 4380. Loss: 2.7901275157928467\n","Processed batch: 4390. Loss: 3.171560287475586\n","Processed batch: 4400. Loss: 2.6809165477752686\n","Processed batch: 4410. Loss: 3.1851680278778076\n","Processed batch: 4420. Loss: 2.763824224472046\n","Processed batch: 4430. Loss: 2.6788320541381836\n","Processed batch: 4440. Loss: 2.5890274047851562\n","Processed batch: 4450. Loss: 3.069291591644287\n","Processed batch: 4460. Loss: 2.704007625579834\n","Processed batch: 4470. Loss: 3.105541467666626\n","Processed batch: 4480. Loss: 2.906399726867676\n","Processed batch: 4490. Loss: 3.023861885070801\n","Processed batch: 4500. Loss: 3.382164239883423\n","Processed batch: 4510. Loss: 3.2648799419403076\n","Processed batch: 4520. Loss: 2.413524627685547\n","Processed batch: 4530. Loss: 2.625122308731079\n","Processed batch: 4540. Loss: 2.942662477493286\n","Processed batch: 4550. Loss: 2.747544527053833\n","Processed batch: 4560. Loss: 2.8485267162323\n","Processed batch: 4570. Loss: 2.690355062484741\n","Processed batch: 4580. Loss: 3.041651487350464\n","Processed batch: 4590. Loss: 2.679774284362793\n","Processed batch: 4600. Loss: 2.7729976177215576\n","Processed batch: 4610. Loss: 2.6895158290863037\n","Processed batch: 4620. Loss: 2.4535045623779297\n","Processed batch: 4630. Loss: 2.905945062637329\n","Processed batch: 4640. Loss: 2.637115955352783\n","Processed batch: 4650. Loss: 2.8448727130889893\n","Processed batch: 4660. Loss: 2.771340847015381\n","Processed batch: 4670. Loss: 2.767591953277588\n","Processed batch: 4680. Loss: 2.711294651031494\n","Processed batch: 4690. Loss: 2.897343397140503\n","Processed batch: 4700. Loss: 2.7573468685150146\n","Processed batch: 4710. Loss: 2.444709062576294\n","Processed batch: 4720. Loss: 2.860438823699951\n","Processed batch: 4730. Loss: 3.0619008541107178\n","Processed batch: 4740. Loss: 2.945452928543091\n","Processed batch: 4750. Loss: 2.611206293106079\n","Processed batch: 4760. Loss: 2.9941580295562744\n","Processed batch: 4770. Loss: 2.6028265953063965\n","Processed batch: 4780. Loss: 2.97509765625\n","Processed batch: 4790. Loss: 2.8832180500030518\n","Processed batch: 4800. Loss: 3.023860216140747\n","Processed batch: 4810. Loss: 2.825735092163086\n","Processed batch: 4820. Loss: 2.6901185512542725\n","Processed batch: 4830. Loss: 2.7135815620422363\n","Processed batch: 4840. Loss: 3.1253654956817627\n","Processed batch: 4850. Loss: 2.819031238555908\n","Processed batch: 4860. Loss: 2.886559247970581\n","Processed batch: 4870. Loss: 3.0492100715637207\n","Processed batch: 4880. Loss: 2.9085021018981934\n","Processed batch: 4890. Loss: 3.1517302989959717\n","Processed batch: 4900. Loss: 2.906259298324585\n","Processed batch: 4910. Loss: 3.443079710006714\n","Processed batch: 4920. Loss: 2.8349547386169434\n","Processed batch: 4930. Loss: 2.755742073059082\n","Processed batch: 4940. Loss: 2.935180425643921\n","Processed batch: 4950. Loss: 2.9216179847717285\n","Processed batch: 4960. Loss: 2.7507834434509277\n","Processed batch: 4970. Loss: 2.7304279804229736\n","Processed batch: 4980. Loss: 3.0835869312286377\n","Processed batch: 4990. Loss: 3.0713796615600586\n","Processed batch: 5000. Loss: 3.1791269779205322\n","Processed batch: 5010. Loss: 2.8795857429504395\n","Epoch:1, train loss: 2.9454, time: 5165.86s\n","Processed batch: 10. Loss: 2.4113495349884033\n","Processed batch: 20. Loss: 2.615396738052368\n","Processed batch: 30. Loss: 3.1937003135681152\n","Processed batch: 40. Loss: 2.5546677112579346\n","Processed batch: 50. Loss: 2.9569644927978516\n","Processed batch: 60. Loss: 2.586395502090454\n","Processed batch: 70. Loss: 2.8115198612213135\n","Processed batch: 80. Loss: 2.9262959957122803\n","Processed batch: 90. Loss: 2.8383452892303467\n","Processed batch: 100. Loss: 2.837998151779175\n","Processed batch: 110. Loss: 3.2334327697753906\n","Processed batch: 120. Loss: 2.7622480392456055\n","Processed batch: 130. Loss: 3.0684757232666016\n","Processed batch: 140. Loss: 3.0502212047576904\n","Processed batch: 150. Loss: 2.2472946643829346\n","Processed batch: 160. Loss: 2.3922574520111084\n","Processed batch: 170. Loss: 2.7473042011260986\n","Processed batch: 180. Loss: 2.4659085273742676\n","Processed batch: 190. Loss: 2.5943849086761475\n","Processed batch: 200. Loss: 2.807981491088867\n","Processed batch: 210. Loss: 2.506624698638916\n","Processed batch: 220. Loss: 2.3667995929718018\n","Processed batch: 230. Loss: 2.639524221420288\n","Processed batch: 240. Loss: 2.7080678939819336\n","Processed batch: 250. Loss: 3.1313636302948\n","Processed batch: 260. Loss: 2.9444477558135986\n","Processed batch: 270. Loss: 2.9301674365997314\n","Processed batch: 280. Loss: 2.962085008621216\n","Processed batch: 290. Loss: 2.642857313156128\n","Processed batch: 300. Loss: 2.3051557540893555\n","Processed batch: 310. Loss: 2.580916404724121\n","Processed batch: 320. Loss: 2.773096799850464\n","Processed batch: 330. Loss: 3.135098457336426\n","Processed batch: 340. Loss: 2.708996534347534\n","Processed batch: 350. Loss: 3.031968593597412\n","Processed batch: 360. Loss: 2.8271963596343994\n","Processed batch: 370. Loss: 2.402850389480591\n","Processed batch: 380. Loss: 2.57993221282959\n","Processed batch: 390. Loss: 2.466216802597046\n","Processed batch: 400. Loss: 2.480708360671997\n","Processed batch: 410. Loss: 2.5295145511627197\n","Processed batch: 420. Loss: 2.6097140312194824\n","Processed batch: 430. Loss: 2.898405075073242\n","Processed batch: 440. Loss: 3.5578320026397705\n","Processed batch: 450. Loss: 2.8690690994262695\n","Processed batch: 460. Loss: 2.9723427295684814\n","Processed batch: 470. Loss: 2.9204163551330566\n","Processed batch: 480. Loss: 2.718214750289917\n","Processed batch: 490. Loss: 2.7917864322662354\n","Processed batch: 500. Loss: 2.8175323009490967\n","Processed batch: 510. Loss: 3.01112961769104\n","Processed batch: 520. Loss: 2.4536023139953613\n","Processed batch: 530. Loss: 2.624218702316284\n","Processed batch: 540. Loss: 3.080479145050049\n","Processed batch: 550. Loss: 2.940460681915283\n","Processed batch: 560. Loss: 2.4748733043670654\n","Processed batch: 570. Loss: 2.9254250526428223\n","Processed batch: 580. Loss: 2.535625457763672\n","Processed batch: 590. Loss: 2.49959659576416\n","Processed batch: 600. Loss: 2.7061867713928223\n","Processed batch: 610. Loss: 2.4272501468658447\n","Processed batch: 620. Loss: 2.602944850921631\n","------------------------------------------------------------\n","Epoch:1, valid loss: 2.7175, time: 205.83s\n","------------------------------------------------------------\n","Processed batch: 10. Loss: 2.3395750522613525\n","Processed batch: 20. Loss: 2.8204076290130615\n","Processed batch: 30. Loss: 2.725213050842285\n","Processed batch: 40. Loss: 2.528726816177368\n","Processed batch: 50. Loss: 2.6526596546173096\n","Processed batch: 60. Loss: 2.604417085647583\n","Processed batch: 70. Loss: 2.7870934009552\n","Processed batch: 80. Loss: 2.968390464782715\n","Processed batch: 90. Loss: 2.710810422897339\n","Processed batch: 100. Loss: 2.866647720336914\n","Processed batch: 110. Loss: 2.507308006286621\n","Processed batch: 120. Loss: 2.9309146404266357\n","Processed batch: 130. Loss: 2.9812660217285156\n","Processed batch: 140. Loss: 2.6579229831695557\n","Processed batch: 150. Loss: 2.8262925148010254\n","Processed batch: 160. Loss: 2.813472032546997\n","Processed batch: 170. Loss: 2.382449150085449\n","Processed batch: 180. Loss: 2.8851335048675537\n","Processed batch: 190. Loss: 2.52473521232605\n","Processed batch: 200. Loss: 2.6388769149780273\n","Processed batch: 210. Loss: 2.9810080528259277\n","Processed batch: 220. Loss: 2.6257686614990234\n","Processed batch: 230. Loss: 2.7001194953918457\n","Processed batch: 240. Loss: 2.6343469619750977\n","Processed batch: 250. Loss: 2.7328405380249023\n","Processed batch: 260. Loss: 2.4604668617248535\n","Processed batch: 270. Loss: 2.799960136413574\n","Processed batch: 280. Loss: 2.7752318382263184\n","Processed batch: 290. Loss: 2.6632931232452393\n","Processed batch: 300. Loss: 2.8409969806671143\n","Processed batch: 310. Loss: 2.89601731300354\n","Processed batch: 320. Loss: 2.7802882194519043\n","Processed batch: 330. Loss: 2.811642646789551\n","Processed batch: 340. Loss: 2.697901964187622\n","Processed batch: 350. Loss: 2.4654641151428223\n","Processed batch: 360. Loss: 2.6418051719665527\n","Processed batch: 370. Loss: 2.595353603363037\n","Processed batch: 380. Loss: 2.718686580657959\n","Processed batch: 390. Loss: 2.673858165740967\n","Processed batch: 400. Loss: 2.95871901512146\n","Processed batch: 410. Loss: 2.4455461502075195\n","Processed batch: 420. Loss: 2.6896204948425293\n","Processed batch: 430. Loss: 2.622236728668213\n","Processed batch: 440. Loss: 2.734513521194458\n","Processed batch: 450. Loss: 2.870436429977417\n","Processed batch: 460. Loss: 2.9603662490844727\n","Processed batch: 470. Loss: 2.760582208633423\n","Processed batch: 480. Loss: 2.7592265605926514\n","Processed batch: 490. Loss: 2.595296859741211\n","Processed batch: 500. Loss: 2.550626754760742\n","Processed batch: 510. Loss: 2.4862334728240967\n","Processed batch: 520. Loss: 2.9358816146850586\n","Processed batch: 530. Loss: 2.7050321102142334\n","Processed batch: 540. Loss: 2.6997387409210205\n","Processed batch: 550. Loss: 2.8203659057617188\n","Processed batch: 560. Loss: 2.5612168312072754\n","Processed batch: 570. Loss: 2.4936211109161377\n","Processed batch: 580. Loss: 2.517869710922241\n","Processed batch: 590. Loss: 2.8660647869110107\n","Processed batch: 600. Loss: 2.9499998092651367\n","Processed batch: 610. Loss: 2.735666275024414\n","Processed batch: 620. Loss: 2.8445487022399902\n","Processed batch: 630. Loss: 2.7158186435699463\n","Processed batch: 640. Loss: 2.7680184841156006\n","Processed batch: 650. Loss: 2.792895555496216\n","Processed batch: 660. Loss: 2.692472219467163\n","Processed batch: 670. Loss: 2.723841428756714\n","Processed batch: 680. Loss: 2.4330286979675293\n","Processed batch: 690. Loss: 2.6288022994995117\n","Processed batch: 700. Loss: 2.435549259185791\n","Processed batch: 710. Loss: 2.737623453140259\n","Processed batch: 720. Loss: 3.0242910385131836\n","Processed batch: 730. Loss: 2.676180362701416\n","Processed batch: 740. Loss: 2.437490701675415\n","Processed batch: 750. Loss: 2.3729395866394043\n","Processed batch: 760. Loss: 2.650186538696289\n","Processed batch: 770. Loss: 2.9378156661987305\n","Processed batch: 780. Loss: 2.5443289279937744\n","Processed batch: 790. Loss: 2.5262794494628906\n","Processed batch: 800. Loss: 2.804593324661255\n","Processed batch: 810. Loss: 2.4662301540374756\n","Processed batch: 820. Loss: 2.76550030708313\n","Processed batch: 830. Loss: 2.969743251800537\n","Processed batch: 840. Loss: 2.591980218887329\n","Processed batch: 850. Loss: 2.6788461208343506\n","Processed batch: 860. Loss: 2.8535211086273193\n","Processed batch: 870. Loss: 2.762009620666504\n","Processed batch: 880. Loss: 2.676236391067505\n","Processed batch: 890. Loss: 2.8324408531188965\n","Processed batch: 900. Loss: 2.4436089992523193\n","Processed batch: 910. Loss: 2.990204334259033\n","Processed batch: 920. Loss: 2.796450614929199\n","Processed batch: 930. Loss: 2.7926619052886963\n","Processed batch: 940. Loss: 2.8197457790374756\n","Processed batch: 950. Loss: 2.6157925128936768\n","Processed batch: 960. Loss: 2.832249879837036\n","Processed batch: 970. Loss: 2.9803435802459717\n","Processed batch: 980. Loss: 2.6612465381622314\n","Processed batch: 990. Loss: 3.0236096382141113\n","Processed batch: 1000. Loss: 2.831754207611084\n","Processed batch: 1010. Loss: 2.60166072845459\n","Processed batch: 1020. Loss: 2.8351573944091797\n","Processed batch: 1030. Loss: 2.6594455242156982\n","Processed batch: 1040. Loss: 2.6222245693206787\n","Processed batch: 1050. Loss: 2.685654640197754\n","Processed batch: 1060. Loss: 2.747244358062744\n","Processed batch: 1070. Loss: 2.8751492500305176\n","Processed batch: 1080. Loss: 2.7640936374664307\n","Processed batch: 1090. Loss: 2.5089175701141357\n","Processed batch: 1100. Loss: 2.350221633911133\n","Processed batch: 1110. Loss: 2.6713638305664062\n","Processed batch: 1120. Loss: 2.295161247253418\n","Processed batch: 1130. Loss: 2.4749536514282227\n","Processed batch: 1140. Loss: 2.784295082092285\n","Processed batch: 1150. Loss: 2.6432151794433594\n","Processed batch: 1160. Loss: 2.7185428142547607\n","Processed batch: 1170. Loss: 2.617781639099121\n","Processed batch: 1180. Loss: 2.509364128112793\n","Processed batch: 1190. Loss: 2.737318515777588\n","Processed batch: 1200. Loss: 3.011704921722412\n","Processed batch: 1210. Loss: 2.8158369064331055\n","Processed batch: 1220. Loss: 2.71830677986145\n","Processed batch: 1230. Loss: 2.670449733734131\n","Processed batch: 1240. Loss: 2.662440538406372\n","Processed batch: 1250. Loss: 2.9260616302490234\n","Processed batch: 1260. Loss: 2.857100009918213\n","Processed batch: 1270. Loss: 2.9147391319274902\n","Processed batch: 1280. Loss: 2.8083574771881104\n","Processed batch: 1290. Loss: 2.58394193649292\n","Processed batch: 1300. Loss: 2.6543707847595215\n","Processed batch: 1310. Loss: 2.6001687049865723\n","Processed batch: 1320. Loss: 2.6396169662475586\n","Processed batch: 1330. Loss: 2.984758138656616\n","Processed batch: 1340. Loss: 2.7283098697662354\n","Processed batch: 1350. Loss: 2.780754566192627\n","Processed batch: 1360. Loss: 2.5061256885528564\n","Processed batch: 1370. Loss: 2.637763023376465\n","Processed batch: 1380. Loss: 2.796771764755249\n","Processed batch: 1390. Loss: 2.9586236476898193\n","Processed batch: 1400. Loss: 2.6004905700683594\n","Processed batch: 1410. Loss: 2.77120304107666\n","Processed batch: 1420. Loss: 2.583578586578369\n","Processed batch: 1430. Loss: 2.681394577026367\n","Processed batch: 1440. Loss: 2.8852319717407227\n","Processed batch: 1450. Loss: 2.66339373588562\n","Processed batch: 1460. Loss: 2.8380825519561768\n","Processed batch: 1470. Loss: 2.651259422302246\n","Processed batch: 1480. Loss: 2.540701150894165\n","Processed batch: 1490. Loss: 2.7262539863586426\n","Processed batch: 1500. Loss: 2.85718035697937\n","Processed batch: 1510. Loss: 2.6130878925323486\n","Processed batch: 1520. Loss: 2.9405646324157715\n","Processed batch: 1530. Loss: 2.9967057704925537\n","Processed batch: 1540. Loss: 2.81135892868042\n","Processed batch: 1550. Loss: 2.496443748474121\n","Processed batch: 1560. Loss: 2.360058307647705\n","Processed batch: 1570. Loss: 2.4925718307495117\n","Processed batch: 1580. Loss: 2.551328420639038\n","Processed batch: 1590. Loss: 2.641997814178467\n","Processed batch: 1600. Loss: 2.588273048400879\n","Processed batch: 1610. Loss: 2.641805410385132\n","Processed batch: 1620. Loss: 2.77705454826355\n","Processed batch: 1630. Loss: 2.9532413482666016\n","Processed batch: 1640. Loss: 2.930596113204956\n","Processed batch: 1650. Loss: 2.7015953063964844\n","Processed batch: 1660. Loss: 2.5029654502868652\n","Processed batch: 1670. Loss: 2.632002592086792\n","Processed batch: 1680. Loss: 2.8818328380584717\n","Processed batch: 1690. Loss: 2.783635139465332\n","Processed batch: 1700. Loss: 2.98317551612854\n","Processed batch: 1710. Loss: 2.640136241912842\n","Processed batch: 1720. Loss: 2.676717758178711\n","Processed batch: 1730. Loss: 2.6195592880249023\n","Processed batch: 1740. Loss: 2.756542444229126\n","Processed batch: 1750. Loss: 2.741786479949951\n","Processed batch: 1760. Loss: 2.7769527435302734\n","Processed batch: 1770. Loss: 3.0235421657562256\n","Processed batch: 1780. Loss: 2.4118423461914062\n","Processed batch: 1790. Loss: 3.1148977279663086\n","Processed batch: 1800. Loss: 2.621701955795288\n","Processed batch: 1810. Loss: 2.6753289699554443\n","Processed batch: 1820. Loss: 2.723388910293579\n","Processed batch: 1830. Loss: 2.6250290870666504\n","Processed batch: 1840. Loss: 2.68327260017395\n","Processed batch: 1850. Loss: 2.473881959915161\n","Processed batch: 1860. Loss: 3.1323821544647217\n","Processed batch: 1870. Loss: 3.0097415447235107\n","Processed batch: 1880. Loss: 3.1180505752563477\n","Processed batch: 1890. Loss: 2.418316602706909\n","Processed batch: 1900. Loss: 2.730494976043701\n","Processed batch: 1910. Loss: 2.499241590499878\n","Processed batch: 1920. Loss: 2.9550373554229736\n","Processed batch: 1930. Loss: 2.77839732170105\n","Processed batch: 1940. Loss: 2.754683256149292\n","Processed batch: 1950. Loss: 2.7596919536590576\n","Processed batch: 1960. Loss: 2.841911554336548\n","Processed batch: 1970. Loss: 2.610062599182129\n","Processed batch: 1980. Loss: 2.857337236404419\n","Processed batch: 1990. Loss: 2.668254852294922\n","Processed batch: 2000. Loss: 2.462341785430908\n","Processed batch: 2010. Loss: 2.4090652465820312\n","Processed batch: 2020. Loss: 2.6891136169433594\n","Processed batch: 2030. Loss: 2.747976303100586\n","Processed batch: 2040. Loss: 2.763404607772827\n","Processed batch: 2050. Loss: 2.74177622795105\n","Processed batch: 2060. Loss: 2.6002037525177\n","Processed batch: 2070. Loss: 2.526693820953369\n","Processed batch: 2080. Loss: 2.829317808151245\n","Processed batch: 2090. Loss: 2.807276487350464\n","Processed batch: 2100. Loss: 2.97649884223938\n","Processed batch: 2110. Loss: 2.56748628616333\n","Processed batch: 2120. Loss: 2.590787172317505\n","Processed batch: 2130. Loss: 3.0547101497650146\n","Processed batch: 2140. Loss: 2.48366379737854\n","Processed batch: 2150. Loss: 2.881150007247925\n","Processed batch: 2160. Loss: 2.554459571838379\n","Processed batch: 2170. Loss: 2.4291396141052246\n","Processed batch: 2180. Loss: 2.938894271850586\n","Processed batch: 2190. Loss: 2.63944935798645\n","Processed batch: 2200. Loss: 3.1622674465179443\n","Processed batch: 2210. Loss: 2.5491137504577637\n","Processed batch: 2220. Loss: 2.8275792598724365\n","Processed batch: 2230. Loss: 2.8369619846343994\n","Processed batch: 2240. Loss: 2.7420830726623535\n","Processed batch: 2250. Loss: 2.8467373847961426\n","Processed batch: 2260. Loss: 2.717092752456665\n","Processed batch: 2270. Loss: 2.650905132293701\n","Processed batch: 2280. Loss: 3.1782734394073486\n","Processed batch: 2290. Loss: 2.3846092224121094\n","Processed batch: 2300. Loss: 2.5266366004943848\n","Processed batch: 2310. Loss: 2.642815351486206\n","Processed batch: 2320. Loss: 2.4747936725616455\n","Processed batch: 2330. Loss: 2.7957875728607178\n","Processed batch: 2340. Loss: 2.7417256832122803\n","Processed batch: 2350. Loss: 2.7562434673309326\n","Processed batch: 2360. Loss: 2.626375913619995\n","Processed batch: 2370. Loss: 2.6059560775756836\n","Processed batch: 2380. Loss: 2.7011489868164062\n","Processed batch: 2390. Loss: 2.802819013595581\n","Processed batch: 2400. Loss: 2.597076892852783\n","Processed batch: 2410. Loss: 2.8156962394714355\n","Processed batch: 2420. Loss: 2.855684518814087\n","Processed batch: 2430. Loss: 2.9680354595184326\n","Processed batch: 2440. Loss: 2.9977598190307617\n","Processed batch: 2450. Loss: 2.711977005004883\n","Processed batch: 2460. Loss: 2.9649860858917236\n","Processed batch: 2470. Loss: 2.735935926437378\n","Processed batch: 2480. Loss: 2.8137943744659424\n","Processed batch: 2490. Loss: 2.393310308456421\n","Processed batch: 2500. Loss: 2.2354514598846436\n","Processed batch: 2510. Loss: 2.7516400814056396\n","Processed batch: 2520. Loss: 2.888550281524658\n","Processed batch: 2530. Loss: 2.6386892795562744\n","Processed batch: 2540. Loss: 2.7722716331481934\n","Processed batch: 2550. Loss: 2.5919995307922363\n","Processed batch: 2560. Loss: 2.8498661518096924\n","Processed batch: 2570. Loss: 2.5206120014190674\n","Processed batch: 2580. Loss: 2.3328311443328857\n","Processed batch: 2590. Loss: 3.1200459003448486\n","Processed batch: 2600. Loss: 2.7980620861053467\n","Processed batch: 2610. Loss: 2.672743320465088\n","Processed batch: 2620. Loss: 2.9491043090820312\n","Processed batch: 2630. Loss: 2.8374929428100586\n","Processed batch: 2640. Loss: 2.754148006439209\n","Processed batch: 2650. Loss: 2.3441321849823\n","Processed batch: 2660. Loss: 2.961312770843506\n","Processed batch: 2670. Loss: 2.7559902667999268\n","Processed batch: 2680. Loss: 2.566521167755127\n","Processed batch: 2690. Loss: 3.046278715133667\n","Processed batch: 2700. Loss: 2.9296040534973145\n","Processed batch: 2710. Loss: 2.9811630249023438\n","Processed batch: 2720. Loss: 2.8150999546051025\n","Processed batch: 2730. Loss: 2.769984722137451\n","Processed batch: 2740. Loss: 2.887194871902466\n","Processed batch: 2750. Loss: 3.377314805984497\n","Processed batch: 2760. Loss: 2.8123528957366943\n","Processed batch: 2770. Loss: 2.4589250087738037\n","Processed batch: 2780. Loss: 2.3156633377075195\n","Processed batch: 2790. Loss: 2.4907121658325195\n","Processed batch: 2800. Loss: 2.9007043838500977\n","Processed batch: 2810. Loss: 2.676392078399658\n","Processed batch: 2820. Loss: 2.677659749984741\n","Processed batch: 2830. Loss: 2.56280779838562\n","Processed batch: 2840. Loss: 2.993152141571045\n","Processed batch: 2850. Loss: 2.40665340423584\n","Processed batch: 2860. Loss: 2.7427525520324707\n","Processed batch: 2870. Loss: 2.507280111312866\n","Processed batch: 2880. Loss: 2.38264536857605\n","Processed batch: 2890. Loss: 2.625915050506592\n","Processed batch: 2900. Loss: 2.9533462524414062\n","Processed batch: 2910. Loss: 2.6946892738342285\n","Processed batch: 2920. Loss: 2.9406943321228027\n","Processed batch: 2930. Loss: 2.8767826557159424\n","Processed batch: 2940. Loss: 2.873530864715576\n","Processed batch: 2950. Loss: 2.4520139694213867\n","Processed batch: 2960. Loss: 2.4167590141296387\n","Processed batch: 2970. Loss: 2.7038989067077637\n","Processed batch: 2980. Loss: 2.9632439613342285\n","Processed batch: 2990. Loss: 2.3503997325897217\n","Processed batch: 3000. Loss: 2.702197551727295\n","Processed batch: 3010. Loss: 2.7745885848999023\n","Processed batch: 3020. Loss: 2.7986292839050293\n","Processed batch: 3030. Loss: 2.773399591445923\n","Processed batch: 3040. Loss: 2.0909135341644287\n","Processed batch: 3050. Loss: 2.8101894855499268\n","Processed batch: 3060. Loss: 2.428222894668579\n","Processed batch: 3070. Loss: 2.323228120803833\n","Processed batch: 3080. Loss: 2.562519073486328\n","Processed batch: 3090. Loss: 2.7763450145721436\n","Processed batch: 3100. Loss: 2.5541303157806396\n","Processed batch: 3110. Loss: 2.5858263969421387\n","Processed batch: 3120. Loss: 2.6619324684143066\n","Processed batch: 3130. Loss: 2.6372287273406982\n","Processed batch: 3140. Loss: 2.813474178314209\n","Processed batch: 3150. Loss: 2.378847122192383\n","Processed batch: 3160. Loss: 2.5569114685058594\n","Processed batch: 3170. Loss: 2.8779146671295166\n","Processed batch: 3180. Loss: 2.5851190090179443\n","Processed batch: 3190. Loss: 2.8963606357574463\n","Processed batch: 3200. Loss: 2.792928695678711\n","Processed batch: 3210. Loss: 2.4578211307525635\n","Processed batch: 3220. Loss: 2.596618890762329\n","Processed batch: 3230. Loss: 2.695080518722534\n","Processed batch: 3240. Loss: 2.860840320587158\n","Processed batch: 3250. Loss: 2.7616429328918457\n","Processed batch: 3260. Loss: 2.8933677673339844\n","Processed batch: 3270. Loss: 2.461179733276367\n","Processed batch: 3280. Loss: 2.8850321769714355\n","Processed batch: 3290. Loss: 2.8516838550567627\n","Processed batch: 3300. Loss: 2.9562594890594482\n","Processed batch: 3310. Loss: 2.506119728088379\n","Processed batch: 3320. Loss: 3.1282949447631836\n","Processed batch: 3330. Loss: 2.690230131149292\n","Processed batch: 3340. Loss: 2.756699800491333\n","Processed batch: 3350. Loss: 2.6076505184173584\n","Processed batch: 3360. Loss: 2.832782745361328\n","Processed batch: 3370. Loss: 2.404179811477661\n","Processed batch: 3380. Loss: 2.8174984455108643\n","Processed batch: 3390. Loss: 2.583353042602539\n","Processed batch: 3400. Loss: 2.5702409744262695\n","Processed batch: 3410. Loss: 2.5781962871551514\n","Processed batch: 3420. Loss: 2.9283289909362793\n","Processed batch: 3430. Loss: 2.618891716003418\n","Processed batch: 3440. Loss: 2.7046096324920654\n","Processed batch: 3450. Loss: 2.8632562160491943\n","Processed batch: 3460. Loss: 2.6666789054870605\n","Processed batch: 3470. Loss: 2.5040886402130127\n","Processed batch: 3480. Loss: 2.8355724811553955\n","Processed batch: 3490. Loss: 2.8252758979797363\n","Processed batch: 3500. Loss: 2.4538886547088623\n","Processed batch: 3510. Loss: 2.6547958850860596\n","Processed batch: 3520. Loss: 2.756239891052246\n","Processed batch: 3530. Loss: 2.905902862548828\n","Processed batch: 3540. Loss: 2.432405471801758\n","Processed batch: 3550. Loss: 2.339273691177368\n","Processed batch: 3560. Loss: 2.8812460899353027\n","Processed batch: 3570. Loss: 2.837918519973755\n","Processed batch: 3580. Loss: 2.774231433868408\n","Processed batch: 3590. Loss: 2.6752405166625977\n","Processed batch: 3600. Loss: 2.836149215698242\n","Processed batch: 3610. Loss: 2.6585094928741455\n","Processed batch: 3620. Loss: 2.654803991317749\n","Processed batch: 3630. Loss: 2.6946284770965576\n","Processed batch: 3640. Loss: 2.7428412437438965\n","Processed batch: 3650. Loss: 3.1233603954315186\n","Processed batch: 3660. Loss: 2.7260193824768066\n","Processed batch: 3670. Loss: 2.7519171237945557\n","Processed batch: 3680. Loss: 2.3085737228393555\n","Processed batch: 3690. Loss: 2.6940548419952393\n","Processed batch: 3700. Loss: 2.6124660968780518\n","Processed batch: 3710. Loss: 2.771214723587036\n","Processed batch: 3720. Loss: 3.0958657264709473\n","Processed batch: 3730. Loss: 2.7205071449279785\n","Processed batch: 3740. Loss: 2.628872871398926\n","Processed batch: 3750. Loss: 2.52558970451355\n","Processed batch: 3760. Loss: 2.887561559677124\n","Processed batch: 3770. Loss: 2.471057653427124\n","Processed batch: 3780. Loss: 3.1531527042388916\n","Processed batch: 3790. Loss: 3.1556522846221924\n","Processed batch: 3800. Loss: 2.401407480239868\n","Processed batch: 3810. Loss: 2.4843878746032715\n","Processed batch: 3820. Loss: 2.8325159549713135\n","Processed batch: 3830. Loss: 2.753164052963257\n","Processed batch: 3840. Loss: 2.6976253986358643\n","Processed batch: 3850. Loss: 2.6630327701568604\n","Processed batch: 3860. Loss: 2.6807782649993896\n","Processed batch: 3870. Loss: 2.2773566246032715\n","Processed batch: 3880. Loss: 2.87223482131958\n","Processed batch: 3890. Loss: 2.6842830181121826\n","Processed batch: 3900. Loss: 2.5352821350097656\n","Processed batch: 3910. Loss: 2.705983877182007\n","Processed batch: 3920. Loss: 2.5932037830352783\n","Processed batch: 3930. Loss: 2.758726119995117\n","Processed batch: 3940. Loss: 2.7184817790985107\n","Processed batch: 3950. Loss: 2.4862685203552246\n","Processed batch: 3960. Loss: 2.7731049060821533\n","Processed batch: 3970. Loss: 2.7885489463806152\n","Processed batch: 3980. Loss: 2.5919594764709473\n","Processed batch: 3990. Loss: 2.467921495437622\n","Processed batch: 4000. Loss: 2.762185573577881\n","Processed batch: 4010. Loss: 2.520521640777588\n","Processed batch: 4020. Loss: 2.8901748657226562\n","Processed batch: 4030. Loss: 3.061250686645508\n","Processed batch: 4040. Loss: 2.895287275314331\n","Processed batch: 4050. Loss: 2.5448005199432373\n","Processed batch: 4060. Loss: 2.881124258041382\n","Processed batch: 4070. Loss: 2.6059255599975586\n","Processed batch: 4080. Loss: 3.023747682571411\n","Processed batch: 4090. Loss: 2.6112923622131348\n","Processed batch: 4100. Loss: 2.4437527656555176\n","Processed batch: 4110. Loss: 2.5499768257141113\n","Processed batch: 4120. Loss: 2.622818946838379\n","Processed batch: 4130. Loss: 2.525480031967163\n","Processed batch: 4140. Loss: 2.8004143238067627\n","Processed batch: 4150. Loss: 2.8114233016967773\n","Processed batch: 4160. Loss: 3.211819887161255\n","Processed batch: 4170. Loss: 2.654381036758423\n","Processed batch: 4180. Loss: 2.8451383113861084\n","Processed batch: 4190. Loss: 2.620877504348755\n","Processed batch: 4200. Loss: 2.5903148651123047\n","Processed batch: 4210. Loss: 2.7832021713256836\n","Processed batch: 4220. Loss: 3.0593576431274414\n","Processed batch: 4230. Loss: 2.606750965118408\n","Processed batch: 4240. Loss: 2.9126369953155518\n","Processed batch: 4250. Loss: 2.856729030609131\n","Processed batch: 4260. Loss: 2.629530668258667\n","Processed batch: 4270. Loss: 3.06257963180542\n","Processed batch: 4280. Loss: 2.8666815757751465\n","Processed batch: 4290. Loss: 2.4794540405273438\n","Processed batch: 4300. Loss: 2.836613893508911\n","Processed batch: 4310. Loss: 2.7595019340515137\n","Processed batch: 4320. Loss: 2.695500373840332\n","Processed batch: 4330. Loss: 2.6046149730682373\n","Processed batch: 4340. Loss: 2.6587717533111572\n","Processed batch: 4350. Loss: 2.3914873600006104\n","Processed batch: 4360. Loss: 2.593209981918335\n","Processed batch: 4370. Loss: 2.6422619819641113\n","Processed batch: 4380. Loss: 2.8635828495025635\n","Processed batch: 4390. Loss: 2.62080717086792\n","Processed batch: 4400. Loss: 2.8549747467041016\n","Processed batch: 4410. Loss: 2.7513184547424316\n","Processed batch: 4420. Loss: 3.2311818599700928\n","Processed batch: 4430. Loss: 2.8237602710723877\n","Processed batch: 4440. Loss: 2.67219877243042\n","Processed batch: 4450. Loss: 2.775472402572632\n","Processed batch: 4460. Loss: 2.899351119995117\n","Processed batch: 4470. Loss: 2.513031244277954\n","Processed batch: 4480. Loss: 3.1024818420410156\n","Processed batch: 4490. Loss: 3.0351574420928955\n","Processed batch: 4500. Loss: 2.3724048137664795\n","Processed batch: 4510. Loss: 2.631391763687134\n","Processed batch: 4520. Loss: 2.838365316390991\n","Processed batch: 4530. Loss: 2.4980413913726807\n","Processed batch: 4540. Loss: 2.5706772804260254\n","Processed batch: 4550. Loss: 2.729422092437744\n","Processed batch: 4560. Loss: 2.839531183242798\n","Processed batch: 4570. Loss: 2.4228436946868896\n","Processed batch: 4580. Loss: 2.6342196464538574\n","Processed batch: 4590. Loss: 2.591388463973999\n","Processed batch: 4600. Loss: 2.760422468185425\n","Processed batch: 4610. Loss: 2.565308094024658\n","Processed batch: 4620. Loss: 2.850806474685669\n","Processed batch: 4630. Loss: 2.505991220474243\n","Processed batch: 4640. Loss: 2.369858503341675\n","Processed batch: 4650. Loss: 2.707932949066162\n","Processed batch: 4660. Loss: 2.567364454269409\n","Processed batch: 4670. Loss: 3.059699058532715\n","Processed batch: 4680. Loss: 2.555102586746216\n","Processed batch: 4690. Loss: 2.5749411582946777\n","Processed batch: 4700. Loss: 2.724132776260376\n","Processed batch: 4710. Loss: 2.8041510581970215\n","Processed batch: 4720. Loss: 2.620526075363159\n","Processed batch: 4730. Loss: 2.9316489696502686\n","Processed batch: 4740. Loss: 2.8307838439941406\n","Processed batch: 4750. Loss: 2.945159912109375\n","Processed batch: 4760. Loss: 2.682722330093384\n","Processed batch: 4770. Loss: 2.787086009979248\n","Processed batch: 4780. Loss: 2.822291851043701\n","Processed batch: 4790. Loss: 2.670445203781128\n","Processed batch: 4800. Loss: 2.676945686340332\n","Processed batch: 4810. Loss: 2.6966381072998047\n","Processed batch: 4820. Loss: 3.03757643699646\n","Processed batch: 4830. Loss: 2.8138577938079834\n","Processed batch: 4840. Loss: 2.6524646282196045\n","Processed batch: 4850. Loss: 2.727081537246704\n","Processed batch: 4860. Loss: 2.5546844005584717\n","Processed batch: 4870. Loss: 2.6882216930389404\n","Processed batch: 4880. Loss: 3.068053960800171\n","Processed batch: 4890. Loss: 2.7568209171295166\n","Processed batch: 4900. Loss: 2.9976916313171387\n","Processed batch: 4910. Loss: 2.5964455604553223\n","Processed batch: 4920. Loss: 2.8232195377349854\n","Processed batch: 4930. Loss: 2.933764934539795\n","Processed batch: 4940. Loss: 2.6455445289611816\n","Processed batch: 4950. Loss: 2.777247667312622\n","Processed batch: 4960. Loss: 2.6923000812530518\n","Processed batch: 4970. Loss: 2.659670829772949\n","Processed batch: 4980. Loss: 2.622849464416504\n","Processed batch: 4990. Loss: 2.6307106018066406\n","Processed batch: 5000. Loss: 2.495875120162964\n","Processed batch: 5010. Loss: 2.836407423019409\n","Epoch:2, train loss: 2.7141, time: 5157.31s\n","Processed batch: 10. Loss: 2.3688278198242188\n","Processed batch: 20. Loss: 2.5615129470825195\n","Processed batch: 30. Loss: 3.173482894897461\n","Processed batch: 40. Loss: 2.5200140476226807\n","Processed batch: 50. Loss: 2.962428331375122\n","Processed batch: 60. Loss: 2.5779266357421875\n","Processed batch: 70. Loss: 2.8051888942718506\n","Processed batch: 80. Loss: 2.9047141075134277\n","Processed batch: 90. Loss: 2.8258121013641357\n","Processed batch: 100. Loss: 2.81673264503479\n","Processed batch: 110. Loss: 3.1685845851898193\n","Processed batch: 120. Loss: 2.7162022590637207\n","Processed batch: 130. Loss: 3.028341770172119\n","Processed batch: 140. Loss: 3.0347952842712402\n","Processed batch: 150. Loss: 2.229755401611328\n","Processed batch: 160. Loss: 2.342529058456421\n","Processed batch: 170. Loss: 2.7311511039733887\n","Processed batch: 180. Loss: 2.467790365219116\n","Processed batch: 190. Loss: 2.545058488845825\n","Processed batch: 200. Loss: 2.739010810852051\n","Processed batch: 210. Loss: 2.481750726699829\n","Processed batch: 220. Loss: 2.294387102127075\n","Processed batch: 230. Loss: 2.6590492725372314\n","Processed batch: 240. Loss: 2.6656298637390137\n","Processed batch: 250. Loss: 3.156977653503418\n","Processed batch: 260. Loss: 2.9363276958465576\n","Processed batch: 270. Loss: 2.9137938022613525\n","Processed batch: 280. Loss: 2.9269134998321533\n","Processed batch: 290. Loss: 2.6715192794799805\n","Processed batch: 300. Loss: 2.3151626586914062\n","Processed batch: 310. Loss: 2.56386661529541\n","Processed batch: 320. Loss: 2.783257007598877\n","Processed batch: 330. Loss: 3.11311411857605\n","Processed batch: 340. Loss: 2.6834990978240967\n","Processed batch: 350. Loss: 3.0260086059570312\n","Processed batch: 360. Loss: 2.8666679859161377\n","Processed batch: 370. Loss: 2.3401811122894287\n","Processed batch: 380. Loss: 2.495129346847534\n","Processed batch: 390. Loss: 2.4576401710510254\n","Processed batch: 400. Loss: 2.466702461242676\n","Processed batch: 410. Loss: 2.4795827865600586\n","Processed batch: 420. Loss: 2.6196627616882324\n","Processed batch: 430. Loss: 2.907782793045044\n","Processed batch: 440. Loss: 3.4929394721984863\n","Processed batch: 450. Loss: 2.8077194690704346\n","Processed batch: 460. Loss: 2.9260454177856445\n","Processed batch: 470. Loss: 2.9140663146972656\n","Processed batch: 480. Loss: 2.680297613143921\n","Processed batch: 490. Loss: 2.7730653285980225\n","Processed batch: 500. Loss: 2.7781472206115723\n","Processed batch: 510. Loss: 3.0076568126678467\n","Processed batch: 520. Loss: 2.4328341484069824\n","Processed batch: 530. Loss: 2.594181776046753\n","Processed batch: 540. Loss: 3.0639195442199707\n","Processed batch: 550. Loss: 2.903075933456421\n","Processed batch: 560. Loss: 2.4185192584991455\n","Processed batch: 570. Loss: 2.872044324874878\n","Processed batch: 580. Loss: 2.5074069499969482\n","Processed batch: 590. Loss: 2.4973747730255127\n","Processed batch: 600. Loss: 2.656858444213867\n","Processed batch: 610. Loss: 2.4129810333251953\n","Processed batch: 620. Loss: 2.567722797393799\n","------------------------------------------------------------\n","Epoch:2, valid loss: 2.6984, time: 204.26s\n","------------------------------------------------------------\n","Processed batch: 10. Loss: 2.422053098678589\n","Processed batch: 20. Loss: 2.856677532196045\n","Processed batch: 30. Loss: 2.689772844314575\n","Processed batch: 40. Loss: 2.5962395668029785\n","Processed batch: 50. Loss: 2.782834768295288\n","Processed batch: 60. Loss: 2.7822086811065674\n","Processed batch: 70. Loss: 2.5454154014587402\n","Processed batch: 80. Loss: 2.550114393234253\n","Processed batch: 90. Loss: 2.3476545810699463\n","Processed batch: 100. Loss: 2.811732292175293\n","Processed batch: 110. Loss: 2.462585210800171\n","Processed batch: 120. Loss: 2.123075008392334\n","Processed batch: 130. Loss: 2.7048068046569824\n","Processed batch: 140. Loss: 2.531765937805176\n","Processed batch: 150. Loss: 2.686824321746826\n","Processed batch: 160. Loss: 2.535369873046875\n","Processed batch: 170. Loss: 2.534910202026367\n","Processed batch: 180. Loss: 2.7045929431915283\n","Processed batch: 190. Loss: 2.4675281047821045\n","Processed batch: 200. Loss: 2.7648231983184814\n","Processed batch: 210. Loss: 2.4212708473205566\n","Processed batch: 220. Loss: 2.255563497543335\n","Processed batch: 230. Loss: 2.865375280380249\n","Processed batch: 240. Loss: 2.459233045578003\n","Processed batch: 250. Loss: 2.4357621669769287\n","Processed batch: 260. Loss: 2.3283915519714355\n","Processed batch: 270. Loss: 2.8677642345428467\n","Processed batch: 280. Loss: 2.7089807987213135\n","Processed batch: 290. Loss: 2.334207773208618\n","Processed batch: 300. Loss: 2.208714723587036\n","Processed batch: 310. Loss: 2.4745075702667236\n","Processed batch: 320. Loss: 2.45296311378479\n","Processed batch: 330. Loss: 2.479323387145996\n","Processed batch: 340. Loss: 2.4180970191955566\n","Processed batch: 350. Loss: 2.3482115268707275\n","Processed batch: 360. Loss: 2.650644063949585\n","Processed batch: 370. Loss: 2.6454684734344482\n","Processed batch: 380. Loss: 2.845092535018921\n","Processed batch: 390. Loss: 2.6574785709381104\n","Processed batch: 400. Loss: 2.479918956756592\n","Processed batch: 410. Loss: 2.3546712398529053\n","Processed batch: 420. Loss: 2.39167857170105\n","Processed batch: 430. Loss: 2.841362714767456\n","Processed batch: 440. Loss: 2.5393033027648926\n","Processed batch: 450. Loss: 2.8156580924987793\n","Processed batch: 460. Loss: 2.62290620803833\n","Processed batch: 470. Loss: 2.5419211387634277\n","Processed batch: 480. Loss: 2.722511053085327\n","Processed batch: 490. Loss: 2.446566104888916\n","Processed batch: 500. Loss: 2.56109881401062\n","Processed batch: 510. Loss: 2.4969136714935303\n","Processed batch: 520. Loss: 2.9617133140563965\n","Processed batch: 530. Loss: 2.3285815715789795\n","Processed batch: 540. Loss: 2.616909980773926\n","Processed batch: 550. Loss: 2.936384916305542\n","Processed batch: 560. Loss: 2.683133363723755\n","Processed batch: 570. Loss: 2.384923219680786\n","Processed batch: 580. Loss: 2.623737335205078\n","Processed batch: 590. Loss: 2.476991653442383\n","Processed batch: 600. Loss: 2.5318551063537598\n","Processed batch: 610. Loss: 2.789445638656616\n","Processed batch: 620. Loss: 2.6990928649902344\n","Processed batch: 630. Loss: 2.4111390113830566\n","Processed batch: 640. Loss: 2.798795223236084\n","Processed batch: 650. Loss: 2.7341434955596924\n","Processed batch: 660. Loss: 2.1864118576049805\n","Processed batch: 670. Loss: 2.573146343231201\n","Processed batch: 680. Loss: 2.3847012519836426\n","Processed batch: 690. Loss: 2.290058135986328\n","Processed batch: 700. Loss: 2.519869804382324\n","Processed batch: 710. Loss: 2.403120279312134\n","Processed batch: 720. Loss: 3.0847997665405273\n","Processed batch: 730. Loss: 2.7453627586364746\n","Processed batch: 740. Loss: 2.803117036819458\n","Processed batch: 750. Loss: 2.572632074356079\n","Processed batch: 760. Loss: 2.545224666595459\n","Processed batch: 770. Loss: 2.541260004043579\n","Processed batch: 780. Loss: 2.5014405250549316\n","Processed batch: 790. Loss: 2.5843565464019775\n","Processed batch: 800. Loss: 2.6714720726013184\n","Processed batch: 810. Loss: 2.620030164718628\n","Processed batch: 820. Loss: 2.349883794784546\n","Processed batch: 830. Loss: 2.5032215118408203\n","Processed batch: 840. Loss: 2.7606749534606934\n","Processed batch: 850. Loss: 2.373488664627075\n","Processed batch: 860. Loss: 2.5517830848693848\n","Processed batch: 870. Loss: 2.6274077892303467\n","Processed batch: 880. Loss: 2.83830189704895\n","Processed batch: 890. Loss: 2.4838476181030273\n","Processed batch: 900. Loss: 2.735750913619995\n","Processed batch: 910. Loss: 2.5763280391693115\n","Processed batch: 920. Loss: 2.4379212856292725\n","Processed batch: 930. Loss: 3.0257070064544678\n","Processed batch: 940. Loss: 2.379167318344116\n","Processed batch: 950. Loss: 2.5927278995513916\n","Processed batch: 960. Loss: 2.8331806659698486\n","Processed batch: 970. Loss: 2.486280679702759\n","Processed batch: 980. Loss: 2.5148849487304688\n","Processed batch: 990. Loss: 2.811450481414795\n","Processed batch: 1000. Loss: 2.5776100158691406\n","Processed batch: 1010. Loss: 2.588047504425049\n","Processed batch: 1020. Loss: 2.918160915374756\n","Processed batch: 1030. Loss: 2.714038610458374\n","Processed batch: 1040. Loss: 2.57974910736084\n","Processed batch: 1050. Loss: 2.4389290809631348\n","Processed batch: 1060. Loss: 2.5095527172088623\n","Processed batch: 1070. Loss: 2.4337656497955322\n","Processed batch: 1080. Loss: 2.4339520931243896\n","Processed batch: 1090. Loss: 2.2820186614990234\n","Processed batch: 1100. Loss: 2.3718464374542236\n","Processed batch: 1110. Loss: 2.724879741668701\n","Processed batch: 1120. Loss: 2.545551300048828\n","Processed batch: 1130. Loss: 2.7479658126831055\n","Processed batch: 1140. Loss: 2.601975440979004\n","Processed batch: 1150. Loss: 2.393519878387451\n","Processed batch: 1160. Loss: 2.256713390350342\n","Processed batch: 1170. Loss: 2.499427080154419\n","Processed batch: 1180. Loss: 2.4682321548461914\n","Processed batch: 1190. Loss: 2.8009259700775146\n","Processed batch: 1200. Loss: 2.5509440898895264\n","Processed batch: 1210. Loss: 2.777285099029541\n","Processed batch: 1220. Loss: 2.7212460041046143\n","Processed batch: 1230. Loss: 3.0274977684020996\n","Processed batch: 1240. Loss: 2.601318597793579\n","Processed batch: 1250. Loss: 2.35091495513916\n","Processed batch: 1260. Loss: 2.680403232574463\n","Processed batch: 1270. Loss: 2.9726173877716064\n","Processed batch: 1280. Loss: 2.7630631923675537\n","Processed batch: 1290. Loss: 2.628704071044922\n","Processed batch: 1300. Loss: 2.557894706726074\n","Processed batch: 1310. Loss: 2.346013069152832\n","Processed batch: 1320. Loss: 2.323626756668091\n","Processed batch: 1330. Loss: 2.44596529006958\n","Processed batch: 1340. Loss: 2.273347854614258\n","Processed batch: 1350. Loss: 2.6754236221313477\n","Processed batch: 1360. Loss: 2.5172250270843506\n","Processed batch: 1370. Loss: 2.5113508701324463\n","Processed batch: 1380. Loss: 2.254384756088257\n","Processed batch: 1390. Loss: 2.6260251998901367\n","Processed batch: 1400. Loss: 2.5413286685943604\n","Processed batch: 1410. Loss: 2.323531150817871\n","Processed batch: 1420. Loss: 2.6300055980682373\n","Processed batch: 1430. Loss: 2.330846071243286\n","Processed batch: 1440. Loss: 2.7336416244506836\n","Processed batch: 1450. Loss: 2.5725724697113037\n","Processed batch: 1460. Loss: 2.636678457260132\n","Processed batch: 1470. Loss: 2.2736172676086426\n","Processed batch: 1480. Loss: 2.630629777908325\n","Processed batch: 1490. Loss: 2.5345869064331055\n","Processed batch: 1500. Loss: 2.4873361587524414\n","Processed batch: 1510. Loss: 2.687774658203125\n","Processed batch: 1520. Loss: 2.3433923721313477\n","Processed batch: 1530. Loss: 2.6695001125335693\n","Processed batch: 1540. Loss: 2.618985414505005\n","Processed batch: 1550. Loss: 2.641083240509033\n","Processed batch: 1560. Loss: 2.6013383865356445\n","Processed batch: 1570. Loss: 2.3701276779174805\n","Processed batch: 1580. Loss: 2.709782361984253\n","Processed batch: 1590. Loss: 2.448312520980835\n","Processed batch: 1600. Loss: 2.5541434288024902\n","Processed batch: 1610. Loss: 2.140909194946289\n","Processed batch: 1620. Loss: 2.459139347076416\n","Processed batch: 1630. Loss: 2.6550180912017822\n","Processed batch: 1640. Loss: 2.504070281982422\n","Processed batch: 1650. Loss: 2.4945523738861084\n","Processed batch: 1660. Loss: 2.8461952209472656\n","Processed batch: 1670. Loss: 2.35372257232666\n","Processed batch: 1680. Loss: 2.4434118270874023\n","Processed batch: 1690. Loss: 2.6684021949768066\n","Processed batch: 1700. Loss: 2.626441717147827\n","Processed batch: 1710. Loss: 2.6403143405914307\n","Processed batch: 1720. Loss: 2.5129687786102295\n","Processed batch: 1730. Loss: 2.6192073822021484\n","Processed batch: 1740. Loss: 2.2865443229675293\n","Processed batch: 1750. Loss: 2.5681416988372803\n","Processed batch: 1760. Loss: 2.5317633152008057\n","Processed batch: 1770. Loss: 2.7427852153778076\n","Processed batch: 1780. Loss: 2.561119794845581\n","Processed batch: 1790. Loss: 3.037787914276123\n","Processed batch: 1800. Loss: 2.51847767829895\n","Processed batch: 1810. Loss: 2.872098207473755\n","Processed batch: 1820. Loss: 2.4466609954833984\n","Processed batch: 1830. Loss: 2.4790215492248535\n","Processed batch: 1840. Loss: 2.1924993991851807\n","Processed batch: 1850. Loss: 2.8200509548187256\n","Processed batch: 1860. Loss: 2.3694093227386475\n","Processed batch: 1870. Loss: 2.462177038192749\n","Processed batch: 1880. Loss: 2.485833168029785\n","Processed batch: 1890. Loss: 2.4243218898773193\n","Processed batch: 1900. Loss: 2.267198324203491\n","Processed batch: 1910. Loss: 2.6392178535461426\n","Processed batch: 1920. Loss: 2.5519816875457764\n","Processed batch: 1930. Loss: 2.6857481002807617\n","Processed batch: 1940. Loss: 2.617475748062134\n","Processed batch: 1950. Loss: 2.2518889904022217\n","Processed batch: 1960. Loss: 2.6837410926818848\n","Processed batch: 1970. Loss: 2.639586925506592\n","Processed batch: 1980. Loss: 2.5652449131011963\n","Processed batch: 1990. Loss: 2.5507972240448\n","Processed batch: 2000. Loss: 2.301905632019043\n","Processed batch: 2010. Loss: 2.800231456756592\n","Processed batch: 2020. Loss: 2.61834454536438\n","Processed batch: 2030. Loss: 2.7993271350860596\n","Processed batch: 2040. Loss: 2.4624810218811035\n","Processed batch: 2050. Loss: 2.373684883117676\n","Processed batch: 2060. Loss: 2.5460360050201416\n","Processed batch: 2070. Loss: 2.4077703952789307\n","Processed batch: 2080. Loss: 2.673111915588379\n","Processed batch: 2090. Loss: 2.8226468563079834\n","Processed batch: 2100. Loss: 2.2737619876861572\n","Processed batch: 2110. Loss: 2.56827974319458\n","Processed batch: 2120. Loss: 2.5142579078674316\n","Processed batch: 2130. Loss: 2.718101978302002\n","Processed batch: 2140. Loss: 2.4741873741149902\n","Processed batch: 2150. Loss: 2.782015800476074\n","Processed batch: 2160. Loss: 2.794813394546509\n","Processed batch: 2170. Loss: 2.454214572906494\n","Processed batch: 2180. Loss: 2.761725664138794\n","Processed batch: 2190. Loss: 2.601229429244995\n","Processed batch: 2200. Loss: 2.437556505203247\n","Processed batch: 2210. Loss: 2.4912900924682617\n","Processed batch: 2220. Loss: 2.2955737113952637\n","Processed batch: 2230. Loss: 3.0981760025024414\n","Processed batch: 2240. Loss: 2.703078508377075\n","Processed batch: 2250. Loss: 2.316746711730957\n","Processed batch: 2260. Loss: 2.6193463802337646\n","Processed batch: 2270. Loss: 2.7528250217437744\n","Processed batch: 2280. Loss: 2.4243738651275635\n","Processed batch: 2290. Loss: 2.3956871032714844\n","Processed batch: 2300. Loss: 2.656810998916626\n","Processed batch: 2310. Loss: 2.5310142040252686\n","Processed batch: 2320. Loss: 2.3055624961853027\n","Processed batch: 2330. Loss: 2.6355395317077637\n","Processed batch: 2340. Loss: 2.3889827728271484\n","Processed batch: 2350. Loss: 2.6120476722717285\n","Processed batch: 2360. Loss: 2.7392518520355225\n","Processed batch: 2370. Loss: 2.7008583545684814\n","Processed batch: 2380. Loss: 2.3894386291503906\n","Processed batch: 2390. Loss: 2.6900970935821533\n","Processed batch: 2400. Loss: 2.2106423377990723\n","Processed batch: 2410. Loss: 2.5860605239868164\n","Processed batch: 2420. Loss: 2.518918991088867\n","Processed batch: 2430. Loss: 2.4343667030334473\n","Processed batch: 2440. Loss: 2.4007277488708496\n","Processed batch: 2450. Loss: 2.943068265914917\n","Processed batch: 2460. Loss: 2.433307409286499\n","Processed batch: 2470. Loss: 2.4312267303466797\n","Processed batch: 2480. Loss: 2.3417015075683594\n","Processed batch: 2490. Loss: 2.332869052886963\n","Processed batch: 2500. Loss: 2.783668279647827\n","Processed batch: 2510. Loss: 2.5861799716949463\n","Processed batch: 2520. Loss: 2.9342331886291504\n","Processed batch: 2530. Loss: 2.745781660079956\n","Processed batch: 2540. Loss: 2.7117626667022705\n","Processed batch: 2550. Loss: 2.622528076171875\n","Processed batch: 2560. Loss: 2.2720909118652344\n","Processed batch: 2570. Loss: 2.472503185272217\n","Processed batch: 2580. Loss: 2.548330545425415\n","Processed batch: 2590. Loss: 2.860079526901245\n","Processed batch: 2600. Loss: 2.4130258560180664\n","Processed batch: 2610. Loss: 2.6107826232910156\n","Processed batch: 2620. Loss: 2.5595712661743164\n","Processed batch: 2630. Loss: 2.499558210372925\n","Processed batch: 2640. Loss: 2.3808083534240723\n","Processed batch: 2650. Loss: 2.475830316543579\n","Processed batch: 2660. Loss: 2.7873613834381104\n","Processed batch: 2670. Loss: 2.321096897125244\n","Processed batch: 2680. Loss: 2.5168514251708984\n","Processed batch: 2690. Loss: 2.8552439212799072\n","Processed batch: 2700. Loss: 2.642002582550049\n","Processed batch: 2710. Loss: 2.4385058879852295\n","Processed batch: 2720. Loss: 2.520066976547241\n","Processed batch: 2730. Loss: 2.4278104305267334\n","Processed batch: 2740. Loss: 2.397434949874878\n","Processed batch: 2750. Loss: 2.512958526611328\n","Processed batch: 2760. Loss: 2.6789886951446533\n","Processed batch: 2770. Loss: 2.6875414848327637\n","Processed batch: 2780. Loss: 2.6584932804107666\n","Processed batch: 2790. Loss: 2.717041254043579\n","Processed batch: 2800. Loss: 2.664391040802002\n","Processed batch: 2810. Loss: 2.667306661605835\n","Processed batch: 2820. Loss: 2.755873918533325\n","Processed batch: 2830. Loss: 2.742401361465454\n","Processed batch: 2840. Loss: 2.476146697998047\n","Processed batch: 2850. Loss: 2.7016658782958984\n","Processed batch: 2860. Loss: 2.7565181255340576\n","Processed batch: 2870. Loss: 2.2915053367614746\n","Processed batch: 2880. Loss: 2.4843556880950928\n","Processed batch: 2890. Loss: 2.719930648803711\n","Processed batch: 2900. Loss: 2.595519781112671\n","Processed batch: 2910. Loss: 2.585000991821289\n","Processed batch: 2920. Loss: 2.726130723953247\n","Processed batch: 2930. Loss: 2.8250064849853516\n","Processed batch: 2940. Loss: 2.517110586166382\n","Processed batch: 2950. Loss: 2.694110631942749\n","Processed batch: 2960. Loss: 2.3586297035217285\n","Processed batch: 2970. Loss: 2.7124853134155273\n","Processed batch: 2980. Loss: 2.37560772895813\n","Processed batch: 2990. Loss: 2.1802940368652344\n","Processed batch: 3000. Loss: 2.5009968280792236\n","Processed batch: 3010. Loss: 2.7024996280670166\n","Processed batch: 3020. Loss: 2.482189655303955\n","Processed batch: 3030. Loss: 2.85139799118042\n","Processed batch: 3040. Loss: 2.4009013175964355\n","Processed batch: 3050. Loss: 2.4611735343933105\n","Processed batch: 3060. Loss: 2.426987886428833\n","Processed batch: 3070. Loss: 2.586846351623535\n","Processed batch: 3080. Loss: 2.767397403717041\n","Processed batch: 3090. Loss: 2.2496261596679688\n","Processed batch: 3100. Loss: 2.371967077255249\n","Processed batch: 3110. Loss: 2.440293788909912\n","Processed batch: 3120. Loss: 2.250077486038208\n","Processed batch: 3130. Loss: 2.7098143100738525\n","Processed batch: 3140. Loss: 2.6172373294830322\n","Processed batch: 3150. Loss: 2.6642725467681885\n","Processed batch: 3160. Loss: 2.5624942779541016\n","Processed batch: 3170. Loss: 2.318991184234619\n","Processed batch: 3180. Loss: 2.6252808570861816\n","Processed batch: 3190. Loss: 2.4540345668792725\n","Processed batch: 3200. Loss: 2.7989933490753174\n","Processed batch: 3210. Loss: 2.472590923309326\n","Processed batch: 3220. Loss: 2.633532762527466\n","Processed batch: 3230. Loss: 2.4665026664733887\n","Processed batch: 3240. Loss: 2.750009298324585\n","Processed batch: 3250. Loss: 2.4647693634033203\n","Processed batch: 3260. Loss: 2.491450786590576\n","Processed batch: 3270. Loss: 2.7962560653686523\n","Processed batch: 3280. Loss: 2.6393675804138184\n","Processed batch: 3290. Loss: 2.2387948036193848\n","Processed batch: 3300. Loss: 2.5787441730499268\n","Processed batch: 3310. Loss: 2.5934157371520996\n","Processed batch: 3320. Loss: 2.440181255340576\n","Processed batch: 3330. Loss: 2.3847129344940186\n","Processed batch: 3340. Loss: 2.2391254901885986\n","Processed batch: 3350. Loss: 2.4087629318237305\n","Processed batch: 3360. Loss: 2.6159138679504395\n","Processed batch: 3370. Loss: 2.6682491302490234\n","Processed batch: 3380. Loss: 2.070697069168091\n","Processed batch: 3390. Loss: 2.8400979042053223\n","Processed batch: 3400. Loss: 2.80637264251709\n","Processed batch: 3410. Loss: 2.4948976039886475\n","Processed batch: 3420. Loss: 2.3529930114746094\n","Processed batch: 3430. Loss: 2.547926902770996\n","Processed batch: 3440. Loss: 2.816272258758545\n","Processed batch: 3450. Loss: 2.351806879043579\n","Processed batch: 3460. Loss: 2.4795145988464355\n","Processed batch: 3470. Loss: 2.528144598007202\n","Processed batch: 3480. Loss: 2.6461620330810547\n","Processed batch: 3490. Loss: 2.683501720428467\n","Processed batch: 3500. Loss: 2.576139211654663\n","Processed batch: 3510. Loss: 2.5128276348114014\n","Processed batch: 3520. Loss: 2.886007070541382\n","Processed batch: 3530. Loss: 2.6321136951446533\n","Processed batch: 3540. Loss: 2.7791800498962402\n","Processed batch: 3550. Loss: 2.826059103012085\n","Processed batch: 3560. Loss: 2.38061261177063\n","Processed batch: 3570. Loss: 2.429682493209839\n","Processed batch: 3580. Loss: 2.8219869136810303\n","Processed batch: 3590. Loss: 2.6995716094970703\n","Processed batch: 3600. Loss: 2.454775094985962\n","Processed batch: 3610. Loss: 2.630901575088501\n","Processed batch: 3620. Loss: 2.580188035964966\n","Processed batch: 3630. Loss: 2.5259366035461426\n","Processed batch: 3640. Loss: 2.707733154296875\n","Processed batch: 3650. Loss: 2.4971401691436768\n","Processed batch: 3660. Loss: 2.5775156021118164\n","Processed batch: 3670. Loss: 2.654801368713379\n","Processed batch: 3680. Loss: 2.4243013858795166\n","Processed batch: 3690. Loss: 2.35825252532959\n","Processed batch: 3700. Loss: 2.434990167617798\n","Processed batch: 3710. Loss: 2.7177083492279053\n","Processed batch: 3720. Loss: 2.5877573490142822\n","Processed batch: 3730. Loss: 2.5606131553649902\n","Processed batch: 3740. Loss: 2.667214870452881\n","Processed batch: 3750. Loss: 2.7611896991729736\n","Processed batch: 3760. Loss: 2.2721128463745117\n","Processed batch: 3770. Loss: 2.6089048385620117\n","Processed batch: 3780. Loss: 2.5159080028533936\n","Processed batch: 3790. Loss: 2.6479458808898926\n","Processed batch: 3800. Loss: 2.352775812149048\n","Processed batch: 3810. Loss: 2.713714838027954\n","Processed batch: 3820. Loss: 2.6514673233032227\n","Processed batch: 3830. Loss: 2.6951584815979004\n","Processed batch: 3840. Loss: 2.583376884460449\n","Processed batch: 3850. Loss: 2.9348769187927246\n","Processed batch: 3860. Loss: 2.808183193206787\n","Processed batch: 3870. Loss: 2.6259329319000244\n","Processed batch: 3880. Loss: 2.583144426345825\n","Processed batch: 3890. Loss: 2.4860787391662598\n","Processed batch: 3900. Loss: 2.806856393814087\n","Processed batch: 3910. Loss: 2.486469030380249\n","Processed batch: 3920. Loss: 2.590470552444458\n","Processed batch: 3930. Loss: 2.6889445781707764\n","Processed batch: 3940. Loss: 2.842634439468384\n","Processed batch: 3950. Loss: 2.7119762897491455\n","Processed batch: 3960. Loss: 2.2939951419830322\n","Processed batch: 3970. Loss: 2.5547444820404053\n","Processed batch: 3980. Loss: 2.700828790664673\n","Processed batch: 3990. Loss: 2.823025703430176\n","Processed batch: 4000. Loss: 2.670666217803955\n","Processed batch: 4010. Loss: 2.4712107181549072\n","Processed batch: 4020. Loss: 2.3539721965789795\n","Processed batch: 4030. Loss: 2.442481517791748\n","Processed batch: 4040. Loss: 2.715280771255493\n","Processed batch: 4050. Loss: 2.675386905670166\n","Processed batch: 4060. Loss: 2.3641645908355713\n","Processed batch: 4070. Loss: 2.6467461585998535\n","Processed batch: 4080. Loss: 2.6113524436950684\n","Processed batch: 4090. Loss: 2.5108532905578613\n","Processed batch: 4100. Loss: 2.6752309799194336\n","Processed batch: 4110. Loss: 2.423755168914795\n","Processed batch: 4120. Loss: 2.4029312133789062\n","Processed batch: 4130. Loss: 2.373032808303833\n","Processed batch: 4140. Loss: 2.63519024848938\n","Processed batch: 4150. Loss: 2.7681667804718018\n","Processed batch: 4160. Loss: 2.648198366165161\n","Processed batch: 4170. Loss: 2.892439126968384\n","Processed batch: 4180. Loss: 2.564742088317871\n","Processed batch: 4190. Loss: 2.4711859226226807\n","Processed batch: 4200. Loss: 2.438493490219116\n","Processed batch: 4210. Loss: 2.572998285293579\n","Processed batch: 4220. Loss: 2.876960277557373\n","Processed batch: 4230. Loss: 2.4616687297821045\n","Processed batch: 4240. Loss: 2.513957977294922\n","Processed batch: 4250. Loss: 2.4929652214050293\n","Processed batch: 4260. Loss: 2.7800967693328857\n","Processed batch: 4270. Loss: 2.686490774154663\n","Processed batch: 4280. Loss: 2.3495373725891113\n","Processed batch: 4290. Loss: 2.3096492290496826\n","Processed batch: 4300. Loss: 2.3944692611694336\n","Processed batch: 4310. Loss: 2.636420249938965\n","Processed batch: 4320. Loss: 2.475027322769165\n","Processed batch: 4330. Loss: 2.649343252182007\n","Processed batch: 4340. Loss: 2.473689079284668\n","Processed batch: 4350. Loss: 2.498826742172241\n","Processed batch: 4360. Loss: 2.988565683364868\n","Processed batch: 4370. Loss: 2.5005710124969482\n","Processed batch: 4380. Loss: 2.5490992069244385\n","Processed batch: 4390. Loss: 2.6395976543426514\n","Processed batch: 4400. Loss: 2.6986968517303467\n","Processed batch: 4410. Loss: 2.5656161308288574\n","Processed batch: 4420. Loss: 2.5218448638916016\n","Processed batch: 4430. Loss: 2.467833995819092\n","Processed batch: 4440. Loss: 2.2820372581481934\n","Processed batch: 4450. Loss: 2.850006103515625\n","Processed batch: 4460. Loss: 2.399838924407959\n","Processed batch: 4470. Loss: 2.4019243717193604\n","Processed batch: 4480. Loss: 2.686826705932617\n","Processed batch: 4490. Loss: 2.275041103363037\n","Processed batch: 4500. Loss: 2.4803736209869385\n","Processed batch: 4510. Loss: 2.7644975185394287\n","Processed batch: 4520. Loss: 2.323978900909424\n","Processed batch: 4530. Loss: 2.4929990768432617\n","Processed batch: 4540. Loss: 2.393195867538452\n","Processed batch: 4550. Loss: 2.5665745735168457\n","Processed batch: 4560. Loss: 2.7698404788970947\n","Processed batch: 4570. Loss: 2.541316032409668\n","Processed batch: 4580. Loss: 2.4781856536865234\n","Processed batch: 4590. Loss: 2.9599483013153076\n","Processed batch: 4600. Loss: 2.42010235786438\n","Processed batch: 4610. Loss: 2.47816801071167\n","Processed batch: 4620. Loss: 2.7554547786712646\n","Processed batch: 4630. Loss: 2.529590368270874\n","Processed batch: 4640. Loss: 2.675175428390503\n","Processed batch: 4650. Loss: 2.9420230388641357\n","Processed batch: 4660. Loss: 2.4094457626342773\n","Processed batch: 4670. Loss: 2.4080111980438232\n","Processed batch: 4680. Loss: 2.880666494369507\n","Processed batch: 4690. Loss: 2.8180153369903564\n","Processed batch: 4700. Loss: 2.634181499481201\n","Processed batch: 4710. Loss: 2.5227460861206055\n","Processed batch: 4720. Loss: 2.8170218467712402\n","Processed batch: 4730. Loss: 2.8311996459960938\n","Processed batch: 4740. Loss: 2.578636884689331\n","Processed batch: 4750. Loss: 3.0217766761779785\n","Processed batch: 4760. Loss: 2.823122262954712\n","Processed batch: 4770. Loss: 2.609327793121338\n","Processed batch: 4780. Loss: 2.7890825271606445\n","Processed batch: 4790. Loss: 2.6943180561065674\n","Processed batch: 4800. Loss: 2.7527668476104736\n","Processed batch: 4810. Loss: 2.5011112689971924\n","Processed batch: 4820. Loss: 2.479757785797119\n","Processed batch: 4830. Loss: 2.3379335403442383\n","Processed batch: 4840. Loss: 2.8372387886047363\n","Processed batch: 4850. Loss: 2.5730748176574707\n","Processed batch: 4860. Loss: 2.480433702468872\n","Processed batch: 4870. Loss: 2.622933864593506\n","Processed batch: 4880. Loss: 2.636889934539795\n","Processed batch: 4890. Loss: 2.3798627853393555\n","Processed batch: 4900. Loss: 2.4080052375793457\n","Processed batch: 4910. Loss: 2.5401601791381836\n","Processed batch: 4920. Loss: 2.680199384689331\n","Processed batch: 4930. Loss: 2.634436845779419\n","Processed batch: 4940. Loss: 2.3385305404663086\n","Processed batch: 4950. Loss: 2.415773391723633\n","Processed batch: 4960. Loss: 2.645254611968994\n","Processed batch: 4970. Loss: 2.6064846515655518\n","Processed batch: 4980. Loss: 2.265963554382324\n","Processed batch: 4990. Loss: 2.8610565662384033\n","Processed batch: 5000. Loss: 2.68739914894104\n","Processed batch: 5010. Loss: 2.2913448810577393\n","Epoch:3, train loss: 2.5726, time: 5160.96s\n","Processed batch: 10. Loss: 2.3354272842407227\n","Processed batch: 20. Loss: 2.591191291809082\n","Processed batch: 30. Loss: 3.175318956375122\n","Processed batch: 40. Loss: 2.522094488143921\n","Processed batch: 50. Loss: 2.996762752532959\n","Processed batch: 60. Loss: 2.595032215118408\n","Processed batch: 70. Loss: 2.784269332885742\n","Processed batch: 80. Loss: 2.8794326782226562\n","Processed batch: 90. Loss: 2.84734845161438\n","Processed batch: 100. Loss: 2.8220374584198\n","Processed batch: 110. Loss: 3.1723968982696533\n","Processed batch: 120. Loss: 2.722778558731079\n","Processed batch: 130. Loss: 3.1003799438476562\n","Processed batch: 140. Loss: 3.01963472366333\n","Processed batch: 150. Loss: 2.219109296798706\n","Processed batch: 160. Loss: 2.3251171112060547\n","Processed batch: 170. Loss: 2.725661277770996\n","Processed batch: 180. Loss: 2.4561660289764404\n","Processed batch: 190. Loss: 2.554676055908203\n","Processed batch: 200. Loss: 2.705138921737671\n","Processed batch: 210. Loss: 2.459339141845703\n","Processed batch: 220. Loss: 2.3470640182495117\n","Processed batch: 230. Loss: 2.656376838684082\n","Processed batch: 240. Loss: 2.632025718688965\n","Processed batch: 250. Loss: 3.152031421661377\n","Processed batch: 260. Loss: 2.9535000324249268\n","Processed batch: 270. Loss: 2.910588502883911\n","Processed batch: 280. Loss: 2.9489357471466064\n","Processed batch: 290. Loss: 2.717102527618408\n","Processed batch: 300. Loss: 2.363558053970337\n","Processed batch: 310. Loss: 2.5022974014282227\n","Processed batch: 320. Loss: 2.749741315841675\n","Processed batch: 330. Loss: 3.133754253387451\n","Processed batch: 340. Loss: 2.6866555213928223\n","Processed batch: 350. Loss: 3.0048439502716064\n","Processed batch: 360. Loss: 2.8010213375091553\n","Processed batch: 370. Loss: 2.365360975265503\n","Processed batch: 380. Loss: 2.4974617958068848\n","Processed batch: 390. Loss: 2.4724557399749756\n","Processed batch: 400. Loss: 2.416672706604004\n","Processed batch: 410. Loss: 2.4374260902404785\n","Processed batch: 420. Loss: 2.6264030933380127\n","Processed batch: 430. Loss: 2.8659048080444336\n","Processed batch: 440. Loss: 3.5536153316497803\n","Processed batch: 450. Loss: 2.8363513946533203\n","Processed batch: 460. Loss: 2.9574739933013916\n","Processed batch: 470. Loss: 2.925668478012085\n","Processed batch: 480. Loss: 2.6363327503204346\n","Processed batch: 490. Loss: 2.821561574935913\n","Processed batch: 500. Loss: 2.772143840789795\n","Processed batch: 510. Loss: 3.0295002460479736\n","Processed batch: 520. Loss: 2.381802797317505\n","Processed batch: 530. Loss: 2.605276584625244\n","Processed batch: 540. Loss: 3.079479455947876\n","Processed batch: 550. Loss: 2.8817245960235596\n","Processed batch: 560. Loss: 2.369229793548584\n","Processed batch: 570. Loss: 2.855569839477539\n","Processed batch: 580. Loss: 2.4802393913269043\n","Processed batch: 590. Loss: 2.5233547687530518\n","Processed batch: 600. Loss: 2.6829845905303955\n","Processed batch: 610. Loss: 2.4094536304473877\n","Processed batch: 620. Loss: 2.558210611343384\n","------------------------------------------------------------\n","Epoch:3, valid loss: 2.6953, time: 204.06s\n","------------------------------------------------------------\n","Processed batch: 10. Loss: 2.3142781257629395\n","Processed batch: 20. Loss: 2.46227765083313\n","Processed batch: 30. Loss: 2.4835262298583984\n","Processed batch: 40. Loss: 2.4286961555480957\n","Processed batch: 50. Loss: 2.26888108253479\n","Processed batch: 60. Loss: 2.229079484939575\n","Processed batch: 70. Loss: 2.2986345291137695\n","Processed batch: 80. Loss: 2.3131678104400635\n","Processed batch: 90. Loss: 2.263042449951172\n","Processed batch: 100. Loss: 2.614868402481079\n","Processed batch: 110. Loss: 2.5096607208251953\n","Processed batch: 120. Loss: 2.357635021209717\n","Processed batch: 130. Loss: 2.37119722366333\n","Processed batch: 140. Loss: 2.3348476886749268\n","Processed batch: 150. Loss: 2.236057758331299\n","Processed batch: 160. Loss: 2.670416831970215\n","Processed batch: 170. Loss: 2.2484195232391357\n","Processed batch: 180. Loss: 2.703265905380249\n","Processed batch: 190. Loss: 2.3904099464416504\n","Processed batch: 200. Loss: 2.4867141246795654\n","Processed batch: 210. Loss: 2.4085910320281982\n","Processed batch: 220. Loss: 2.4565329551696777\n","Processed batch: 230. Loss: 2.3771002292633057\n","Processed batch: 240. Loss: 2.4403045177459717\n","Processed batch: 250. Loss: 2.0498671531677246\n","Processed batch: 260. Loss: 2.5310728549957275\n","Processed batch: 270. Loss: 2.3969919681549072\n","Processed batch: 280. Loss: 2.3512279987335205\n","Processed batch: 290. Loss: 2.702194929122925\n","Processed batch: 300. Loss: 2.294402599334717\n","Processed batch: 310. Loss: 2.6426355838775635\n","Processed batch: 320. Loss: 2.380419969558716\n","Processed batch: 330. Loss: 2.497591257095337\n","Processed batch: 340. Loss: 2.5187530517578125\n","Processed batch: 350. Loss: 2.2462503910064697\n","Processed batch: 360. Loss: 2.343363046646118\n","Processed batch: 370. Loss: 2.2664926052093506\n","Processed batch: 380. Loss: 2.3377368450164795\n","Processed batch: 390. Loss: 2.4581594467163086\n","Processed batch: 400. Loss: 2.771793842315674\n","Processed batch: 410. Loss: 2.306109666824341\n","Processed batch: 420. Loss: 2.522052764892578\n","Processed batch: 430. Loss: 2.4450488090515137\n","Processed batch: 440. Loss: 2.425776958465576\n","Processed batch: 450. Loss: 2.566213607788086\n","Processed batch: 460. Loss: 2.3607099056243896\n","Processed batch: 470. Loss: 2.4000353813171387\n","Processed batch: 480. Loss: 2.465592384338379\n","Processed batch: 490. Loss: 2.5289695262908936\n","Processed batch: 500. Loss: 2.0358200073242188\n","Processed batch: 510. Loss: 2.4162347316741943\n","Processed batch: 520. Loss: 2.486751079559326\n","Processed batch: 530. Loss: 2.6669223308563232\n","Processed batch: 540. Loss: 2.289714813232422\n","Processed batch: 550. Loss: 2.4086923599243164\n","Processed batch: 560. Loss: 2.4168334007263184\n","Processed batch: 570. Loss: 2.420529842376709\n","Processed batch: 580. Loss: 2.148561716079712\n","Processed batch: 590. Loss: 2.5425777435302734\n","Processed batch: 600. Loss: 2.433014154434204\n","Processed batch: 610. Loss: 2.458237886428833\n","Processed batch: 620. Loss: 2.17535400390625\n","Processed batch: 630. Loss: 2.6862103939056396\n","Processed batch: 640. Loss: 2.697124719619751\n","Processed batch: 650. Loss: 2.443516492843628\n","Processed batch: 660. Loss: 2.6064553260803223\n","Processed batch: 670. Loss: 2.4135453701019287\n","Processed batch: 680. Loss: 2.6175806522369385\n","Processed batch: 690. Loss: 2.5983920097351074\n","Processed batch: 700. Loss: 2.2984001636505127\n","Processed batch: 710. Loss: 2.6851561069488525\n","Processed batch: 720. Loss: 2.177691698074341\n","Processed batch: 730. Loss: 2.241882562637329\n","Processed batch: 740. Loss: 2.524366617202759\n","Processed batch: 750. Loss: 2.3227198123931885\n","Processed batch: 760. Loss: 2.3686256408691406\n","Processed batch: 770. Loss: 2.3592185974121094\n","Processed batch: 780. Loss: 2.5157766342163086\n","Processed batch: 790. Loss: 2.3327138423919678\n","Processed batch: 800. Loss: 2.292928695678711\n","Processed batch: 810. Loss: 2.4176580905914307\n","Processed batch: 820. Loss: 2.2237679958343506\n","Processed batch: 830. Loss: 2.5147957801818848\n","Processed batch: 840. Loss: 2.5249671936035156\n","Processed batch: 850. Loss: 2.6280438899993896\n","Processed batch: 860. Loss: 2.364046573638916\n","Processed batch: 870. Loss: 2.500993251800537\n","Processed batch: 880. Loss: 2.194441556930542\n","Processed batch: 890. Loss: 2.3004698753356934\n","Processed batch: 900. Loss: 2.033280372619629\n","Processed batch: 910. Loss: 2.28204607963562\n","Processed batch: 920. Loss: 2.3695147037506104\n","Processed batch: 930. Loss: 2.55704927444458\n","Processed batch: 940. Loss: 2.545811414718628\n","Processed batch: 950. Loss: 2.6661057472229004\n","Processed batch: 960. Loss: 2.4917750358581543\n","Processed batch: 970. Loss: 2.5682411193847656\n","Processed batch: 980. Loss: 2.6007206439971924\n","Processed batch: 990. Loss: 2.3577792644500732\n","Processed batch: 1000. Loss: 2.2007243633270264\n","Processed batch: 1010. Loss: 2.2626636028289795\n","Processed batch: 1020. Loss: 2.5486841201782227\n","Processed batch: 1030. Loss: 2.3717150688171387\n","Processed batch: 1040. Loss: 2.6088144779205322\n","Processed batch: 1050. Loss: 2.306342124938965\n","Processed batch: 1060. Loss: 2.215416193008423\n","Processed batch: 1070. Loss: 2.1426451206207275\n","Processed batch: 1080. Loss: 2.5195047855377197\n","Processed batch: 1090. Loss: 2.5005102157592773\n","Processed batch: 1100. Loss: 2.583510637283325\n","Processed batch: 1110. Loss: 2.3611843585968018\n","Processed batch: 1120. Loss: 2.545677900314331\n","Processed batch: 1130. Loss: 2.587932825088501\n","Processed batch: 1140. Loss: 2.2512972354888916\n","Processed batch: 1150. Loss: 2.3540992736816406\n","Processed batch: 1160. Loss: 2.655982732772827\n","Processed batch: 1170. Loss: 2.2723941802978516\n","Processed batch: 1180. Loss: 2.425567865371704\n","Processed batch: 1190. Loss: 2.418933868408203\n","Processed batch: 1200. Loss: 2.3400092124938965\n","Processed batch: 1210. Loss: 2.562159538269043\n","Processed batch: 1220. Loss: 2.489743232727051\n","Processed batch: 1230. Loss: 2.5696563720703125\n","Processed batch: 1240. Loss: 2.41485333442688\n","Processed batch: 1250. Loss: 2.330237627029419\n","Processed batch: 1260. Loss: 2.498764991760254\n","Processed batch: 1270. Loss: 2.255528211593628\n","Processed batch: 1280. Loss: 2.4893620014190674\n","Processed batch: 1290. Loss: 2.349454402923584\n","Processed batch: 1300. Loss: 2.4862916469573975\n","Processed batch: 1310. Loss: 2.526313066482544\n","Processed batch: 1320. Loss: 2.664224863052368\n","Processed batch: 1330. Loss: 2.499786853790283\n","Processed batch: 1340. Loss: 2.540282964706421\n","Processed batch: 1350. Loss: 2.504685163497925\n","Processed batch: 1360. Loss: 2.801637649536133\n","Processed batch: 1370. Loss: 2.684644937515259\n","Processed batch: 1380. Loss: 2.593531608581543\n","Processed batch: 1390. Loss: 2.459686756134033\n","Processed batch: 1400. Loss: 2.5852041244506836\n","Processed batch: 1410. Loss: 2.754302501678467\n","Processed batch: 1420. Loss: 2.556295394897461\n","Processed batch: 1430. Loss: 2.268052339553833\n","Processed batch: 1440. Loss: 2.267871856689453\n","Processed batch: 1450. Loss: 2.393078327178955\n","Processed batch: 1460. Loss: 2.412262201309204\n","Processed batch: 1470. Loss: 2.321709632873535\n","Processed batch: 1480. Loss: 2.4267632961273193\n","Processed batch: 1490. Loss: 2.3288192749023438\n","Processed batch: 1500. Loss: 2.471926689147949\n","Processed batch: 1510. Loss: 2.4553208351135254\n","Processed batch: 1520. Loss: 2.4377634525299072\n","Processed batch: 1530. Loss: 2.3379604816436768\n","Processed batch: 1540. Loss: 2.309634208679199\n","Processed batch: 1550. Loss: 2.675980567932129\n","Processed batch: 1560. Loss: 2.1763768196105957\n","Processed batch: 1570. Loss: 2.3740999698638916\n","Processed batch: 1580. Loss: 2.778864622116089\n","Processed batch: 1590. Loss: 2.261201858520508\n","Processed batch: 1600. Loss: 2.590989112854004\n","Processed batch: 1610. Loss: 2.41684627532959\n","Processed batch: 1620. Loss: 2.386401653289795\n","Processed batch: 1630. Loss: 2.4318389892578125\n","Processed batch: 1640. Loss: 2.637373208999634\n","Processed batch: 1650. Loss: 2.386369228363037\n","Processed batch: 1660. Loss: 2.596794366836548\n","Processed batch: 1670. Loss: 2.2884254455566406\n","Processed batch: 1680. Loss: 2.4414381980895996\n","Processed batch: 1690. Loss: 2.2910876274108887\n","Processed batch: 1700. Loss: 2.4491875171661377\n","Processed batch: 1710. Loss: 2.2304599285125732\n","Processed batch: 1720. Loss: 2.477491617202759\n","Processed batch: 1730. Loss: 2.375927448272705\n","Processed batch: 1740. Loss: 2.3811380863189697\n","Processed batch: 1750. Loss: 2.589972972869873\n","Processed batch: 1760. Loss: 2.372931480407715\n","Processed batch: 1770. Loss: 2.71921706199646\n","Processed batch: 1780. Loss: 2.3020269870758057\n","Processed batch: 1790. Loss: 2.219236373901367\n","Processed batch: 1800. Loss: 2.316136121749878\n","Processed batch: 1810. Loss: 2.523980140686035\n","Processed batch: 1820. Loss: 2.5471110343933105\n","Processed batch: 1830. Loss: 2.542938232421875\n","Processed batch: 1840. Loss: 2.707089424133301\n","Processed batch: 1850. Loss: 2.725498676300049\n","Processed batch: 1860. Loss: 2.2136220932006836\n","Processed batch: 1870. Loss: 2.528895854949951\n","Processed batch: 1880. Loss: 2.628598690032959\n","Processed batch: 1890. Loss: 2.422964572906494\n","Processed batch: 1900. Loss: 2.165426731109619\n","Processed batch: 1910. Loss: 2.5186386108398438\n","Processed batch: 1920. Loss: 2.7531378269195557\n","Processed batch: 1930. Loss: 2.26475191116333\n","Processed batch: 1940. Loss: 2.252384901046753\n","Processed batch: 1950. Loss: 2.5460875034332275\n","Processed batch: 1960. Loss: 2.838815927505493\n","Processed batch: 1970. Loss: 2.269895553588867\n","Processed batch: 1980. Loss: 2.634767532348633\n","Processed batch: 1990. Loss: 2.5238571166992188\n","Processed batch: 2000. Loss: 2.124605655670166\n","Processed batch: 2010. Loss: 2.4643068313598633\n","Processed batch: 2020. Loss: 2.5412752628326416\n","Processed batch: 2030. Loss: 2.378485918045044\n","Processed batch: 2040. Loss: 2.2977309226989746\n","Processed batch: 2050. Loss: 2.233711004257202\n","Processed batch: 2060. Loss: 2.6996216773986816\n","Processed batch: 2070. Loss: 2.2052292823791504\n","Processed batch: 2080. Loss: 2.365946054458618\n","Processed batch: 2090. Loss: 2.2908589839935303\n","Processed batch: 2100. Loss: 2.4309871196746826\n","Processed batch: 2110. Loss: 2.4954445362091064\n","Processed batch: 2120. Loss: 2.333834409713745\n","Processed batch: 2130. Loss: 2.6956920623779297\n","Processed batch: 2140. Loss: 2.2087559700012207\n","Processed batch: 2150. Loss: 2.6626760959625244\n","Processed batch: 2160. Loss: 2.5293142795562744\n","Processed batch: 2170. Loss: 2.3809313774108887\n","Processed batch: 2180. Loss: 2.356409788131714\n","Processed batch: 2190. Loss: 2.6893527507781982\n","Processed batch: 2200. Loss: 2.13506817817688\n","Processed batch: 2210. Loss: 2.2535572052001953\n","Processed batch: 2220. Loss: 2.450834274291992\n","Processed batch: 2230. Loss: 2.612605094909668\n","Processed batch: 2240. Loss: 2.695059061050415\n","Processed batch: 2250. Loss: 2.488008975982666\n","Processed batch: 2260. Loss: 2.666685104370117\n","Processed batch: 2270. Loss: 2.426391363143921\n","Processed batch: 2280. Loss: 2.69376802444458\n","Processed batch: 2290. Loss: 2.4232277870178223\n","Processed batch: 2300. Loss: 2.466557502746582\n","Processed batch: 2310. Loss: 2.2602877616882324\n","Processed batch: 2320. Loss: 2.305835008621216\n","Processed batch: 2330. Loss: 2.3835830688476562\n","Processed batch: 2340. Loss: 2.57597017288208\n","Processed batch: 2350. Loss: 2.468275547027588\n","Processed batch: 2360. Loss: 2.5245227813720703\n","Processed batch: 2370. Loss: 2.6147773265838623\n","Processed batch: 2380. Loss: 2.537943124771118\n","Processed batch: 2390. Loss: 2.4636030197143555\n","Processed batch: 2400. Loss: 2.5381720066070557\n","Processed batch: 2410. Loss: 2.4754674434661865\n","Processed batch: 2420. Loss: 2.4713363647460938\n","Processed batch: 2430. Loss: 2.0839016437530518\n","Processed batch: 2440. Loss: 2.5127627849578857\n","Processed batch: 2450. Loss: 2.515002489089966\n","Processed batch: 2460. Loss: 2.5469350814819336\n","Processed batch: 2470. Loss: 2.4209649562835693\n","Processed batch: 2480. Loss: 2.593472957611084\n","Processed batch: 2490. Loss: 2.8204455375671387\n","Processed batch: 2500. Loss: 2.785017967224121\n","Processed batch: 2510. Loss: 2.41070818901062\n","Processed batch: 2520. Loss: 2.7236204147338867\n","Processed batch: 2530. Loss: 2.2927279472351074\n","Processed batch: 2540. Loss: 2.281205892562866\n","Processed batch: 2550. Loss: 2.4498977661132812\n","Processed batch: 2560. Loss: 2.4390368461608887\n","Processed batch: 2570. Loss: 2.2847228050231934\n","Processed batch: 2580. Loss: 2.151355504989624\n","Processed batch: 2590. Loss: 2.4674692153930664\n","Processed batch: 2600. Loss: 2.544658899307251\n","Processed batch: 2610. Loss: 2.5745437145233154\n","Processed batch: 2620. Loss: 2.3029820919036865\n","Processed batch: 2630. Loss: 2.4349730014801025\n","Processed batch: 2640. Loss: 2.4034173488616943\n","Processed batch: 2650. Loss: 2.2436158657073975\n","Processed batch: 2660. Loss: 2.5116353034973145\n","Processed batch: 2670. Loss: 2.63067364692688\n","Processed batch: 2680. Loss: 2.3711419105529785\n","Processed batch: 2690. Loss: 2.6000490188598633\n","Processed batch: 2700. Loss: 2.5755367279052734\n","Processed batch: 2710. Loss: 2.4696292877197266\n","Processed batch: 2720. Loss: 2.292314291000366\n","Processed batch: 2730. Loss: 2.5794289112091064\n","Processed batch: 2740. Loss: 2.717477321624756\n","Processed batch: 2750. Loss: 2.513338804244995\n","Processed batch: 2760. Loss: 2.5063509941101074\n","Processed batch: 2770. Loss: 2.293120861053467\n","Processed batch: 2780. Loss: 2.0323801040649414\n","Processed batch: 2790. Loss: 2.2723259925842285\n","Processed batch: 2800. Loss: 2.521233081817627\n","Processed batch: 2810. Loss: 2.390174388885498\n","Processed batch: 2820. Loss: 2.3908934593200684\n","Processed batch: 2830. Loss: 2.1819026470184326\n","Processed batch: 2840. Loss: 2.933001756668091\n","Processed batch: 2850. Loss: 2.237457513809204\n","Processed batch: 2860. Loss: 2.4040117263793945\n","Processed batch: 2870. Loss: 2.5954651832580566\n","Processed batch: 2880. Loss: 2.5014524459838867\n","Processed batch: 2890. Loss: 2.475172996520996\n","Processed batch: 2900. Loss: 2.424482822418213\n","Processed batch: 2910. Loss: 2.636310577392578\n","Processed batch: 2920. Loss: 2.520878314971924\n","Processed batch: 2930. Loss: 2.365902900695801\n","Processed batch: 2940. Loss: 2.5616605281829834\n","Processed batch: 2950. Loss: 2.6056768894195557\n","Processed batch: 2960. Loss: 2.4091150760650635\n","Processed batch: 2970. Loss: 2.268561601638794\n","Processed batch: 2980. Loss: 2.6068568229675293\n","Processed batch: 2990. Loss: 2.4826271533966064\n","Processed batch: 3000. Loss: 2.579587936401367\n","Processed batch: 3010. Loss: 2.664461612701416\n","Processed batch: 3020. Loss: 2.4554378986358643\n","Processed batch: 3030. Loss: 2.1811132431030273\n","Processed batch: 3040. Loss: 2.377606153488159\n","Processed batch: 3050. Loss: 2.5282835960388184\n","Processed batch: 3060. Loss: 2.2717084884643555\n","Processed batch: 3070. Loss: 2.438830614089966\n","Processed batch: 3080. Loss: 2.3192248344421387\n","Processed batch: 3090. Loss: 2.51163649559021\n","Processed batch: 3100. Loss: 2.7082550525665283\n","Processed batch: 3110. Loss: 2.3810434341430664\n","Processed batch: 3120. Loss: 2.403115749359131\n","Processed batch: 3130. Loss: 2.733931541442871\n","Processed batch: 3140. Loss: 2.345773458480835\n","Processed batch: 3150. Loss: 2.329272747039795\n","Processed batch: 3160. Loss: 2.755234479904175\n","Processed batch: 3170. Loss: 2.5355312824249268\n","Processed batch: 3180. Loss: 2.4857168197631836\n","Processed batch: 3190. Loss: 2.201667308807373\n","Processed batch: 3200. Loss: 2.5551352500915527\n","Processed batch: 3210. Loss: 2.4126479625701904\n","Processed batch: 3220. Loss: 2.5302255153656006\n","Processed batch: 3230. Loss: 2.4868271350860596\n","Processed batch: 3240. Loss: 2.3817577362060547\n","Processed batch: 3250. Loss: 2.7351765632629395\n","Processed batch: 3260. Loss: 2.3259406089782715\n","Processed batch: 3270. Loss: 2.2376604080200195\n","Processed batch: 3280. Loss: 2.412419557571411\n","Processed batch: 3290. Loss: 2.3518879413604736\n","Processed batch: 3300. Loss: 2.2478740215301514\n","Processed batch: 3310. Loss: 2.4024810791015625\n","Processed batch: 3320. Loss: 2.432344436645508\n","Processed batch: 3330. Loss: 2.806940793991089\n","Processed batch: 3340. Loss: 2.6530494689941406\n","Processed batch: 3350. Loss: 2.268954038619995\n","Processed batch: 3360. Loss: 2.0346224308013916\n","Processed batch: 3370. Loss: 2.5489959716796875\n","Processed batch: 3380. Loss: 2.720794200897217\n","Processed batch: 3390. Loss: 2.5194544792175293\n","Processed batch: 3400. Loss: 2.6329586505889893\n","Processed batch: 3410. Loss: 2.406115770339966\n","Processed batch: 3420. Loss: 2.33239483833313\n","Processed batch: 3430. Loss: 2.7903575897216797\n","Processed batch: 3440. Loss: 2.473093271255493\n","Processed batch: 3450. Loss: 2.4608349800109863\n","Processed batch: 3460. Loss: 2.3692445755004883\n","Processed batch: 3470. Loss: 2.765939712524414\n","Processed batch: 3480. Loss: 2.462346076965332\n","Processed batch: 3490. Loss: 2.717947483062744\n","Processed batch: 3500. Loss: 2.5898211002349854\n","Processed batch: 3510. Loss: 2.718665599822998\n","Processed batch: 3520. Loss: 2.440061569213867\n","Processed batch: 3530. Loss: 2.231107234954834\n","Processed batch: 3540. Loss: 2.3641531467437744\n","Processed batch: 3550. Loss: 2.3757004737854004\n","Processed batch: 3560. Loss: 2.530254364013672\n","Processed batch: 3570. Loss: 2.492234706878662\n","Processed batch: 3580. Loss: 2.6451144218444824\n","Processed batch: 3590. Loss: 2.624753713607788\n","Processed batch: 3600. Loss: 2.300550699234009\n","Processed batch: 3610. Loss: 2.5619871616363525\n","Processed batch: 3620. Loss: 2.3787434101104736\n","Processed batch: 3630. Loss: 2.4229936599731445\n","Processed batch: 3640. Loss: 2.9286224842071533\n","Processed batch: 3650. Loss: 2.5212600231170654\n","Processed batch: 3660. Loss: 2.439453601837158\n","Processed batch: 3670. Loss: 2.383692979812622\n","Processed batch: 3680. Loss: 2.2941884994506836\n","Processed batch: 3690. Loss: 2.5882444381713867\n","Processed batch: 3700. Loss: 2.3466787338256836\n","Processed batch: 3710. Loss: 2.1119184494018555\n","Processed batch: 3720. Loss: 2.141005277633667\n","Processed batch: 3730. Loss: 2.6358449459075928\n","Processed batch: 3740. Loss: 2.284432888031006\n","Processed batch: 3750. Loss: 2.390965461730957\n","Processed batch: 3760. Loss: 2.3872199058532715\n","Processed batch: 3770. Loss: 2.325099468231201\n","Processed batch: 3780. Loss: 2.3455698490142822\n","Processed batch: 3790. Loss: 2.6690683364868164\n","Processed batch: 3800. Loss: 2.6006903648376465\n","Processed batch: 3810. Loss: 2.491633415222168\n","Processed batch: 3820. Loss: 2.742121696472168\n","Processed batch: 3830. Loss: 2.734837055206299\n","Processed batch: 3840. Loss: 2.4718668460845947\n","Processed batch: 3850. Loss: 2.241798162460327\n","Processed batch: 3860. Loss: 2.543836832046509\n","Processed batch: 3870. Loss: 2.5706961154937744\n","Processed batch: 3880. Loss: 2.4122393131256104\n","Processed batch: 3890. Loss: 2.5957159996032715\n","Processed batch: 3900. Loss: 2.3651139736175537\n","Processed batch: 3910. Loss: 2.5108399391174316\n","Processed batch: 3920. Loss: 2.552846908569336\n","Processed batch: 3930. Loss: 2.52287220954895\n","Processed batch: 3940. Loss: 2.2346456050872803\n","Processed batch: 3950. Loss: 2.5174648761749268\n","Processed batch: 3960. Loss: 2.7603821754455566\n","Processed batch: 3970. Loss: 2.56020188331604\n","Processed batch: 3980. Loss: 2.317514181137085\n","Processed batch: 3990. Loss: 2.060939311981201\n","Processed batch: 4000. Loss: 2.515085220336914\n","Processed batch: 4010. Loss: 2.547689437866211\n","Processed batch: 4020. Loss: 2.663607120513916\n","Processed batch: 4030. Loss: 2.4107797145843506\n","Processed batch: 4040. Loss: 2.389676570892334\n","Processed batch: 4050. Loss: 2.209017515182495\n","Processed batch: 4060. Loss: 2.183192253112793\n","Processed batch: 4070. Loss: 2.6825103759765625\n","Processed batch: 4080. Loss: 2.623884916305542\n","Processed batch: 4090. Loss: 2.187006711959839\n","Processed batch: 4100. Loss: 2.648571252822876\n","Processed batch: 4110. Loss: 2.385427236557007\n","Processed batch: 4120. Loss: 2.3302907943725586\n","Processed batch: 4130. Loss: 2.3090734481811523\n","Processed batch: 4140. Loss: 2.7083349227905273\n","Processed batch: 4150. Loss: 2.2795393466949463\n","Processed batch: 4160. Loss: 2.7739315032958984\n","Processed batch: 4170. Loss: 2.292179822921753\n","Processed batch: 4180. Loss: 2.9840190410614014\n","Processed batch: 4190. Loss: 2.2879865169525146\n","Processed batch: 4200. Loss: 2.6675314903259277\n","Processed batch: 4210. Loss: 2.4252476692199707\n","Processed batch: 4220. Loss: 2.715322256088257\n","Processed batch: 4230. Loss: 2.5929524898529053\n","Processed batch: 4240. Loss: 2.441901445388794\n","Processed batch: 4250. Loss: 2.738563299179077\n","Processed batch: 4260. Loss: 2.238642454147339\n","Processed batch: 4270. Loss: 2.367342710494995\n","Processed batch: 4280. Loss: 2.256173610687256\n","Processed batch: 4290. Loss: 2.3833680152893066\n","Processed batch: 4300. Loss: 2.4786601066589355\n","Processed batch: 4310. Loss: 2.2679500579833984\n","Processed batch: 4320. Loss: 2.4002890586853027\n","Processed batch: 4330. Loss: 2.416861057281494\n","Processed batch: 4340. Loss: 2.804868459701538\n","Processed batch: 4350. Loss: 2.4178426265716553\n","Processed batch: 4360. Loss: 2.2824602127075195\n","Processed batch: 4370. Loss: 2.4293971061706543\n","Processed batch: 4380. Loss: 2.5422756671905518\n","Processed batch: 4390. Loss: 2.5123584270477295\n","Processed batch: 4400. Loss: 2.718029737472534\n","Processed batch: 4410. Loss: 2.6584534645080566\n","Processed batch: 4420. Loss: 2.4013044834136963\n","Processed batch: 4430. Loss: 2.3864123821258545\n","Processed batch: 4440. Loss: 2.6247034072875977\n","Processed batch: 4450. Loss: 2.8384711742401123\n","Processed batch: 4460. Loss: 2.373070478439331\n","Processed batch: 4470. Loss: 2.4136598110198975\n","Processed batch: 4480. Loss: 2.6466338634490967\n","Processed batch: 4490. Loss: 2.7030539512634277\n","Processed batch: 4500. Loss: 2.6758615970611572\n","Processed batch: 4510. Loss: 2.5316717624664307\n","Processed batch: 4520. Loss: 2.3609166145324707\n","Processed batch: 4530. Loss: 2.637463331222534\n","Processed batch: 4540. Loss: 2.5742123126983643\n","Processed batch: 4550. Loss: 2.2195937633514404\n","Processed batch: 4560. Loss: 2.361926555633545\n","Processed batch: 4570. Loss: 2.6555070877075195\n","Processed batch: 4580. Loss: 2.239929437637329\n","Processed batch: 4590. Loss: 2.517099380493164\n","Processed batch: 4600. Loss: 2.326467514038086\n","Processed batch: 4610. Loss: 2.8621020317077637\n","Processed batch: 4620. Loss: 2.392984628677368\n","Processed batch: 4630. Loss: 2.308014392852783\n","Processed batch: 4640. Loss: 2.3029253482818604\n","Processed batch: 4650. Loss: 2.3562889099121094\n","Processed batch: 4660. Loss: 2.6007802486419678\n","Processed batch: 4670. Loss: 2.3181567192077637\n","Processed batch: 4680. Loss: 2.288240909576416\n","Processed batch: 4690. Loss: 2.229008674621582\n","Processed batch: 4700. Loss: 2.4125306606292725\n","Processed batch: 4710. Loss: 2.391106605529785\n","Processed batch: 4720. Loss: 2.6501553058624268\n","Processed batch: 4730. Loss: 2.335582971572876\n","Processed batch: 4740. Loss: 2.319133996963501\n","Processed batch: 4750. Loss: 2.35241436958313\n","Processed batch: 4760. Loss: 2.3100085258483887\n","Processed batch: 4770. Loss: 2.289337158203125\n","Processed batch: 4780. Loss: 2.419860363006592\n","Processed batch: 4790. Loss: 2.544060468673706\n","Processed batch: 4800. Loss: 2.744584083557129\n","Processed batch: 4810. Loss: 2.559690475463867\n","Processed batch: 4820. Loss: 2.6349375247955322\n","Processed batch: 4830. Loss: 2.6714835166931152\n","Processed batch: 4840. Loss: 2.3091001510620117\n","Processed batch: 4850. Loss: 2.67724871635437\n","Processed batch: 4860. Loss: 2.3959264755249023\n","Processed batch: 4870. Loss: 2.5503313541412354\n","Processed batch: 4880. Loss: 2.3741023540496826\n","Processed batch: 4890. Loss: 2.1518044471740723\n","Processed batch: 4900. Loss: 2.499061346054077\n","Processed batch: 4910. Loss: 2.4512972831726074\n","Processed batch: 4920. Loss: 2.614933729171753\n","Processed batch: 4930. Loss: 2.4644720554351807\n","Processed batch: 4940. Loss: 2.608656167984009\n","Processed batch: 4950. Loss: 2.863635301589966\n","Processed batch: 4960. Loss: 2.27361798286438\n","Processed batch: 4970. Loss: 2.5511462688446045\n","Processed batch: 4980. Loss: 2.4049439430236816\n","Processed batch: 4990. Loss: 2.667666435241699\n","Processed batch: 5000. Loss: 2.643272876739502\n","Processed batch: 5010. Loss: 2.6576147079467773\n","Epoch:4, train loss: 2.4525, time: 5142.03s\n","Processed batch: 10. Loss: 2.3926122188568115\n","Processed batch: 20. Loss: 2.5672483444213867\n","Processed batch: 30. Loss: 3.156284809112549\n","Processed batch: 40. Loss: 2.495792865753174\n","Processed batch: 50. Loss: 3.0068130493164062\n","Processed batch: 60. Loss: 2.5652177333831787\n","Processed batch: 70. Loss: 2.791252374649048\n","Processed batch: 80. Loss: 2.852334499359131\n","Processed batch: 90. Loss: 2.863981246948242\n","Processed batch: 100. Loss: 2.7793238162994385\n","Processed batch: 110. Loss: 3.231750249862671\n","Processed batch: 120. Loss: 2.7266762256622314\n","Processed batch: 130. Loss: 3.0916237831115723\n","Processed batch: 140. Loss: 3.058011054992676\n","Processed batch: 150. Loss: 2.1953253746032715\n","Processed batch: 160. Loss: 2.376723289489746\n","Processed batch: 170. Loss: 2.743950843811035\n","Processed batch: 180. Loss: 2.4581081867218018\n","Processed batch: 190. Loss: 2.5704421997070312\n","Processed batch: 200. Loss: 2.7596657276153564\n","Processed batch: 210. Loss: 2.4655134677886963\n","Processed batch: 220. Loss: 2.316910982131958\n","Processed batch: 230. Loss: 2.6676254272460938\n","Processed batch: 240. Loss: 2.6715662479400635\n","Processed batch: 250. Loss: 3.126944065093994\n","Processed batch: 260. Loss: 2.9760351181030273\n","Processed batch: 270. Loss: 2.8947131633758545\n","Processed batch: 280. Loss: 2.961732864379883\n","Processed batch: 290. Loss: 2.693296194076538\n","Processed batch: 300. Loss: 2.369340181350708\n","Processed batch: 310. Loss: 2.4982059001922607\n","Processed batch: 320. Loss: 2.7370569705963135\n","Processed batch: 330. Loss: 3.16384220123291\n","Processed batch: 340. Loss: 2.721611261367798\n","Processed batch: 350. Loss: 3.0232861042022705\n","Processed batch: 360. Loss: 2.8380494117736816\n","Processed batch: 370. Loss: 2.348717451095581\n","Processed batch: 380. Loss: 2.494310140609741\n","Processed batch: 390. Loss: 2.4687721729278564\n","Processed batch: 400. Loss: 2.4221088886260986\n","Processed batch: 410. Loss: 2.4673240184783936\n","Processed batch: 420. Loss: 2.6724741458892822\n","Processed batch: 430. Loss: 2.927889823913574\n","Processed batch: 440. Loss: 3.5660247802734375\n","Processed batch: 450. Loss: 2.86543607711792\n","Processed batch: 460. Loss: 2.9171504974365234\n","Processed batch: 470. Loss: 2.899674415588379\n","Processed batch: 480. Loss: 2.6527774333953857\n","Processed batch: 490. Loss: 2.8018949031829834\n","Processed batch: 500. Loss: 2.7994768619537354\n","Processed batch: 510. Loss: 3.0497167110443115\n","Processed batch: 520. Loss: 2.436326026916504\n","Processed batch: 530. Loss: 2.6057307720184326\n","Processed batch: 540. Loss: 3.131333827972412\n","Processed batch: 550. Loss: 2.892319440841675\n","Processed batch: 560. Loss: 2.341299057006836\n","Processed batch: 570. Loss: 2.864187240600586\n","Processed batch: 580. Loss: 2.5055387020111084\n","Processed batch: 590. Loss: 2.5429911613464355\n","Processed batch: 600. Loss: 2.7110748291015625\n","Processed batch: 610. Loss: 2.4352469444274902\n","Processed batch: 620. Loss: 2.5537140369415283\n","------------------------------------------------------------\n","Epoch:4, valid loss: 2.7059, time: 204.43s\n","------------------------------------------------------------\n","Processed batch: 10. Loss: 2.1808347702026367\n","Processed batch: 20. Loss: 2.30135440826416\n","Processed batch: 30. Loss: 2.4257664680480957\n","Processed batch: 40. Loss: 2.3742752075195312\n","Processed batch: 50. Loss: 2.437349319458008\n","Processed batch: 60. Loss: 2.2119579315185547\n","Processed batch: 70. Loss: 2.3318710327148438\n","Processed batch: 80. Loss: 2.3229522705078125\n","Processed batch: 90. Loss: 2.0481081008911133\n","Processed batch: 100. Loss: 2.538618564605713\n","Processed batch: 110. Loss: 2.298576831817627\n","Processed batch: 120. Loss: 2.1726059913635254\n","Processed batch: 130. Loss: 2.3790271282196045\n","Processed batch: 140. Loss: 2.4557456970214844\n","Processed batch: 150. Loss: 2.218899965286255\n","Processed batch: 160. Loss: 2.1669299602508545\n","Processed batch: 170. Loss: 2.253614902496338\n","Processed batch: 180. Loss: 2.3483917713165283\n","Processed batch: 190. Loss: 2.2516868114471436\n","Processed batch: 200. Loss: 2.4008607864379883\n","Processed batch: 210. Loss: 2.4681220054626465\n","Processed batch: 220. Loss: 2.307131767272949\n","Processed batch: 230. Loss: 1.9772263765335083\n","Processed batch: 240. Loss: 2.339099407196045\n","Processed batch: 250. Loss: 2.445046901702881\n","Processed batch: 260. Loss: 2.382206916809082\n","Processed batch: 270. Loss: 2.2285964488983154\n","Processed batch: 280. Loss: 2.209287405014038\n","Processed batch: 290. Loss: 1.8633402585983276\n","Processed batch: 300. Loss: 2.2858288288116455\n","Processed batch: 310. Loss: 2.25785231590271\n","Processed batch: 320. Loss: 2.3177194595336914\n","Processed batch: 330. Loss: 2.0844314098358154\n","Processed batch: 340. Loss: 2.553863525390625\n","Processed batch: 350. Loss: 2.4991331100463867\n","Processed batch: 360. Loss: 2.2121458053588867\n","Processed batch: 370. Loss: 2.367936372756958\n","Processed batch: 380. Loss: 1.9314368963241577\n","Processed batch: 390. Loss: 2.39847731590271\n","Processed batch: 400. Loss: 2.3357651233673096\n","Processed batch: 410. Loss: 2.313683032989502\n","Processed batch: 420. Loss: 2.2434611320495605\n","Processed batch: 430. Loss: 2.459169864654541\n","Processed batch: 440. Loss: 2.4660894870758057\n","Processed batch: 450. Loss: 2.213233470916748\n","Processed batch: 460. Loss: 2.4115188121795654\n","Processed batch: 470. Loss: 2.3801205158233643\n","Processed batch: 480. Loss: 2.027189016342163\n","Processed batch: 490. Loss: 2.0104587078094482\n","Processed batch: 500. Loss: 2.213559150695801\n","Processed batch: 510. Loss: 2.3615005016326904\n","Processed batch: 520. Loss: 2.2213456630706787\n","Processed batch: 530. Loss: 2.5269088745117188\n","Processed batch: 540. Loss: 2.323617935180664\n","Processed batch: 550. Loss: 2.486335277557373\n","Processed batch: 560. Loss: 2.3626327514648438\n","Processed batch: 570. Loss: 2.0237674713134766\n","Processed batch: 580. Loss: 2.534144878387451\n","Processed batch: 590. Loss: 2.330031633377075\n","Processed batch: 600. Loss: 2.2493128776550293\n","Processed batch: 610. Loss: 2.3597307205200195\n","Processed batch: 620. Loss: 2.402487277984619\n","Processed batch: 630. Loss: 2.2800796031951904\n","Processed batch: 640. Loss: 2.486584186553955\n","Processed batch: 650. Loss: 2.095390796661377\n","Processed batch: 660. Loss: 2.5888447761535645\n","Processed batch: 670. Loss: 2.437535524368286\n","Processed batch: 680. Loss: 2.0912270545959473\n","Processed batch: 690. Loss: 2.414125680923462\n","Processed batch: 700. Loss: 2.5988545417785645\n","Processed batch: 710. Loss: 2.1817548274993896\n","Processed batch: 720. Loss: 2.1642444133758545\n","Processed batch: 730. Loss: 2.0993053913116455\n","Processed batch: 740. Loss: 2.4396345615386963\n","Processed batch: 750. Loss: 2.1829302310943604\n","Processed batch: 760. Loss: 2.3084936141967773\n","Processed batch: 770. Loss: 2.5230677127838135\n","Processed batch: 780. Loss: 2.198908567428589\n","Processed batch: 790. Loss: 2.4803760051727295\n","Processed batch: 800. Loss: 2.434612989425659\n","Processed batch: 810. Loss: 2.69596266746521\n","Processed batch: 820. Loss: 2.368152141571045\n","Processed batch: 830. Loss: 1.872070550918579\n","Processed batch: 840. Loss: 2.2385571002960205\n","Processed batch: 850. Loss: 2.3377280235290527\n","Processed batch: 860. Loss: 2.3152928352355957\n","Processed batch: 870. Loss: 2.38901686668396\n","Processed batch: 880. Loss: 2.3047900199890137\n","Processed batch: 890. Loss: 2.2793962955474854\n","Processed batch: 900. Loss: 2.460961103439331\n","Processed batch: 910. Loss: 2.3320188522338867\n","Processed batch: 920. Loss: 2.652491807937622\n","Processed batch: 930. Loss: 2.1822993755340576\n","Processed batch: 940. Loss: 2.459359645843506\n","Processed batch: 950. Loss: 2.414943218231201\n","Processed batch: 960. Loss: 2.713200092315674\n","Processed batch: 970. Loss: 2.2022364139556885\n","Processed batch: 980. Loss: 2.409122943878174\n","Processed batch: 990. Loss: 2.5201973915100098\n","Processed batch: 1000. Loss: 2.186241388320923\n","Processed batch: 1010. Loss: 2.4886984825134277\n","Processed batch: 1020. Loss: 2.4730005264282227\n","Processed batch: 1030. Loss: 2.454349994659424\n","Processed batch: 1040. Loss: 2.351513147354126\n","Processed batch: 1050. Loss: 2.24613094329834\n","Processed batch: 1060. Loss: 2.348872423171997\n","Processed batch: 1070. Loss: 2.049802541732788\n","Processed batch: 1080. Loss: 2.180976629257202\n","Processed batch: 1090. Loss: 2.302842855453491\n","Processed batch: 1100. Loss: 2.075377941131592\n","Processed batch: 1110. Loss: 2.421781539916992\n","Processed batch: 1120. Loss: 2.2470579147338867\n","Processed batch: 1130. Loss: 2.1617352962493896\n","Processed batch: 1140. Loss: 2.3796579837799072\n","Processed batch: 1150. Loss: 2.342047691345215\n","Processed batch: 1160. Loss: 2.2366902828216553\n","Processed batch: 1170. Loss: 2.6545937061309814\n","Processed batch: 1180. Loss: 2.230915069580078\n","Processed batch: 1190. Loss: 2.632535219192505\n","Processed batch: 1200. Loss: 2.2677533626556396\n","Processed batch: 1210. Loss: 2.4514877796173096\n","Processed batch: 1220. Loss: 2.4504566192626953\n","Processed batch: 1230. Loss: 2.7519354820251465\n","Processed batch: 1240. Loss: 2.288926839828491\n","Processed batch: 1250. Loss: 2.4196557998657227\n","Processed batch: 1260. Loss: 2.1226305961608887\n","Processed batch: 1270. Loss: 2.284053325653076\n","Processed batch: 1280. Loss: 2.0053560733795166\n","Processed batch: 1290. Loss: 2.509493589401245\n","Processed batch: 1300. Loss: 2.25272798538208\n","Processed batch: 1310. Loss: 2.289675712585449\n","Processed batch: 1320. Loss: 2.2510218620300293\n","Processed batch: 1330. Loss: 2.4171082973480225\n","Processed batch: 1340. Loss: 2.141721248626709\n","Processed batch: 1350. Loss: 2.277412176132202\n","Processed batch: 1360. Loss: 2.2773945331573486\n","Processed batch: 1370. Loss: 2.0147695541381836\n","Processed batch: 1380. Loss: 2.2782371044158936\n","Processed batch: 1390. Loss: 2.42098331451416\n","Processed batch: 1400. Loss: 2.375014305114746\n","Processed batch: 1410. Loss: 2.4031147956848145\n","Processed batch: 1420. Loss: 2.2401506900787354\n","Processed batch: 1430. Loss: 2.497619867324829\n","Processed batch: 1440. Loss: 2.696873664855957\n","Processed batch: 1450. Loss: 2.721318483352661\n","Processed batch: 1460. Loss: 2.1914806365966797\n","Processed batch: 1470. Loss: 2.2950198650360107\n","Processed batch: 1480. Loss: 2.1439120769500732\n","Processed batch: 1490. Loss: 2.5260560512542725\n","Processed batch: 1500. Loss: 2.4689605236053467\n","Processed batch: 1510. Loss: 2.398911714553833\n","Processed batch: 1520. Loss: 2.3287625312805176\n","Processed batch: 1530. Loss: 2.4204962253570557\n","Processed batch: 1540. Loss: 2.112492322921753\n","Processed batch: 1550. Loss: 2.1152751445770264\n","Processed batch: 1560. Loss: 2.3136298656463623\n","Processed batch: 1570. Loss: 2.2748584747314453\n","Processed batch: 1580. Loss: 2.360975742340088\n","Processed batch: 1590. Loss: 2.4579663276672363\n","Processed batch: 1600. Loss: 2.5347437858581543\n","Processed batch: 1610. Loss: 2.1983370780944824\n","Processed batch: 1620. Loss: 2.419004201889038\n","Processed batch: 1630. Loss: 2.376371383666992\n","Processed batch: 1640. Loss: 2.212033748626709\n","Processed batch: 1650. Loss: 2.3666114807128906\n","Processed batch: 1660. Loss: 2.291809558868408\n","Processed batch: 1670. Loss: 2.3496851921081543\n","Processed batch: 1680. Loss: 2.264204978942871\n","Processed batch: 1690. Loss: 2.628986120223999\n","Processed batch: 1700. Loss: 2.3122446537017822\n","Processed batch: 1710. Loss: 1.953913688659668\n","Processed batch: 1720. Loss: 2.465437412261963\n","Processed batch: 1730. Loss: 2.2704918384552\n","Processed batch: 1740. Loss: 2.4980404376983643\n","Processed batch: 1750. Loss: 2.3156511783599854\n","Processed batch: 1760. Loss: 2.5135130882263184\n","Processed batch: 1770. Loss: 2.2228634357452393\n","Processed batch: 1780. Loss: 2.4360575675964355\n","Processed batch: 1790. Loss: 2.5747458934783936\n","Processed batch: 1800. Loss: 2.2727861404418945\n","Processed batch: 1810. Loss: 2.4099488258361816\n","Processed batch: 1820. Loss: 2.3682479858398438\n","Processed batch: 1830. Loss: 2.397745132446289\n","Processed batch: 1840. Loss: 2.2107555866241455\n","Processed batch: 1850. Loss: 2.4852049350738525\n","Processed batch: 1860. Loss: 2.2854905128479004\n","Processed batch: 1870. Loss: 2.6291584968566895\n","Processed batch: 1880. Loss: 2.222365140914917\n","Processed batch: 1890. Loss: 2.660834550857544\n","Processed batch: 1900. Loss: 2.4639182090759277\n","Processed batch: 1910. Loss: 2.4742074012756348\n","Processed batch: 1920. Loss: 2.167571783065796\n","Processed batch: 1930. Loss: 2.443516969680786\n","Processed batch: 1940. Loss: 2.298626184463501\n","Processed batch: 1950. Loss: 2.171189785003662\n","Processed batch: 1960. Loss: 2.508124828338623\n","Processed batch: 1970. Loss: 2.186413288116455\n","Processed batch: 1980. Loss: 2.3711071014404297\n","Processed batch: 1990. Loss: 2.3581981658935547\n","Processed batch: 2000. Loss: 2.129889726638794\n","Processed batch: 2010. Loss: 2.2464423179626465\n","Processed batch: 2020. Loss: 2.1439249515533447\n","Processed batch: 2030. Loss: 2.437488079071045\n","Processed batch: 2040. Loss: 2.249325752258301\n","Processed batch: 2050. Loss: 2.2999913692474365\n","Processed batch: 2060. Loss: 2.1335482597351074\n","Processed batch: 2070. Loss: 2.4082796573638916\n","Processed batch: 2080. Loss: 2.233307361602783\n","Processed batch: 2090. Loss: 2.5857017040252686\n","Processed batch: 2100. Loss: 2.369373083114624\n","Processed batch: 2110. Loss: 2.6468827724456787\n","Processed batch: 2120. Loss: 2.032702684402466\n","Processed batch: 2130. Loss: 2.4230101108551025\n","Processed batch: 2140. Loss: 2.132964611053467\n","Processed batch: 2150. Loss: 2.5400571823120117\n","Processed batch: 2160. Loss: 2.50585675239563\n","Processed batch: 2170. Loss: 2.354329824447632\n","Processed batch: 2180. Loss: 2.2097716331481934\n","Processed batch: 2190. Loss: 2.3792812824249268\n","Processed batch: 2200. Loss: 2.1331350803375244\n","Processed batch: 2210. Loss: 2.399487018585205\n","Processed batch: 2220. Loss: 2.5919251441955566\n","Processed batch: 2230. Loss: 2.22326397895813\n","Processed batch: 2240. Loss: 2.364738702774048\n","Processed batch: 2250. Loss: 2.7175540924072266\n","Processed batch: 2260. Loss: 2.31447172164917\n","Processed batch: 2270. Loss: 2.418137311935425\n","Processed batch: 2280. Loss: 2.277374267578125\n","Processed batch: 2290. Loss: 2.4201395511627197\n","Processed batch: 2300. Loss: 2.4569766521453857\n","Processed batch: 2310. Loss: 2.3617630004882812\n","Processed batch: 2320. Loss: 2.4503471851348877\n","Processed batch: 2330. Loss: 2.3565351963043213\n","Processed batch: 2340. Loss: 2.3292346000671387\n","Processed batch: 2350. Loss: 2.5403692722320557\n","Processed batch: 2360. Loss: 2.3491647243499756\n","Processed batch: 2370. Loss: 2.449760675430298\n","Processed batch: 2380. Loss: 2.3460381031036377\n","Processed batch: 2390. Loss: 2.525947332382202\n","Processed batch: 2400. Loss: 2.544165849685669\n","Processed batch: 2410. Loss: 2.059805154800415\n","Processed batch: 2420. Loss: 2.481782913208008\n","Processed batch: 2430. Loss: 2.5269296169281006\n","Processed batch: 2440. Loss: 2.3279521465301514\n","Processed batch: 2450. Loss: 2.442270278930664\n","Processed batch: 2460. Loss: 2.176818370819092\n","Processed batch: 2470. Loss: 1.9760881662368774\n","Processed batch: 2480. Loss: 2.4089035987854004\n","Processed batch: 2490. Loss: 2.374999523162842\n","Processed batch: 2500. Loss: 2.2073004245758057\n","Processed batch: 2510. Loss: 2.1487812995910645\n","Processed batch: 2520. Loss: 2.4388651847839355\n","Processed batch: 2530. Loss: 2.3174185752868652\n","Processed batch: 2540. Loss: 2.0927815437316895\n","Processed batch: 2550. Loss: 2.7460110187530518\n","Processed batch: 2560. Loss: 2.626223087310791\n","Processed batch: 2570. Loss: 2.381472110748291\n","Processed batch: 2580. Loss: 2.307161331176758\n","Processed batch: 2590. Loss: 2.6579174995422363\n","Processed batch: 2600. Loss: 2.382568597793579\n","Processed batch: 2610. Loss: 2.5407910346984863\n","Processed batch: 2620. Loss: 2.303192138671875\n","Processed batch: 2630. Loss: 2.214726448059082\n","Processed batch: 2640. Loss: 2.205545425415039\n","Processed batch: 2650. Loss: 2.124849796295166\n","Processed batch: 2660. Loss: 2.46057391166687\n","Processed batch: 2670. Loss: 2.533601760864258\n","Processed batch: 2680. Loss: 2.674929618835449\n","Processed batch: 2690. Loss: 2.0756781101226807\n","Processed batch: 2700. Loss: 2.272838592529297\n","Processed batch: 2710. Loss: 2.398529291152954\n","Processed batch: 2720. Loss: 2.427722215652466\n","Processed batch: 2730. Loss: 2.233423948287964\n","Processed batch: 2740. Loss: 2.61072039604187\n","Processed batch: 2750. Loss: 2.1140329837799072\n","Processed batch: 2760. Loss: 2.2718968391418457\n","Processed batch: 2770. Loss: 2.247797966003418\n","Processed batch: 2780. Loss: 2.140380620956421\n","Processed batch: 2790. Loss: 2.4950485229492188\n","Processed batch: 2800. Loss: 2.365434408187866\n","Processed batch: 2810. Loss: 2.542090654373169\n","Processed batch: 2820. Loss: 2.3210761547088623\n","Processed batch: 2830. Loss: 2.3157031536102295\n","Processed batch: 2840. Loss: 2.259255886077881\n","Processed batch: 2850. Loss: 2.4826269149780273\n","Processed batch: 2860. Loss: 2.5488829612731934\n","Processed batch: 2870. Loss: 2.3495514392852783\n","Processed batch: 2880. Loss: 2.3826522827148438\n","Processed batch: 2890. Loss: 2.282435655593872\n","Processed batch: 2900. Loss: 2.291423797607422\n","Processed batch: 2910. Loss: 2.400160074234009\n","Processed batch: 2920. Loss: 2.257086753845215\n","Processed batch: 2930. Loss: 2.417818784713745\n","Processed batch: 2940. Loss: 2.3086323738098145\n","Processed batch: 2950. Loss: 2.637319803237915\n","Processed batch: 2960. Loss: 2.2168729305267334\n","Processed batch: 2970. Loss: 2.4840586185455322\n","Processed batch: 2980. Loss: 2.2480359077453613\n","Processed batch: 2990. Loss: 2.3423397541046143\n","Processed batch: 3000. Loss: 2.407238245010376\n","Processed batch: 3010. Loss: 2.486374616622925\n","Processed batch: 3020. Loss: 2.0926902294158936\n","Processed batch: 3030. Loss: 2.480025053024292\n","Processed batch: 3040. Loss: 2.188343048095703\n","Processed batch: 3050. Loss: 2.378984212875366\n","Processed batch: 3060. Loss: 2.570585250854492\n","Processed batch: 3070. Loss: 2.1743173599243164\n","Processed batch: 3080. Loss: 2.1807212829589844\n","Processed batch: 3090. Loss: 2.04563570022583\n","Processed batch: 3100. Loss: 2.355602741241455\n","Processed batch: 3110. Loss: 2.365586996078491\n","Processed batch: 3120. Loss: 2.465003728866577\n","Processed batch: 3130. Loss: 2.191352605819702\n","Processed batch: 3140. Loss: 2.6023035049438477\n","Processed batch: 3150. Loss: 2.22542142868042\n","Processed batch: 3160. Loss: 2.1039958000183105\n","Processed batch: 3170. Loss: 2.4232661724090576\n","Processed batch: 3180. Loss: 2.5104990005493164\n","Processed batch: 3190. Loss: 2.2436537742614746\n","Processed batch: 3200. Loss: 2.4825289249420166\n","Processed batch: 3210. Loss: 2.521707773208618\n","Processed batch: 3220. Loss: 2.0554351806640625\n","Processed batch: 3230. Loss: 2.550142288208008\n","Processed batch: 3240. Loss: 2.279303550720215\n","Processed batch: 3250. Loss: 2.16457200050354\n","Processed batch: 3260. Loss: 2.6239333152770996\n","Processed batch: 3270. Loss: 2.4224421977996826\n","Processed batch: 3280. Loss: 2.376589059829712\n","Processed batch: 3290. Loss: 2.6857385635375977\n","Processed batch: 3300. Loss: 2.5356385707855225\n","Processed batch: 3310. Loss: 2.5596940517425537\n","Processed batch: 3320. Loss: 2.271606683731079\n","Processed batch: 3330. Loss: 2.792950391769409\n","Processed batch: 3340. Loss: 2.41741681098938\n","Processed batch: 3350. Loss: 2.1900227069854736\n","Processed batch: 3360. Loss: 2.2845137119293213\n","Processed batch: 3370. Loss: 2.422046184539795\n","Processed batch: 3380. Loss: 2.3436825275421143\n","Processed batch: 3390. Loss: 2.5246081352233887\n","Processed batch: 3400. Loss: 2.4811439514160156\n","Processed batch: 3410. Loss: 2.335618019104004\n","Processed batch: 3420. Loss: 2.412898302078247\n","Processed batch: 3430. Loss: 2.43184757232666\n","Processed batch: 3440. Loss: 2.386244058609009\n","Processed batch: 3450. Loss: 2.4296751022338867\n","Processed batch: 3460. Loss: 2.363281726837158\n","Processed batch: 3470. Loss: 2.580526351928711\n","Processed batch: 3480. Loss: 2.4372425079345703\n","Processed batch: 3490. Loss: 2.6546995639801025\n","Processed batch: 3500. Loss: 2.3009018898010254\n","Processed batch: 3510. Loss: 2.302295446395874\n","Processed batch: 3520. Loss: 2.2529914379119873\n","Processed batch: 3530. Loss: 2.3806910514831543\n","Processed batch: 3540. Loss: 2.462174415588379\n","Processed batch: 3550. Loss: 2.1192777156829834\n","Processed batch: 3560. Loss: 2.641627311706543\n","Processed batch: 3570. Loss: 2.502584218978882\n","Processed batch: 3580. Loss: 2.582366704940796\n","Processed batch: 3590. Loss: 2.399402141571045\n","Processed batch: 3600. Loss: 2.3180384635925293\n","Processed batch: 3610. Loss: 2.597217082977295\n","Processed batch: 3620. Loss: 2.2742228507995605\n","Processed batch: 3630. Loss: 2.2392690181732178\n","Processed batch: 3640. Loss: 2.473632335662842\n","Processed batch: 3650. Loss: 2.356539249420166\n","Processed batch: 3660. Loss: 2.164097785949707\n","Processed batch: 3670. Loss: 2.361811876296997\n","Processed batch: 3680. Loss: 2.3195881843566895\n","Processed batch: 3690. Loss: 2.1053998470306396\n","Processed batch: 3700. Loss: 2.458869695663452\n","Processed batch: 3710. Loss: 2.308044672012329\n","Processed batch: 3720. Loss: 2.5241711139678955\n","Processed batch: 3730. Loss: 2.3172452449798584\n","Processed batch: 3740. Loss: 2.4493894577026367\n","Processed batch: 3750. Loss: 2.054779052734375\n","Processed batch: 3760. Loss: 2.3855817317962646\n","Processed batch: 3770. Loss: 2.564293622970581\n","Processed batch: 3780. Loss: 2.4840970039367676\n","Processed batch: 3790. Loss: 2.2597174644470215\n","Processed batch: 3800. Loss: 2.3107106685638428\n","Processed batch: 3810. Loss: 2.3669440746307373\n","Processed batch: 3820. Loss: 2.17819881439209\n","Processed batch: 3830. Loss: 2.000889778137207\n","Processed batch: 3840. Loss: 2.4444034099578857\n","Processed batch: 3850. Loss: 2.5257468223571777\n","Processed batch: 3860. Loss: 2.666330337524414\n","Processed batch: 3870. Loss: 2.500924825668335\n","Processed batch: 3880. Loss: 2.356325149536133\n","Processed batch: 3890. Loss: 2.1693153381347656\n","Processed batch: 3900. Loss: 2.467066526412964\n","Processed batch: 3910. Loss: 2.4382107257843018\n","Processed batch: 3920. Loss: 2.4405179023742676\n","Processed batch: 3930. Loss: 2.4113001823425293\n","Processed batch: 3940. Loss: 2.3827590942382812\n","Processed batch: 3950. Loss: 2.251652479171753\n","Processed batch: 3960. Loss: 2.6061062812805176\n","Processed batch: 3970. Loss: 2.3981401920318604\n","Processed batch: 3980. Loss: 2.2354745864868164\n","Processed batch: 3990. Loss: 2.4989686012268066\n","Processed batch: 4000. Loss: 2.6472935676574707\n","Processed batch: 4010. Loss: 2.625192165374756\n","Processed batch: 4020. Loss: 2.250839948654175\n","Processed batch: 4030. Loss: 2.1481618881225586\n","Processed batch: 4040. Loss: 2.237468957901001\n","Processed batch: 4050. Loss: 2.370361566543579\n","Processed batch: 4060. Loss: 2.321347713470459\n","Processed batch: 4070. Loss: 2.3570070266723633\n","Processed batch: 4080. Loss: 2.0964694023132324\n","Processed batch: 4090. Loss: 1.987104058265686\n","Processed batch: 4100. Loss: 2.3743762969970703\n","Processed batch: 4110. Loss: 2.5910770893096924\n","Processed batch: 4120. Loss: 2.50312876701355\n","Processed batch: 4130. Loss: 2.268143653869629\n","Processed batch: 4140. Loss: 2.1574270725250244\n","Processed batch: 4150. Loss: 2.5395312309265137\n","Processed batch: 4160. Loss: 2.3007349967956543\n","Processed batch: 4170. Loss: 2.2619690895080566\n","Processed batch: 4180. Loss: 1.923240303993225\n","Processed batch: 4190. Loss: 2.5295639038085938\n","Processed batch: 4200. Loss: 2.440943479537964\n","Processed batch: 4210. Loss: 2.157627820968628\n","Processed batch: 4220. Loss: 1.970246434211731\n","Processed batch: 4230. Loss: 1.9517148733139038\n","Processed batch: 4240. Loss: 2.21663761138916\n","Processed batch: 4250. Loss: 2.3173539638519287\n","Processed batch: 4260. Loss: 2.6332132816314697\n","Processed batch: 4270. Loss: 2.3884360790252686\n","Processed batch: 4280. Loss: 2.370695114135742\n","Processed batch: 4290. Loss: 2.337071657180786\n","Processed batch: 4300. Loss: 2.407862663269043\n","Processed batch: 4310. Loss: 2.6913139820098877\n","Processed batch: 4320. Loss: 2.4678666591644287\n","Processed batch: 4330. Loss: 2.5611190795898438\n","Processed batch: 4340. Loss: 2.592109203338623\n","Processed batch: 4350. Loss: 2.200666666030884\n","Processed batch: 4360. Loss: 2.797973155975342\n","Processed batch: 4370. Loss: 2.7816545963287354\n","Processed batch: 4380. Loss: 2.4582502841949463\n","Processed batch: 4390. Loss: 2.7090837955474854\n","Processed batch: 4400. Loss: 2.6297037601470947\n","Processed batch: 4410. Loss: 2.5050816535949707\n","Processed batch: 4420. Loss: 2.4540140628814697\n","Processed batch: 4430. Loss: 2.621581792831421\n","Processed batch: 4440. Loss: 2.183917999267578\n","Processed batch: 4450. Loss: 2.3728606700897217\n","Processed batch: 4460. Loss: 2.2497823238372803\n","Processed batch: 4470. Loss: 2.202805280685425\n","Processed batch: 4480. Loss: 2.6514525413513184\n","Processed batch: 4490. Loss: 2.617575168609619\n","Processed batch: 4500. Loss: 2.5034425258636475\n","Processed batch: 4510. Loss: 2.360050678253174\n","Processed batch: 4520. Loss: 2.3714475631713867\n","Processed batch: 4530. Loss: 2.0753772258758545\n","Processed batch: 4540. Loss: 2.303321361541748\n","Processed batch: 4550. Loss: 2.386967182159424\n","Processed batch: 4560. Loss: 2.291107416152954\n","Processed batch: 4570. Loss: 2.5727028846740723\n","Processed batch: 4580. Loss: 2.6099653244018555\n","Processed batch: 4590. Loss: 2.2771620750427246\n","Processed batch: 4600. Loss: 2.6656620502471924\n","Processed batch: 4610. Loss: 2.545870542526245\n","Processed batch: 4620. Loss: 2.2657766342163086\n","Processed batch: 4630. Loss: 2.3173084259033203\n","Processed batch: 4640. Loss: 2.3511781692504883\n","Processed batch: 4650. Loss: 2.25484299659729\n","Processed batch: 4660. Loss: 2.5421977043151855\n","Processed batch: 4670. Loss: 2.4542925357818604\n","Processed batch: 4680. Loss: 2.4481923580169678\n","Processed batch: 4690. Loss: 2.0738351345062256\n","Processed batch: 4700. Loss: 2.4227123260498047\n","Processed batch: 4710. Loss: 2.6004841327667236\n","Processed batch: 4720. Loss: 2.1737303733825684\n","Processed batch: 4730. Loss: 2.336845874786377\n","Processed batch: 4740. Loss: 2.5196597576141357\n","Processed batch: 4750. Loss: 2.277913808822632\n","Processed batch: 4760. Loss: 2.255321979522705\n","Processed batch: 4770. Loss: 2.298401355743408\n","Processed batch: 4780. Loss: 2.4109294414520264\n","Processed batch: 4790. Loss: 2.226335048675537\n","Processed batch: 4800. Loss: 2.427858591079712\n","Processed batch: 4810. Loss: 2.385291814804077\n","Processed batch: 4820. Loss: 2.4701778888702393\n","Processed batch: 4830. Loss: 2.8168487548828125\n","Processed batch: 4840. Loss: 2.5092897415161133\n","Processed batch: 4850. Loss: 2.207000494003296\n","Processed batch: 4860. Loss: 2.2538609504699707\n","Processed batch: 4870. Loss: 2.224639892578125\n","Processed batch: 4880. Loss: 2.370645761489868\n","Processed batch: 4890. Loss: 2.3530280590057373\n","Processed batch: 4900. Loss: 2.2103302478790283\n","Processed batch: 4910. Loss: 2.1120784282684326\n","Processed batch: 4920. Loss: 2.522181987762451\n","Processed batch: 4930. Loss: 2.5572073459625244\n","Processed batch: 4940. Loss: 2.0180625915527344\n","Processed batch: 4950. Loss: 2.0346388816833496\n","Processed batch: 4960. Loss: 2.2823657989501953\n","Processed batch: 4970. Loss: 2.208413600921631\n","Processed batch: 4980. Loss: 2.405594825744629\n","Processed batch: 4990. Loss: 2.307539463043213\n","Processed batch: 5000. Loss: 2.202131509780884\n","Processed batch: 5010. Loss: 2.454430103302002\n","Epoch:5, train loss: 2.3452, time: 5145.22s\n","Processed batch: 10. Loss: 2.3658668994903564\n","Processed batch: 20. Loss: 2.588948965072632\n","Processed batch: 30. Loss: 3.2108521461486816\n","Processed batch: 40. Loss: 2.558359384536743\n","Processed batch: 50. Loss: 3.0312280654907227\n","Processed batch: 60. Loss: 2.5857443809509277\n","Processed batch: 70. Loss: 2.8170676231384277\n","Processed batch: 80. Loss: 2.9107489585876465\n","Processed batch: 90. Loss: 2.8662471771240234\n","Processed batch: 100. Loss: 2.8434295654296875\n","Processed batch: 110. Loss: 3.216224431991577\n","Processed batch: 120. Loss: 2.724762201309204\n","Processed batch: 130. Loss: 3.0983238220214844\n","Processed batch: 140. Loss: 3.0926809310913086\n","Processed batch: 150. Loss: 2.1911678314208984\n","Processed batch: 160. Loss: 2.355379581451416\n","Processed batch: 170. Loss: 2.747676134109497\n","Processed batch: 180. Loss: 2.469606876373291\n","Processed batch: 190. Loss: 2.5867936611175537\n","Processed batch: 200. Loss: 2.7601096630096436\n","Processed batch: 210. Loss: 2.5047364234924316\n","Processed batch: 220. Loss: 2.3320672512054443\n","Processed batch: 230. Loss: 2.6898996829986572\n","Processed batch: 240. Loss: 2.6306498050689697\n","Processed batch: 250. Loss: 3.1671698093414307\n","Processed batch: 260. Loss: 3.013336658477783\n","Processed batch: 270. Loss: 2.9353435039520264\n","Processed batch: 280. Loss: 3.033036708831787\n","Processed batch: 290. Loss: 2.728081703186035\n","Processed batch: 300. Loss: 2.3646066188812256\n","Processed batch: 310. Loss: 2.533689260482788\n","Processed batch: 320. Loss: 2.7547922134399414\n","Processed batch: 330. Loss: 3.1728968620300293\n","Processed batch: 340. Loss: 2.7181808948516846\n","Processed batch: 350. Loss: 3.058845043182373\n","Processed batch: 360. Loss: 2.8614017963409424\n","Processed batch: 370. Loss: 2.340085983276367\n","Processed batch: 380. Loss: 2.539102077484131\n","Processed batch: 390. Loss: 2.509195566177368\n","Processed batch: 400. Loss: 2.428148031234741\n","Processed batch: 410. Loss: 2.4542899131774902\n","Processed batch: 420. Loss: 2.6757400035858154\n","Processed batch: 430. Loss: 2.9364378452301025\n","Processed batch: 440. Loss: 3.5956509113311768\n","Processed batch: 450. Loss: 2.8954532146453857\n","Processed batch: 460. Loss: 2.924159288406372\n","Processed batch: 470. Loss: 2.932856798171997\n","Processed batch: 480. Loss: 2.664499282836914\n","Processed batch: 490. Loss: 2.8510961532592773\n","Processed batch: 500. Loss: 2.803633451461792\n","Processed batch: 510. Loss: 3.0716967582702637\n","Processed batch: 520. Loss: 2.3859126567840576\n","Processed batch: 530. Loss: 2.6195321083068848\n","Processed batch: 540. Loss: 3.139734983444214\n","Processed batch: 550. Loss: 2.929008722305298\n","Processed batch: 560. Loss: 2.365764617919922\n","Processed batch: 570. Loss: 2.884784698486328\n","Processed batch: 580. Loss: 2.5108320713043213\n","Processed batch: 590. Loss: 2.540501117706299\n","Processed batch: 600. Loss: 2.749218463897705\n","Processed batch: 610. Loss: 2.4598984718322754\n","Processed batch: 620. Loss: 2.597485065460205\n","------------------------------------------------------------\n","Epoch:5, valid loss: 2.7223, time: 204.13s\n","------------------------------------------------------------\n","Processed batch: 10. Loss: 2.118752956390381\n","Processed batch: 20. Loss: 2.199690103530884\n","Processed batch: 30. Loss: 2.1694490909576416\n","Processed batch: 40. Loss: 2.2400314807891846\n","Processed batch: 50. Loss: 2.379146099090576\n","Processed batch: 60. Loss: 2.4770400524139404\n","Processed batch: 70. Loss: 2.232426881790161\n","Processed batch: 80. Loss: 2.2400660514831543\n","Processed batch: 90. Loss: 2.7058026790618896\n","Processed batch: 100. Loss: 2.2366549968719482\n","Processed batch: 110. Loss: 2.484358787536621\n","Processed batch: 120. Loss: 2.3119590282440186\n","Processed batch: 130. Loss: 2.1148345470428467\n","Processed batch: 140. Loss: 2.2041430473327637\n","Processed batch: 150. Loss: 2.200244426727295\n","Processed batch: 160. Loss: 2.4253063201904297\n","Processed batch: 170. Loss: 2.281125068664551\n","Processed batch: 180. Loss: 2.1995460987091064\n","Processed batch: 190. Loss: 1.9168610572814941\n","Processed batch: 200. Loss: 2.2007369995117188\n","Processed batch: 210. Loss: 2.2539942264556885\n","Processed batch: 220. Loss: 2.3263332843780518\n","Processed batch: 230. Loss: 2.3151848316192627\n","Processed batch: 240. Loss: 2.288989782333374\n","Processed batch: 250. Loss: 2.1161727905273438\n","Processed batch: 260. Loss: 2.0530803203582764\n","Processed batch: 270. Loss: 2.06384539604187\n","Processed batch: 280. Loss: 2.4556021690368652\n","Processed batch: 290. Loss: 2.3789570331573486\n","Processed batch: 300. Loss: 2.304159641265869\n","Processed batch: 310. Loss: 1.9019168615341187\n","Processed batch: 320. Loss: 2.1758272647857666\n","Processed batch: 330. Loss: 2.5012624263763428\n","Processed batch: 340. Loss: 2.196755886077881\n","Processed batch: 350. Loss: 1.8683431148529053\n","Processed batch: 360. Loss: 2.127450704574585\n","Processed batch: 370. Loss: 2.3040852546691895\n","Processed batch: 380. Loss: 2.1614294052124023\n","Processed batch: 390. Loss: 2.081284761428833\n","Processed batch: 400. Loss: 2.305816650390625\n","Processed batch: 410. Loss: 2.0988712310791016\n","Processed batch: 420. Loss: 2.13889479637146\n","Processed batch: 430. Loss: 2.0120391845703125\n","Processed batch: 440. Loss: 2.311276912689209\n","Processed batch: 450. Loss: 2.1498055458068848\n","Processed batch: 460. Loss: 2.080172538757324\n","Processed batch: 470. Loss: 2.142643690109253\n","Processed batch: 480. Loss: 2.5892419815063477\n","Processed batch: 490. Loss: 2.340146064758301\n","Processed batch: 500. Loss: 2.262258291244507\n","Processed batch: 510. Loss: 2.2873928546905518\n","Processed batch: 520. Loss: 2.161679267883301\n","Processed batch: 530. Loss: 2.2960164546966553\n","Processed batch: 540. Loss: 2.1232643127441406\n","Processed batch: 550. Loss: 2.6507794857025146\n","Processed batch: 560. Loss: 2.0643351078033447\n","Processed batch: 570. Loss: 2.172419309616089\n","Processed batch: 580. Loss: 2.1451632976531982\n","Processed batch: 590. Loss: 2.087235927581787\n","Processed batch: 600. Loss: 2.1391215324401855\n","Processed batch: 610. Loss: 2.3202593326568604\n","Processed batch: 620. Loss: 2.2012877464294434\n","Processed batch: 630. Loss: 2.243881940841675\n","Processed batch: 640. Loss: 2.300360918045044\n","Processed batch: 650. Loss: 2.212813138961792\n","Processed batch: 660. Loss: 2.286893606185913\n","Processed batch: 670. Loss: 2.4363625049591064\n","Processed batch: 680. Loss: 2.052258253097534\n","Processed batch: 690. Loss: 2.0851759910583496\n","Processed batch: 700. Loss: 2.168504238128662\n","Processed batch: 710. Loss: 2.1620752811431885\n","Processed batch: 720. Loss: 2.6223974227905273\n","Processed batch: 730. Loss: 1.9808882474899292\n","Processed batch: 740. Loss: 2.2661592960357666\n","Processed batch: 750. Loss: 2.092026948928833\n","Processed batch: 760. Loss: 2.1973352432250977\n","Processed batch: 770. Loss: 2.240720510482788\n","Processed batch: 780. Loss: 2.105433225631714\n","Processed batch: 790. Loss: 2.031675338745117\n","Processed batch: 800. Loss: 2.655543565750122\n","Processed batch: 810. Loss: 2.1699743270874023\n","Processed batch: 820. Loss: 2.408510684967041\n","Processed batch: 830. Loss: 2.2526562213897705\n","Processed batch: 840. Loss: 2.1025891304016113\n","Processed batch: 850. Loss: 2.0158469676971436\n","Processed batch: 860. Loss: 2.0503385066986084\n","Processed batch: 870. Loss: 2.002593994140625\n","Processed batch: 880. Loss: 1.9804840087890625\n","Processed batch: 890. Loss: 2.1148579120635986\n","Processed batch: 900. Loss: 2.4321231842041016\n","Processed batch: 910. Loss: 2.220888137817383\n","Processed batch: 920. Loss: 2.2286176681518555\n","Processed batch: 930. Loss: 2.0944440364837646\n","Processed batch: 940. Loss: 2.312065839767456\n","Processed batch: 950. Loss: 2.0135929584503174\n","Processed batch: 960. Loss: 2.363525390625\n","Processed batch: 970. Loss: 2.228736639022827\n","Processed batch: 980. Loss: 1.9910509586334229\n","Processed batch: 990. Loss: 2.364838123321533\n","Processed batch: 1000. Loss: 2.386075735092163\n","Processed batch: 1010. Loss: 2.3046672344207764\n","Processed batch: 1020. Loss: 2.203234910964966\n","Processed batch: 1030. Loss: 2.430776357650757\n","Processed batch: 1040. Loss: 2.5138776302337646\n","Processed batch: 1050. Loss: 2.0862200260162354\n","Processed batch: 1060. Loss: 2.093897581100464\n","Processed batch: 1070. Loss: 2.621274709701538\n","Processed batch: 1080. Loss: 2.3329148292541504\n","Processed batch: 1090. Loss: 2.2495577335357666\n","Processed batch: 1100. Loss: 2.0953829288482666\n","Processed batch: 1110. Loss: 2.078725814819336\n","Processed batch: 1120. Loss: 2.5208845138549805\n","Processed batch: 1130. Loss: 2.3430840969085693\n","Processed batch: 1140. Loss: 2.195746421813965\n","Processed batch: 1150. Loss: 1.932328701019287\n","Processed batch: 1160. Loss: 2.148247003555298\n","Processed batch: 1170. Loss: 2.3279573917388916\n","Processed batch: 1180. Loss: 2.303326368331909\n","Processed batch: 1190. Loss: 2.296323776245117\n","Processed batch: 1200. Loss: 2.253317356109619\n","Processed batch: 1210. Loss: 2.3126609325408936\n","Processed batch: 1220. Loss: 2.1636908054351807\n","Processed batch: 1230. Loss: 2.137816905975342\n","Processed batch: 1240. Loss: 2.3173038959503174\n","Processed batch: 1250. Loss: 2.2911622524261475\n","Processed batch: 1260. Loss: 2.2347803115844727\n","Processed batch: 1270. Loss: 2.1003830432891846\n","Processed batch: 1280. Loss: 2.5779335498809814\n","Processed batch: 1290. Loss: 2.185492992401123\n","Processed batch: 1300. Loss: 2.613804340362549\n","Processed batch: 1310. Loss: 2.156278133392334\n","Processed batch: 1320. Loss: 2.2194504737854004\n","Processed batch: 1330. Loss: 2.3292670249938965\n","Processed batch: 1340. Loss: 2.6302192211151123\n","Processed batch: 1350. Loss: 2.276453971862793\n","Processed batch: 1360. Loss: 2.09610915184021\n","Processed batch: 1370. Loss: 2.3674702644348145\n","Processed batch: 1380. Loss: 2.124758243560791\n","Processed batch: 1390. Loss: 2.0178263187408447\n","Processed batch: 1400. Loss: 2.0784294605255127\n","Processed batch: 1410. Loss: 2.0980823040008545\n","Processed batch: 1420. Loss: 2.1341323852539062\n","Processed batch: 1430. Loss: 1.9856727123260498\n","Processed batch: 1440. Loss: 2.1959221363067627\n","Processed batch: 1450. Loss: 1.9555085897445679\n","Processed batch: 1460. Loss: 1.8322477340698242\n","Processed batch: 1470. Loss: 2.299797773361206\n","Processed batch: 1480. Loss: 2.2228269577026367\n"]}],"source":["learning_rate = 0.00002\n","weight_decay = 0.00001\n","lr_decay = 0.95\n","num_epochs = 20\n","early_stop = 3\n","save_path = \"/content/drive/Model\"\n","\n","#resume = \"/content/drive/Model/srl2story_CLIP_cap_epoch2.pth.tar\"\n","resume = False\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate,\n","                        weight_decay = weight_decay)\n","\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1.0, gamma = lr_decay)\n","\n","training_info  = {\"num_epochs\": num_epochs,\n","                \"criterion\": criterion,\n","                \"optimizer\": optimizer,\n","                \"scheduler\": scheduler,\n","                \"num_es_epochs\": early_stop,\n","                \"train_loader\": train_dataloader,\n","                \"valid_loader\": valid_dataloader,\n","                \"save_path\": save_path}\n","\n","if resume:\n","    ### start training model from loaded check point\n","    if os.path.isfile(resume):\n","        checkpoint = torch.load(resume)\n","        model.load_state_dict(checkpoint['model'])\n","        start_epoch = checkpoint[\"epoch\"]\n","\n","        print(\"Loaded model checkpoint! Starting at epoch: {}\".format(start_epoch+1))\n","\n","        train_model(model, training_info, start_epoch = start_epoch)\n","else:\n","    ### train model from scratch\n","    train_model(model, training_info)"]},{"cell_type":"markdown","metadata":{"id":"vkjSmEu1McYJ"},"source":["# Save Generated Stories"]},{"cell_type":"code","source":["model_nm = \"facebook/bart-large\"\n","bart_model = BartForConditionalGeneration.from_pretrained(model_nm)\n","\n","model = StoryDecoder(bart_model, tokenizer)\n","model = model.to(device) # move to GPU\n","\n","resume = \"/content/VIST Model/Stage 2 Model/srl2story_CLIP_pmi_epoch4.pth.tar\"\n","\n","if resume:\n","    ### load model\n","    if os.path.isfile(resume):\n","        checkpoint = torch.load(resume) # map_location=torch.device('cpu'))\n","        model.load_state_dict(checkpoint['model'])\n","        start_epoch = checkpoint[\"epoch\"]\n","\n","        print(\"Loaded model checkpoint! Model obtained from epoch: {}\".format(start_epoch))\n","else:\n","    print(\"Could not find model checkpoint! :(\")"],"metadata":{"id":"tH-1MBFjU-Wn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":824,"status":"ok","timestamp":1674436334244,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"Al5FI4iMMdtp","outputId":"f04a5b0b-cb86-4beb-b1ef-05c5a39dbc8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of batches: 316\n"]}],"source":["dataloader = torch.utils.data.DataLoader(test_dl, shuffle = False, batch_size = 16, collate_fn = collate_fn)\n","print(\"Number of batches: {}\".format(len(dataloader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831447,"status":"ok","timestamp":1674438033240,"user":{"displayName":"Eileen","userId":"01091172001417006453"},"user_tz":-660},"id":"BCSH8ZLlJ31f","outputId":"711f9dd3-2476-42fc-fb2a-5192bf12d37e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing batch 50\n","Processing batch 100\n","Processing batch 150\n","Processing batch 200\n","Processing batch 250\n","Processing batch 300\n"]}],"source":["### test collate function\n","model.eval()\n","decoding_method = \"greedy\"\n","gen_stories = {}\n","\n","for batch_idx, data in enumerate(dataloader):\n","\n","    if (batch_idx + 1) % 50 == 0:\n","        print(\"Processing batch {}\".format(batch_idx + 1))\n","\n","    batch_data = data\n","\n","    story_ids = set(batch_data[\"story_ids\"])\n","    batch_data = data\n","\n","    with torch.no_grad():\n","        batch_data = data\n","        input_ids = batch_data[\"inputs\"].to(device)\n","        attention_mask = batch_data[\"input_attention_ids\"].to(device)\n","\n","    if decoding_method == \"beam\":\n","        outputs = model.bart_model.generate(input_ids = input_ids,\n","                                            attention_mask = attention_mask,\n","                                            num_beams = 3, max_length = 200)\n","    elif decoding_method == \"nucleus\":\n","        outputs = model.bart_model.generate(input_ids = input_ids,\n","                                            attention_mask = attention_mask,\n","                                            top_p = 0.9, temperature = 0.9, top_k = 0,\n","                                            max_length = 200, do_sample = True)\n","    elif decoding_method == \"top_k\":\n","        outputs = model.bart_model.generate(input_ids = input_ids,\n","                                            attention_mask = attention_mask,\n","                                            top_k = 50, temperature = 1.5, max_length = 200)\n","    elif decoding_method == \"greedy\":\n","        outputs = model.bart_model.generate(input_ids = input_ids,\n","                                            attention_mask = attention_mask,\n","                                            max_length = 200)\n","\n","    generated_outputs = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n","\n","    for i in range(len(outputs)):\n","        story_id = batch_data[\"story_ids\"][i]\n","        gen_stories[story_id] = generated_outputs[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gT3mRIzOLy7"},"outputs":[],"source":["# save to json file\n","with open('srl_pmi_greedy_stories.json', 'w') as fp:\n","    json.dump(gen_stories, fp)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"140ZkGpEOykdRQQgBiRXhDJSiCjACu1RZ","timestamp":1677717292210}],"toc_visible":true,"authorship_tag":"ABX9TyMRUud9zJNTOSFDvCQ2I5f5"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a0305d7fde414d4599f7d9db12db0a1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_192f87b1bdf0495793f7607c2d1d3411","IPY_MODEL_5893e467596540919dcbfb98cde69661","IPY_MODEL_e95cac1f79594aeb958a69546f58c48c"],"layout":"IPY_MODEL_ce414fff05a242269e19c07d346af84a"}},"192f87b1bdf0495793f7607c2d1d3411":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0310f1742b394dd193f6c62235d42fd0","placeholder":"​","style":"IPY_MODEL_0eb023805c064c988750041e46cd6b92","value":"Downloading: 100%"}},"5893e467596540919dcbfb98cde69661":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_934a99ddf60b4e858a744bb556295827","max":1018571383,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ce6858df0034f66b0d44e8bee22728b","value":1018571383}},"e95cac1f79594aeb958a69546f58c48c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_217efd8702f4403ca088f20a57d9d21c","placeholder":"​","style":"IPY_MODEL_511573d423cd4add98e508710089de96","value":" 1.02G/1.02G [00:16&lt;00:00, 59.7MB/s]"}},"ce414fff05a242269e19c07d346af84a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0310f1742b394dd193f6c62235d42fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb023805c064c988750041e46cd6b92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934a99ddf60b4e858a744bb556295827":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ce6858df0034f66b0d44e8bee22728b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"217efd8702f4403ca088f20a57d9d21c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"511573d423cd4add98e508710089de96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}